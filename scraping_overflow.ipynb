{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "from requests_html import HTMLSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    return links\n",
    " \n",
    "def get_all_links():\n",
    "    links = []\n",
    "    for n in range(1,80):\n",
    "        h = get_link('https://stackoverflow.com/questions/tagged/web-scraping?tab=frequent&page=' + str(n) + '&pagesize=50')\n",
    "        links.append(h)\n",
    "    return links\n",
    "\n",
    "links = get_all_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prendre les urls de la forme https://stackoverflow.com/questions/un_nombre\n",
    "def get_urls(urls):\n",
    "    url = []\n",
    "    for url_list in urls:\n",
    "        for i in url_list:\n",
    "            if i is not None:\n",
    "                if i.startswith('/questions/'):\n",
    "                    for n in range(0,10):\n",
    "                        if i.startswith('/questions/'+str(n)):\n",
    "                            url.append('https://stackoverflow.com'+i)\n",
    "    return url\n",
    "\n",
    "urls = get_urls(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3296"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_series = pd.Series(urls)\n",
    "unique_urls = urls_series.unique()\n",
    "len(unique_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am working with selenium to scrape some data...</td>\n",
       "      <td>Once you wait for the element and moving forwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm trying to develop a simple web scraper. I ...</td>\n",
       "      <td>EDIT Sept 2021: phantomjs isn't maintained any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am trying to import data from the following ...</td>\n",
       "      <td>Tl;Dr\\nAdapted from my answer to How to know i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What’s the difference between getting text and...</td>\n",
       "      <td>To start with, text is a property where as inn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Closed. This question needs details or clarity...</td>\n",
       "      <td>Here's a short snippet using the SoupStrainer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'm trying to pass a variable into a page.eval...</td>\n",
       "      <td>You have to pass the variable as an argument t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I'm having trouble parsing HTML elements with ...</td>\n",
       "      <td>You can refine your search to only find those ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I do a lot of HTML parsing in my line of work....</td>\n",
       "      <td>jsoup\\nSelf plug: I have just released a new J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I'm trying to scrape product information from ...</td>\n",
       "      <td>It really depends on how do you need to scrape...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Basically, I want to use BeautifulSoup to grab...</td>\n",
       "      <td>Try this:\\nfrom bs4 import BeautifulSoup from ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  I am working with selenium to scrape some data...   \n",
       "1  I'm trying to develop a simple web scraper. I ...   \n",
       "2  I am trying to import data from the following ...   \n",
       "3  What’s the difference between getting text and...   \n",
       "4  Closed. This question needs details or clarity...   \n",
       "5  I'm trying to pass a variable into a page.eval...   \n",
       "6  I'm having trouble parsing HTML elements with ...   \n",
       "7  I do a lot of HTML parsing in my line of work....   \n",
       "8  I'm trying to scrape product information from ...   \n",
       "9  Basically, I want to use BeautifulSoup to grab...   \n",
       "\n",
       "                                              answer  \n",
       "0  Once you wait for the element and moving forwa...  \n",
       "1  EDIT Sept 2021: phantomjs isn't maintained any...  \n",
       "2  Tl;Dr\\nAdapted from my answer to How to know i...  \n",
       "3  To start with, text is a property where as inn...  \n",
       "4  Here's a short snippet using the SoupStrainer ...  \n",
       "5  You have to pass the variable as an argument t...  \n",
       "6  You can refine your search to only find those ...  \n",
       "7  jsoup\\nSelf plug: I have just released a new J...  \n",
       "8  It really depends on how do you need to scrape...  \n",
       "9  Try this:\\nfrom bs4 import BeautifulSoup from ...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import random\n",
    "session = HTMLSession()\n",
    "data = []\n",
    "executor = ThreadPoolExecutor(max_workers=11)\n",
    "\n",
    "def scrape_url(urls2) :\n",
    "    for url in urls2:\n",
    "        r = session.get(url)\n",
    "        question_div = r.html.find('div.s-prose.js-post-body', first=True)\n",
    "        question = question_div.text if question_div else \"\"\n",
    "        answer_div = r.html.find('div.answercell.post-layout--right', first=True)\n",
    "        answer = answer_div.text if answer_div else \"\"\n",
    "        data.append({\"question\": question, \"answer\": answer})\n",
    "        time.sleep(np.random.uniform(0.02,0.05))\n",
    "    return data\n",
    "future = executor.submit(scrape_url, urls)\n",
    "data = future.result()\n",
    "dftest = pd.DataFrame(data)\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('stackoverflow1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    return links\n",
    " \n",
    "def get_all_links():\n",
    "    links = []\n",
    "    for n in range(1,1020):\n",
    "        h = get_link('https://stackoverflow.com/questions/tagged/web-scraping?tab=active&page='+ str(n) +'&pagesize=50')\n",
    "        links.append(h)\n",
    "    return links\n",
    "\n",
    "links1 = get_all_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85669"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls1 = get_urls(links1)\n",
    "urls1, len(urls1)\n",
    "#retier les doublons\n",
    "urls_series1 = pd.Series(urls1)\n",
    "unique_urls1 = urls_series1.unique()\n",
    "len(unique_urls1)\n",
    "urls1 = unique_urls1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls1 = unique_urls1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(urls2) :\n",
    "    for url in urls2:\n",
    "        r = session.get(url)\n",
    "        question_div = r.html.find('div.s-prose.js-post-body', first=True)\n",
    "        question = question_div.text if question_div else \"\"\n",
    "        answer_div = r.html.find('div.answercell.post-layout--right', first=True)\n",
    "        answer = answer_div.text if answer_div else \"\"\n",
    "        data.append({\"question\": question, \"answer\": answer})\n",
    "        time.sleep(np.random.uniform(0.02,0.5))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = HTMLSession()\n",
    "data = []\n",
    "executor = ThreadPoolExecutor(max_workers=11)\n",
    "future1 = executor.submit(scrape_url, urls1)\n",
    "data1 = future1.result()\n",
    "df1 = pd.DataFrame(data1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import time from playwright.sync_api import sy...</td>\n",
       "      <td>Locators check visibility, but you don't care ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am trying to scroll the datasheet automatica...</td>\n",
       "      <td>Try this:\\ndocument.querySelector(\".showMoreRe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I tried to web scraping from this website to g...</td>\n",
       "      <td>Replace\\nsales = cells[4].text.strip()\\nWith\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I would like to extract data from the the tabl...</td>\n",
       "      <td>Instead of using SelectorGadget or element ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm creating a scraper connected to a Telegram...</td>\n",
       "      <td>It's failing because the selenium-wire library...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>I am currently trying to scrape data from the ...</td>\n",
       "      <td>You can do this much faster using Requests and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>options.add_argument('--disable-blink-features...</td>\n",
       "      <td>Javascript fingerprinting and context analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>I'm trying to automate away downloading multip...</td>\n",
       "      <td>I was able to get it to work. The selector is:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>I am trying to web scrape the website savevide...</td>\n",
       "      <td>JSOUP is a Static HTML parser. You cannot pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>I'm using this github code to try to get the p...</td>\n",
       "      <td>In the source is already a similar example (co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2651 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     import time from playwright.sync_api import sy...   \n",
       "1     I am trying to scroll the datasheet automatica...   \n",
       "2     I tried to web scraping from this website to g...   \n",
       "3     I would like to extract data from the the tabl...   \n",
       "4     I'm creating a scraper connected to a Telegram...   \n",
       "...                                                 ...   \n",
       "2646  I am currently trying to scrape data from the ...   \n",
       "2647  options.add_argument('--disable-blink-features...   \n",
       "2648  I'm trying to automate away downloading multip...   \n",
       "2649  I am trying to web scrape the website savevide...   \n",
       "2650  I'm using this github code to try to get the p...   \n",
       "\n",
       "                                                 answer  \n",
       "0     Locators check visibility, but you don't care ...  \n",
       "1     Try this:\\ndocument.querySelector(\".showMoreRe...  \n",
       "2     Replace\\nsales = cells[4].text.strip()\\nWith\\n...  \n",
       "3     Instead of using SelectorGadget or element ins...  \n",
       "4     It's failing because the selenium-wire library...  \n",
       "...                                                 ...  \n",
       "2646  You can do this much faster using Requests and...  \n",
       "2647  Javascript fingerprinting and context analysis...  \n",
       "2648  I was able to get it to work. The selector is:...  \n",
       "2649  JSOUP is a Static HTML parser. You cannot pars...  \n",
       "2650  In the source is already a similar example (co...  \n",
       "\n",
       "[2651 rows x 2 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop des lignes vides ou avec moiiins de 10 caractères\n",
    "df1 = df1[df1['question'].str.len() > 10]\n",
    "df1 = df1[df1['answer'].str.len() > 10]\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('stackoverflow2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    return links\n",
    " \n",
    "def get_all_links():\n",
    "    links = []\n",
    "    for n in range(1,1020):\n",
    "        h = get_link('https://stackoverflow.com/questions/tagged/web-scraping?tab=votes&page='+ str(n) +'&pagesize=50')\n",
    "        links.append(h)\n",
    "    return links\n",
    "\n",
    "links1 = get_all_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48792"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls2 = get_urls(links1)\n",
    "unique_urls2 = pd.Series(urls2).unique()\n",
    "urls2 = unique_urls2.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m executor \u001b[38;5;241m=\u001b[39m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m      4\u001b[0m future2 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(scrape_url, urls2)\n\u001b[0;32m----> 5\u001b[0m data2 \u001b[38;5;241m=\u001b[39m \u001b[43mfuture2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data2)\n\u001b[1;32m      7\u001b[0m df2\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session = HTMLSession()\n",
    "data = []\n",
    "executor = ThreadPoolExecutor(max_workers=11)\n",
    "future2 = executor.submit(scrape_url, urls2)\n",
    "data2 = future2.result()\n",
    "df2 = pd.DataFrame(data2)\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
