{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Web scraping with Node.js and Typescript - the scraper part (1/3)\n",
      "URL: https://dev.to/uiii/web-scraping-with-nodejs-and-typescript-the-scraper-part-ffn\n",
      "Answer: Internet is full of information these days. Almost every website display them to the user in a human readable form. But what if you want to process these data programmatically, do some analysis, present them in a different form or store them in a database to make queries on them later? E.g. collect all the product names with a description, image and a price from your favorite online store. Well, you can open the page by page and copy&paste the data you need, but you won't ü§¶‚Äç‚ôÇÔ∏è. What you definitely can and should is to check if the page has an API which will provide you the data easily. If not, I'm sorry bro there is no way to ... just kidding! üòù\n",
      "... the web scraping comes into play. Yay!\n",
      "üëâ In this article series (3 parts) I will guide you through the whole process of building a web scraper in Node.js and Typescript.\n",
      "In the first article you will learn how to scrape data from a single webpage. In the second article you I will teach you how to crawl the website to find and scrape all the wanted pages. And in the last article I will show you how to use a proxy with the scraper (coming soon) which can have some advantages in certain situations.\n",
      "If you are a beginner or a more skilled programmer who is new to web scraping, transitioning from different programming language or just curious how others do it, you will benefit from this.\n",
      "I assume you are familiar with Javascript and Typescript, know HTML and CSS selectors and have Node.js installed.\n",
      "If not, check out these resources:\n",
      "As you should know the websites are built using HTML and CSS. HTML describes the structure of the information in the page using tags. What web scraper does is extracting required information from the specified HTML tags. CSS selectors are very good way how to tell the scraper which tags to look at.\n",
      "So the input for the scraper is the URL of a page (e.g. product detail). The scraper then loads the HTML source code, parse it, filters the tags specified by CSS selectors and extracts text from them. Then outputs the extracted data in a structured way (e.g. JSON). Easy, right?\n",
      "Wait! You may ask ... Where do I get the page URLs? Do I have to copy&paste them to the crawler manually?\n",
      "Of course not! Web scrapers are usually more robust and also contain the \"crawler\" part to automate the whole process.\n",
      "The crawler will go through (crawls) the website and search for the pages which have the data to be scraped.\n",
      "Actually, it is a special type of scraper which usually starts at homepage and looks for the hyperlinks according to specific rules and follows them and repeats the process until it finds the desired pages.\n",
      "The term \"web scraper\" is often used interchangeably with \"web crawler\".\n",
      "‚ùó‚ùó Important thing to know is you should be careful when scraping any website. Web scraping isn‚Äôt illegal by itself but you should care about how you do it and what you do with the data. There is also an ethical side of it. Do not harm the website and check if you have the rights to use the data the way you are going to. Read more here: https://blog.apify.com/is-web-scraping-legal/. If you are not sure, ask your lawyer.\n",
      "Disclaimer: I am not taking any responsibility for your web scraping activities. Do it at your own risk.\n",
      "üéì For an example, consider we want to have a list of all European capital cities with a basic data like its name, name of the country, current population, area and an image of a city flag. The Wikipedia can be used as a good source of information.\n",
      "First, init the project\n",
      "and install the packages we will need.\n",
      "Axios is an HTTP client which we will use for fetching website data. It is more robust and feature-rich alternative to Fetch API.\n",
      "Cheerio is a tool to parse HTML and gives you the ability to make queries on HTML tags and extract data from them. It is similar to jQuery but more suitable for server side.\n",
      "üíª See the complete project in the GitHub repository\n",
      "As we are prepared, we will start with a \"scraper\" part, so go and look at the capital city page we are going to scrape, e.g. https://en.wikipedia.org/wiki/Prague\n",
      "\n",
      "\n",
      "\n",
      "There it is, the data we need. Ah, ok, but how do we know the location of the data in the page's HTML ü§î? Easily, we use dev tools. I'm using Chrome browser (other modern browsers usually have dev tools too) so right click the article's title element and select Inspect.\n",
      "\n",
      "As you can see, the name of the city resides in <h1> tag with the ID firstHeading. I'm sure you are getting the idea.\n",
      "Stop talking and create some code!\n",
      "Create a file index.ts and put in this code\n",
      "üíª See the commit f13ccee0\n",
      "Are you excited? Run the code\n",
      "üéâ Congratulations, your first scraping! Isn't it beautiful ü§©?\n",
      "I think the code is quite self-explanatory, still I will go through some interesting moments\n",
      "Axios makes an HTTP GET request to the specified URL and returns a promise which will hold the response with an HTML source code (in our case).\n",
      "If you are not familiar with async/await check this https://javascript.info/async-await. Basically, it is very comfortable way to work with JS promises. The code \"waits\" until the promise is resolved and returns its data.\n",
      "Cheerio parses the HTML and returns a querying function bound to a document based on that HTML markup. The querying function ($) accepts CSS selector and finds corresponding element(s) in the document.\n",
      "Here we find the element with ID firstHeading and get its text content. It is also a good practice to trim the leading and trailing whitespace.\n",
      "Ok, this was easy, right? Let's move on to something more difficult.\n",
      "\n",
      "The country's name is in <a> tag, but the tag has no ID or a class. We have to loot at its parent elements. The interesting one is the table row <tr> with a class mergedtoprow. But, there is a catch. If you look around, there are lots of table rows with the same class. Hmm ü§î, how do we select the correct row? Maybe we can use row's index? I wouldn't count on that as the other pages may have different number of info rows. I think there is no easy way with regular CSS selectors. What we can expect is that the row's label will always be \"Country\". Cheerio supports the same selectors as jQuery and it has a special selector :contains() (see jQuery doc) which checks if the element contains specific text. So the idea is we find the <td> element which is after the <th> element (row's label) containing text \"Country\".\n",
      "Add this to the end of the scrapeCity method.\n",
      "üíª See the commit 07db4fbe\n",
      "Run the code again\n",
      "Nice!\n",
      "When you look at area and population, the rows we are interested in have the same label Capital city, therefore we can't use the same selector as for the country name directly. We need to find the relevant row according to the previous label Area or Population. You might be getting the impression that the row is nested inside a box, but, if you look closely, there are no boxes actually.\n",
      "\n",
      "There are \"top level\" rows with mergedtoprow class which may have a \"sub rows\" with a class mergedrow. The \"sub rows\" are placed between two \"top level\" rows and relate to the first one. This is all we need to know.\n",
      "First line find the \"Area\" label, the parent() method select the wrapping row and with nextUntil() we select all the next elements (rows) before next \"top level\" row. With this we get a context (areaRows) where we find the value with the same principle as for the country's name.\n",
      "The same for population\n",
      "üíª See the commit 95c3bb09\n",
      "And after running\n",
      "All right. We have got the information, but in a formatted shape ü§î. We want numbers!\n",
      "It happens quite often when scraping that the information you scrape is formatted as human readable and not structured very well. So you have to make another step to parse (extract) the right data from the strings you scrape. In our case we want to have are as number of squared kilometers and the population as a count of persons.\n",
      "Regular expressions for the win!\n",
      "Modify the code slightly\n",
      "Notice this will work for English localization, different languages can have numbers in different format.\n",
      "In area text, we first drop everything from the unit to the end. And before converting to a number with parseFloat the commas must be removed.\n",
      "üíª See the commit e88b6bae\n",
      "Looks better now!\n",
      "When scraping images, you can just scrape the image's URL or download the file itself. URL is fine if you want to display the image on another website or just want to store the link to it. But if you want to make some modifications to the image or you can't rely on the image's availability on the source website, you need to download it. I will show you the second case.\n",
      "Still, we need to obtain the image's URL first. Let's analyze the HTML for the city flag.\n",
      "\n",
      "The image of the flag is wrapped in <a> tag with image class which is in front of the <div> with a text \"Flag\". However, the <img> tag doesn‚Äôt keep the original SVG file, only the small PNG thumbnail. The anchor tag looks like it keeps the image‚Äôs URL.\n",
      "Actually, it links to another webpage.\n",
      "\n",
      "There it is. The <a> tag there has the URL we are looking for.\n",
      "Get the flag image page URL.\n",
      "I made the selector more universal by using a template string with a city's name. The flagPageLink variable keeps the relative path. The URL object will help us to obtain the full URL, the second argument is a base URL, the one of the city's wiki page in our case.\n",
      "To make the scraper more organized, I moved the code for the image scraping into the separated method scrapeImage. The method can be used to scrape the image from any Wikipedia's image detail page.\n",
      "Everything should be already familiar to you. And again the code related to downloading of the image is separated to another method downloadFile.\n",
      "This method is universal for downloading any file to a specified directory. The option responseType: 'arraybuffer' is crucial here. Axios will then consider the URL as a source of binary data and don't try to parse the response as a text.\n",
      "üíª See the commit 2e0dec92\n",
      "Now, if you run the code, you will see this\n",
      "And if you look into the folder flags you will find the file Flag_of_Prague.svg here ü•≥.\n",
      "Great, we can scrape all the data we need. But all of them are just printed to the console in the moment when they are obtained. This is not good to work with. We want to return them in some form from our scrapeCity method. Plain object is sufficient.\n",
      "For type safety, we will use an interface. Put it above the scraper class.\n",
      "Remove all the console.log commands and put the this code at the end of scrapeCity function.\n",
      "Now this is much better, out scraped data has a specific shape and we can manipulate with them later. For now, we will just modify our main function to get the city object and print it to the console (in whole).\n",
      "üíª See the commit 96426d2d\n",
      "Run the script.\n",
      "I feel quite satisfied now üòé. What about you?\n",
      "üíª See the complete project in the GitHub repository\n",
      "Now you know how to scrape a web page in Javascript/Typescript. I hope you agree it is quite easy and fun.\n",
      "Of course it depends on the website you want to scrape, the less the data you want are structured, the harder is it to get them. There are always many ways how to achieve the goal, sometimes it is straightforward, sometimes tricky. But if you managed it, the result can be quite satisfying when you are giving an order to something unorganised üòÅ.\n",
      "Currently, we can handle a single capital city page only. In the next article I will teach you how to crawl the Wikipedia website to scrape all of them.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping With Python\n",
      "URL: https://dev.to/geofspot/web-scraping-with-python-44hd\n",
      "Answer: In a nutshell, web scraping is the act of an automated extraction of publicly displayed data from the web which could not be reached nor extracted by the use of API.\n",
      "Most times, the basic aim for for web scraping are usually for price or news monitoring, information/data gathering, automated research, automated web/platform engagements and similar events.\n",
      "Web scraping has become very popular and important in recent times due to it‚Äôs relevance in the current business world. However, this tutorial is about web scraping with python, so without further ado we‚Äôll dive into what web scraping with python looks like and the libraries needed to code a simple web scraper.\n",
      "Python is widely known to be useful in many things in tech, but web scraping happens to be one of the major domains where python programming thrives.\n",
      "Selenium is an umbrella project for a range of tools and libraries that enable and support the automation of web browsers.\n",
      "It provides extensions to emulate user interaction with browsers, a distribution server for scaling browser allocation, and the infrastructure for implementations of the W3C WebDriver specification that lets you write interchangeable code for all major web browsers.\n",
      "Meanwhile, selenium is not the only module used for web scraping with python, there are other major modules that are also as popular as selenium. However there are cons and pros for each of them, you just need to know the one you need at every occasion. We‚Äôll discuss a brief comparison of these modules further on.\n",
      "The 3 most popular python modules used for web scraping are as follows:\n",
      "Scrapy is efficient and portable. However it‚Äôs major con is that it‚Äôs not user friendly, especially for beginners.\n",
      "Beautiful soup is easy to learn and understand. However it does have some cons too: Beautiful soup requires dependencies and it‚Äôs less efficient than Scrapy.\n",
      "Selenium is versatile and also works well with javascript. However selenium is also not as efficient as scrapy.\n",
      "In this post we‚Äôll use selenium as our module for web scraping with python, perhaps in my next web scraping post we‚Äôll adopt any of the other modules mentioned above.\n",
      "We are about to code a web scraper that will go to the popular wikipedia‚Äôs website and enter a query in the search box, get results and possibly links too. Make no mistakes there is a module specifically for wikipedia search called wikipedia.\n",
      "However, the aim here is to show how one can access a public website, fill a form, submit it, explore the site contents and more. But we are just going to keep things simple on this particular post.\n",
      "You have the basic experience of HTML and CSS\n",
      "You have at least beginner‚Äôs basic python coding experience, for instance you are familiar with loops, functions, importing of modules and similar knowledge.\n",
      "Meanwhile, if you have not used selenium before, please do yourself a favor, checkout the basic documentation of this module here before you continue with this tutorial.\n",
      "Firstly, we‚Äôll begin by importing the necessary modules. But before importing these modules you‚Äôd need to download your web browser‚Äôs web driver. I personally prefer google chrome driver.\n",
      "Be sure that you downloaded the same version as the version of your browser, to check your browser version, click on the 3 dots on chrome and click on ‚Äúhelp‚Äù then click on ‚Äúabout google chrome‚Äù right there you‚Äôll see the version you are using.\n",
      "Once you are done with the download, extract the file and keep it somewhere close to your code folder and note the path to the driver.\n",
      "Now let‚Äôs import the necessary modules.\n",
      "Now it‚Äôs time to create our main function for this code:\n",
      "Let‚Äôs explain the code above:\n",
      "We created a function called get_wiki, now the variable keyword will get a keyword to search for from the user. link dec variable is for decision about needing links or not by the user.\n",
      "Then we created the the driver path and the driver instance. Now we‚Äôll continue with more codes inside the main function:\n",
      "I presume you have checked out the selenium documentation as I advised earlier, and with your prior knowledge on HTML and CSS, you already know how to find the needed selectors and elements on the wikipedia page.\n",
      "You can just open the website in a new window and explore the elements and selectors with chrome developers tools while simulating the search. This will enable you to check what is working and what‚Äôs not, in case you run into bugs.\n",
      "So with the code above we‚Äôll get the page and enter a keyword to search and press the search button. We wait for 3 seconds, get the results, and print them out using pprint.\n",
      "Now let‚Äôs create an inner function inside the main function that will get the available links if required by the user:\n",
      "As you can see the function above is self explanatory. Next, we‚Äôll call the function if the link_dec was ‚Äúyes‚Äù and quit the driver, next we call the main function:\n",
      "Now let‚Äôs see all the codes in one place:\n",
      "From here you can do other things with your search results, like sending them to an email address, converting them to pdf file and more.\n",
      "In my next web scraping with python post, we‚Äôll focus more on other cools stuffs like getting prices and updates on news, trading and more. We‚Äôll also learn about beautiful soup, regex, sending emails with python and more.\n",
      "You can edit this code and use it on different sites or search engine like google. Now that you have the basic knowledge, you can explore selenium even more, create better scrapers than what I did here.\n",
      "Becoming better in anything requires curiosity, so get curious and explore the available knowledge on the internet about web scraping, you might want to check the popular programming communities for extra knowledge on the topic.\n",
      "To automatically get notification when my next post on web scraping with python and subsequent ones gets published, hit the follow button.\n",
      "Get an affordable and seamless python one on one training today from anywhere in the world, location is never a barrier, we have friendly learning tools to make your python programming training a worthwhile experience.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping Multiple Pages using Python & BeautifulSoup\n",
      "URL: https://dev.to/sudhanshumukherjeexx/web-scraping-multiple-pages-using-python-beautifulsoup-26dg\n",
      "Answer: Web Scraping is used to examine websites for Unstructured Data and store it Structurally for our use. Web Scraping helps us iterate over several web pages and extract the required information, It is then stored in a format suitable for the user.\n",
      "In this project, we will learn 'How to use Python and Beautiful Soup to scrape Mobile Phone Names and Prices of Brand Mi on a Flipkart E-commerce Website'.\n",
      "Click here for a code\n",
      "The objective of this project is to scrape Flipkart's Website and extract the 'Name' and 'Prices' of Mi Mobile Phones.\n",
      "Get the URL that will be needed to make a request.\n",
      "The URL which we will be using: https://www.flipkart.com/mobiles/mi~brand/pr?sid=tyy%2C4io&otracker=nmenu_sub_Electronics_0_Mi&page=\n",
      "Now, It is very necessary to identify the right class of the elements stored which we need to scrape. To inspect elements follow these steps.\n",
      "Step 1 - Visit the URL\n",
      "Step 2 - Right on the website and select inspect or press Ctrl + shift + I together.\n",
      "Step 3 - Hover on the name of the phone and click it. Select the class from the window appearing on the right.\n",
      "Step 4 - Apply the same process for price.\n",
      "Step 5 - Copy this class somewhere, we will need it later in our code.\n",
      "Let's import the required libraries and packages first. To begin with, I will be using Beautiful Soup for parsing HTML documents which can later be used to extract data from HTML. Then, We will import 'requests' which allows us to interact with the web, It contains many useful features and methods to make HTTP requests.\n",
      "Lastly, we will need pandas to store our scraped data in an organized way.\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import pandas as pd\n",
      "Create an empty list for Phone Name and Phone Price.\n",
      "phone_name = []\n",
      "phone_price = []\n",
      "Declare a variable 'page_num' which will help us take input from users about the number of pages they want to scrape.\n",
      "page_num = int(input(\"Enter number of pages:\"))\n",
      "Now we will use for Loop to iterate over multiple pages looking for relevant information and operations to implement.\n",
      "Then, Let's store our URL in the variable url and make requests to build a connection. Don't forget to add(+) str(i). This will help our program to iterate over multiple pages while using for loop.\n",
      "After, making a request, let's use Beautiful Soup for parsing HTML and store all the data in a variable called content.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This is our Aha moment folks. We have successfully created our web scraper using python and Beautiful Soup. To get a better understanding of this, Use a website that you personally like and try scraping it.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Python 101\n",
      "URL: https://dev.to/anderrv/web-scraping-with-python-101-2kg3\n",
      "Answer: The Internet is a vast data source if you know where to look - and how to extract it! Going page by page and copying the data manually is not an option anymore. And yet, many people still are doing that.\n",
      "It would be great if our favorite data source were to expose all of it in a convenient format such as CSV. We wish it were so.\n",
      "What if we told you that there is a solution? We are talking about Website Scraping. It allows you to extract structured information from any source available on the Internet. You can choose the data you will get and how to store it. And it is repeatable, meaning that you can run the same script every day - hour, or whatever- to get new items.\n",
      "Continue with us to learn how to code your own web scraper taking a job board as an example target. We will build step-by-step a web scraping project with Python.\n",
      "Curious? Let's dive in! ü§ø\n",
      "For the code to work, you will need python3 installed. Some systems have it pre-installed. After that, install all the necessary libraries by running pip install.\n",
      "If you prefer video content, watch this video from our Youtube channel.\n",
      "\n",
      "Web scraping consists of extracting data from websites. We could do it manually, but scraping generally refers to the automated way: software - usually called bot or crawler - visits web pages and gets the content we are after.\n",
      "The easier way to access data is via API (Application Programming Interface). The downside is that not many sites offer it or allow limited access. For those sites without API, extracting data directly from the content - scraping - is a viable solution.\n",
      "As we'll see later, many websites have countermeasures in place to avoid massive scraping. Do not worry for the moment; it should work for a few requests.\n",
      "The most famous crawlers are search engines, like Google, which visit and index almost the whole Internet. We'd better start small, so we'll begin from the basics and build upon that. These are the four main parts that form website scraping:\n",
      "Let's zoom out for a second and go back to the fundamentals - sorry, a bit of theory is coming. You can skip the following two sections if you are already familiar with the basics.\n",
      "A web page consists mainly of HTML, CSS, Javascript (JS), and images. Bear with us for a second if you don't understand parts of that sentence.\n",
      "A browser, like Chrome, opens a connection via the Internet to your favorite website using the HTTP protocol. The most common request type is GET, which usually retrieves information without modifying it. Then the server processes it and sends back an HTML (HyperText Markup Language) response. Simplifying, HTML is a text file with a syntax that will tell the browser what content to paint, text to show, and what resources to download.\n",
      "In those extra resources are the ones mentioned above:\n",
      "The browser will handle all the requests/responses and render the final content. Everything shows the style defined by the CSS file. Thanks to the behavior in the JS files, the infinite scroll works perfectly. Images are loaded and displayed where they should. The browser is really doing a lot of work, but usually so fast that we don't even notice as users.\n",
      "However, the critical part for web scraping is the initial HTML. Some pages will load content later, but we will focus - for clarity - on those that load everything initially. That difference is usually called static vs. dynamic pages.\n",
      "If your case involves dynamic pages, you can go to our article on scraping with Selenium, a headless browser. In short, it launches a real browser to access the target webpage. But it is programmatically controlled, so you can extract the content you desire.\n",
      "HTML \"is the standard markup language for documents designed to be displayed in a web browser.\" It will structure the page using tags, each one meaning something different to the browser. For example, <strong> will show text in bold, and <i> will do so in italics.\n",
      "Other components will control what can be done and not the display format. Examples of that are <form> and <input>, which allow us to fill and send forms to the server, such as logging in and registering.\n",
      "HTML elements might have attributes such as class or id, which are name-value pairs, separated by =. They are optional but quite common, especially classes. CSS uses them for styling and Javascript for adding interactivity. Some of them are directly associated with a tag, like href with <a> - URL of the link tag.\n",
      "An example: <a id=\"example\" class=\"link-style\" href=\"http://example.org/\">Go to example.org</a>\n",
      "Now that we've seen the basics, let's use Python and the Requests library to download a page.\n",
      "We will start by importing the library and defining a variable with the URL we want to access. Then, use the get function to obtain the page and print the response. It will be an object with the response's data: the HTML and other essential pieces such as status code.\n",
      "We won't go into further detail on status codes, which indicate whether the request was successful. The ones in the 200 - 299 range mean success (sometimes denoted 2XX), 3XX indicates a redirection, 4XX client error, and 5XX server error. Don't worry, the responses should be 200 in our tests, and you will learn later what to do otherwise.\n",
      "As seen in the response's text above, the data is there, but it is not easy to obtain the interesting bits. It is prepared to be consumed by a browser, not a human! To make it accessible, we will use BeautifulSoup, \"a Python library for pulling data out of HTML.\"\n",
      "It will allow us to get the data we want using the classes and IDs mentioned above. Following the example in the previous section, we will access the title (<h1>) and the link (<a>). We'll see how we know what tags to look for in a moment.\n",
      "\n",
      "What happens if we want to access several items? There are two paragraphs (<p>) in the example, but the find function would only get the first one. find_all will do precisely that, returning a list (ResultSet) instead of an element.\n",
      "But we can access more than just text. For example, the link's target URL is in the element's href property, accessible with the get function.\n",
      "So far, so good. But this is a simple example, and we accessed all the items by the tags. What about a more complex page with tens of different tags? We will probably need classes, IDs, and a new concept: nesting.\n",
      "We will build a functional web scraper with an example site in the following sections. For now, a quick guide on the fundamental selectors:\n",
      "Now that we've got the basics, we can move on to the fun part.\n",
      "Hold your horses üêé! We know that coding is the fun part, but first, you'll need some understanding of the page you're trying to scrape. And not just the content but also the structure. We suggest browsing the target site for a few minutes with DevTools open (or any other tool).\n",
      "For the rest of the examples, we will use remotive.io to demonstrate what can be done. Its homepage contains lists of job offers. We'll go step-by-step, getting the data available and structuring it.\n",
      "To start exploring, go to the page on a new tab and open DevTolls. You can do it by pressing Control+Shift+I (Windows/Linux), Command+Option+I (Mac), or Right-click ‚û° Inspect. Now go to the Elements tab [#1 on the image below], which will show the page's HTML. To inspect an element on the page, click on the select icon (to the left) [#2], and it will allow you to pick an item using the mouse [#3]. It will be highlighted and expanded on the Network tab [#4].\n",
      "\n",
      "Once familiarized with DevTools, take a look, click elements, inspect the HTML, and understand how classes are used. Discover as much as possible and identify the essential parts (hint: look for the job entries with the class job-tile). We picked a well-structured site with classes that reflect the content, but it is not always the case.\n",
      "\n",
      "Then explore also other page types, such as category or job offer. Maybe some details are essential to you but only available on the offer page. That would change the way to approach the coding. Or you realize that, as is the case, the structure for the category pages (i.e., Software Development) is the same as the homepage. That means we could scrape all the category pages for a bigger offer list! We won't do that for now, but we could do it.\n",
      "Spoiler: it is not precisely the same since category pages are dynamically rendered. As mentioned above, headless browsers are needed for those cases.\n",
      "The first part of the code is the same as with example.org. Get the content using requests.get.\n",
      "As mentioned earlier, many sites have anti-scraping software. The most basic action is blocking an IP with too many requests. Adding proxies to requests is simple, and they will hide your IP. This way, in case of being banned, your home/office address would be unaffected. But even better, some proxies add rotating power, which means that they assign a new IP to every request. That makes the banning part much more complicated, giving you room to scrape with fewer restrictions.\n",
      "Tagging IPs is just the most common measure, but they might implement many more such as checking headers or geolocation. But for most sites, this approach should work fine.\n",
      "We are fetching just one URL, which is fine to begin. But it gets more complicated when trying to scrape at scale, so we will keep it sequential and straightforward at this point.\n",
      "This step might look like the easiest one at first since requests handles it for us. But we mention the complications for you to be aware of üòÖ\n",
      "This step is the one that changes most from case to case. Since it decides what data you will extract, it varies for each page type you want. For example, the selectors for the offer page won't work for the homepage. And suppose anything changes on the page (new fields, design change). In that case, you will probably need to modify the scraper to reflect those changes.\n",
      "Following the job board example, the first thing we want to access is the job offer wrapper using ID (id=\"initial_job_list\"), a job offer, and its title using class (class_=\"job-tile\"). Note that it is \"class_\" and not \"class\" since it is a reserved word in Python.\n",
      "See how we use the variable created in the previous line instead of soup? That means that the lookup will take place in the element - i.e., jobs_wrapper - and not the whole page. This is an example of the previously mentioned nesting but storing them in variables instead of concatenation.\n",
      "There is a slight problem there, right? We only got ONE job offer! We need to change jobs_wrapper.find for jobs_wrapper.find_all to get all the job offers and then extract the data we want for each one. We will move that logic to a function to separate concerns.\n",
      "BeautifulSoup also offers the select function, which takes a CSS selector and returns an array of matching elements.\n",
      "For more ideas, check out our article on tricks when scraping. We explain that you can get relevant data from different sources such as hidden inputs, tables, or metadata. The selectors used here are very useful, but there are many alternatives.\n",
      "And the same idea applies to extracting data, although we already moved it to the extract_data function. This way, if we were to change any selector because the paged changed, we would locate it quickly.\n",
      "We can read and operate with the extracted data. But for future analysis and usage, it is better to store it. In a real-world project, a database is usually the chosen option. We will save it in a CSV file for the demo.\n",
      "Since CSV files and formats are beyond this tutorial, we will use the pandas library. The first thing is to create a DataFrame from the data we got. It accepts dictionaries usually without problems. Then, we call the helpful to_csv function to convert the dataframe to CSV and save it in a file.\n",
      "\n",
      "As with any piece of software, we should follow good development practices. Single responsibility is one of them, and we should separate each of the steps above for maintainability. Storing CSV files is probably not a good option long-term. And, when the moment comes, it will be easier to replace if that part is separated from the extraction logic.\n",
      "And the same idea applies to extracting data, although we already moved it to the extract_data function. This way, if we were to change any selector because the paged changed, we would locate it quickly.\n",
      "Celebrate now! You built your first scraper. üéâ\n",
      "We said earlier that you should not worry about the error codes for the tutorial. But for a real-world project, there are a few things that your should consider adding:\n",
      "The following steps would be to scale the scrapers and automatize running them. We won't go deep into these topics, just a quick overview.\n",
      "Following the example, two main things come to mind: scraping category pages instead of the homepage and getting more data from the job offer detail page. Depending on the intention behind the scraping, one might have more sense than the other.\n",
      "We'll take the second one, scraping further details from each offer. We have already extracted the links in the previous steps to get them. We have to loop over them, get the link, and request the URL. Prepend the domain since it is a relative path. We simplified the data extraction part for brevity. The point is the same as above, extract the data you need using selectors.\n",
      "Be careful when running the code below. We added a \"trick\" to slice the array and get only two job offer details. The idea is to avoid a hundred requests to the same host in seconds and make it faster.\n",
      "Many other problems appear when scraping, one of the most common being blocks. For privacy or security, some sites will not allow access to users that requests too many pages. Or will show captchas to ensure that they are real users.\n",
      "We can take many actions to avoid blocks. Still, as mentioned earlier, the most effective one is using smart proxies that will change your IP in every request. If you are interested or have any problems, check out our article on avoiding detection.\n",
      "We'd like you to part with the four main steps:\n",
      "If you leave with those points clear, we'll be more than happy ü•≥\n",
      "Web scraping with Python, or any other language/tool, is a long road. Try not to feel overwhelmed by the immensity of resources available. Scraping Instagram on your first day will probably not be possible since they are well-known blockers. Start with an easier target and gain some confidence.\n",
      "Feel free to ask any follow-up questions or contact us.\n",
      "Thanks for reading! Did you find the content helpful? Please, spread the word and share it. üëà\n",
      "Originally published at https://www.zenrows.com\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping in Golang\n",
      "URL: https://dev.to/forchapearl/web-scraping-in-golang-33kb\n",
      "Answer: Web scraping is a handy tool to have in a data scientist's skill set. It can be useful in a\n",
      "variety of situations to gather data, such as when a website does not provide an API. We will be using this golang package github.com/anaskhan96/soup. It performs the same as beautifulsoup of python.\n",
      "This is the webpage we are going to be scraping.\n",
      "Below is a highlighted image of our desired values on the Browsers Dev tools.\n",
      "\n",
      "Here is our code\n",
      "And Here is our output.\n",
      "Here is the Github Repo\n",
      "Credits to Daniel Whitenack on his book Machine Learning With Go\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with NodeJS: a comprehensive guide [part-3]\n",
      "URL: https://dev.to/aviyel/web-scraping-with-nodejs-a-comprehensive-guide-part-3-1b6n\n",
      "Answer: Let's look at a simpler and faster way to gather selectors to scrape data from a website. This method is useful when you need to get something done quickly without having to worry too much about it. The only disadvantage is that it can be more vulnerable to errors. So, let's get this party started. Previously, when we wanted to scrape something from the IMDB scraper, we went to the website, used the inspector to look at the HTML structure, and then built a specific selector for that type of data. Let's look at an example. If we wanted to extract the title, we used a section with a class of ipc-page-section and then div with a class of TitleBlock, followed by other tags/elements, and then we needed to get the text, but this is a very long method.\n",
      "So, let's see what we can scrape for this example, let's say we wanted to get the movie plot, so what we can do is right-click inspect as we did previously, and what we can see right here is that the element that we want to scrape is a span containing some text.\n",
      "\n",
      "After that, what we want to do is scrape everything, and this is the simplest way to do it without overcomplicating things, so right-click on the element that we want to scrape, click on copy, and then copy selector by right-clicking on the selector and copying it. This implies that the browser will construct the selector for you and will apply its best reasoning to provide you with the results you desire.\n",
      "\n",
      "Let's try it out in our code editor as well, so go to the code and create a new variable called moviePlot, and then do it the same way we did before, copying everything and pasting it right here. We want to get the text and also trim it, so now that we have the variable, let's set a breakpoint right at it or simply console log it to see what the moviePlot variable is spitting out.\n",
      "\n",
      "So, as you can see, all we had to do was copy and paste the selector generated by the Chrome browser. Of course, this is a lot easier than writing the selectors ourselves, and before we go any further, let's talk about the disadvantages of using this method versus the others we've discussed previously. There is no actual problem with using this method; no one is stopping you, but there is one thing to consider. As you can see, this selector is much larger than the others we constructed by hand. This is because the browser works its magic and tries to offer you the selector for the element it feels is the finest. However, there are far more selectors than are required. The disadvantage of having a lot of elements in your query is that the chances of it failing in the future are much higher. As you can see from what we wrote before, we used about one, two, or even three or four elements to scrape something, but in this case, we are using five, six, and seven elements to get that exact text. If one of those elements from the website changes, the query stops working, So, depending on the website and the circumstance, it's preferable to have two or three elements in our query selector than seven or even more. Always remember, the simpler the better. So, here's the question you might want to ask yourself: when would I choose one more than the other?\n",
      "So, if you need to get something done quickly for any reason, or you know that a particular website doesn't change very often, you may use this copy and paste selection method. The beautiful thing about this method is that you don't even need to know how to write CSS selectors to build a scraper and this method may seem way easier and faster.\n",
      "In this segment, we'll learn how to more precisely download anything from a website and save it as a file. We'll go over how to get the image posters for all the movies that you wish to scrape.\n",
      "First, a brief reminder that we'll be using the request library rather than the request-promise library we've been using up to this point. So, before we go any further, let's implement it right at the top, in the import area of the code, and just import the library.\n",
      "Also, don't forget to update the variable name from \"request\" to \"requestPromise.\". The reason we're using the request library rather than the request-promise library is that the request library is the original library dedicated to straightforward processing of the types of requests we make in Node. The request-promise library that we previously used is just a wrapper around the original request library, and it allows you to use the ES 2015 syntax with async-await instead of going into callback hell with the original library. This is the main reason we used request-promise rather than the request library itself for our scraper.\n",
      "Now, before we get started on actually developing the scrapper for obtaining and storing images/posters, let's perform some fast preparation.\n",
      "So, instead of an array of just strings, transform it into an array of objects. So, instead of just a URL as a string, make it an object and pass it URL prop and then close it, and then URL prop again for the next one and close it.\n",
      "Okay, to make this work with our current code, we'll simply alter the url because instead of passing in a string, we'll just pass in the object and access the url of this object, which will be the url that we defined.\n",
      "\n",
      "Also, the last section must be commented out because it is unnecessary and not required as of now.\n",
      "\n",
      "Let's start actually writing some code, so let's say you want to download something from a website, say an image, a word document, music, or a pdf, you will basically need to create a file stream within node.js so that it can put data into it and build the actual file, so let's start and create a variable let's say it \"imageExtractFile\" which equals to file system and attach \"createWriteStream\" function and we only need to pass in a parameter related to the actual path of the file that we want to create, so keep in mind that we are in a for loop that iterates over each of the URLs that we are using, so we must use a dynamic file name; we can't just use \"some_name.jpg\" because it will be overwritten in the second call, and we only have one file.\n",
      "Before we go any further, we need to send in an id or anything for the actual file that we're going to utilize, so let's go to the top and add id, and simply pass in the movie name of this movie or something unique name, like \"the godfather\" and \"the godfather 2\".\n",
      "\n",
      "Now we can use these ids to create the actual file with these names, so let's go back and do that right here. We want to create a file with the name of the movie dot id and then we're going to put it as a jpeg. We need to specify it because we don't know what type of file it is when we're actually downloading it, and this will make it easier because we can find out what type of file it is from either the URL or from the request response, but for now, we'll presume it'll be a jpeg because we already know movie posters are jpeg, and we'll just pass in dot jpg. Now that we have this, we can test it out. However, before you do that, your code should look like this.\n",
      "and we should expect to see two files created on the system with the ids of the ‚Äúthe_godfather.jpg‚Äù and ‚Äúthe_godfather_2.jpg‚Äù, so let's do this quickly to make sure everything works, and then let's return to the files, where we find \"the_godfather.jpg\" and \"the_godfather_2.jpeg,\" which is an empty file because we didn't pass any data into it.\n",
      "\n",
      "\n",
      "Now we need to take care of the request that goes to the server for the actual movie poster and then stream the data into our newly created files, so let's look at a quick example of how to stream data into a file. You need to make the same request as before, pass in the URL, and then use the pipe function to pass in the data to the newly created file that we previously did also with ‚ÄúcreateWriteStream‚Äù.\n",
      "\n",
      "This is very simple, so let's get started. First, let's create a new variable called \"streamImage,\" and then we'll use the request library to do the same thing we did previously in the URI, passing in the \"movieUrl.imagePoster\" which holds the image's actual link, and then for the headers, just copy-paste what we did above copy all of the. Copy all of the headers and paste them below. We only need to remove the HOST part of the code because the host isn't the IMDB main site URL; instead, the posters are using a S3 bucket from Amazon, which will cause some issues, so we'll just remove it. Also, don't forget to add the gzip. All right, right now we just want to pipe it out, so pipe it and then specify the actual imageExtractFile. Finally, here's what your code should look like now.\n",
      "We actually have the code written and ready to go, but first, comment out the second movie because we only want to run it for the first movie. The reason for this is because we're in a for loop, and what's happening is that it's making the first request for the first movie, waiting for it to finish because we use the await syntax, and then it's going to make a request for the imagePoster is going to get saved to the file, but it will not wait for the entire process to complete before continuing; instead, it will immediately return to the top and make the second request, after which it will return to the bottom and which will collide. Hence, run it for a single movie and then deal with the issue later. Let's put it to the test and see how it goes, so fire up the debugger.\n",
      "\n",
      "\n",
      "Now that we have the actual image downloaded, it appears that the_godfather.jpg is the actual image poster that we intended to extract. It completes the task as planned.\n",
      "Let's keep going with the image downloading part that we left off and see if we can address the problem we're having, so the main problem was that the downloading process starts, but it doesn't wait for it to finish before continuing. So, before we go any further, let's deep dive into ‚Äúwhat is promisifying?‚Äù, so promisifying is the process of converting a non-promise function based on callbacks into a promise-returning function.\n",
      "So let‚Äôs start fixing this issue, by default in node.js you can create your promise here is a quick example.\n",
      "Let's imagine we're waiting for a new promise, and this function only has two parameters: resolve and reject. Let's say we have a variable called alien, and we're going to build a quick if statement that says if an alien is true, resolve with true and if not, reject with false.\n",
      "Let's put everything into a variable and place a breakpoint at the end so we can observe what value that variable is spitting out. Let's run it quickly and test the value.\n",
      "\n",
      "We have true since we checked to see if the alien is true and if it is, we resolve the promise using the function true, passing a string inside resolve, and then running it again, this \"promisifyStuff\" variable will be the exact string.\n",
      "Let's see if we can quickly test it for the false as well, and see what we get when it rejects. Let's add a random string message in there and run it again, and we now receive an unhandled promise rejection error.\n",
      "\n",
      "That's because we didn't catch the actual error. The reject throws an error, which we need to capture by wrapping it in a try-catch, then catching the error and console. log the error, and if we test again, the false message should be console logged.\n",
      "\n",
      "This was only a quick introduction to javascript promises, and it is highly advised that you investigate them further.\n",
      "Let's return to our objective and start implementing this into what we need to accomplish. Basically, we need to wrap around this new promise thing into our stream request and let's get started right away. We'll just await a new Promise with resolve reject param and put it at the top of the streamImage, and then we'll end/wrap the promise. Now we need to figure out when the actual stream is completed. We can figure this out by adding an on() listener to an event. The event that we need to listen to is \"finish,\" since after the request library is finished with the actual request, it will throw a finished event, which we need to grab onto. Now we'll just open up a function and declare a console. We'll log \"some message or whatever custom message you want,\" and then we'll specify the resolve, which we'll say is true or you can leave them empty because we don't have to catch the message and we don't have to utilize it, so leaving it empty is fine. This is what your code should look like.\n",
      "If we execute this, the scraper will go to the first movie and ask for the details, parse them, and then go to the \"streamImage\" and start downloading and waiting for it to finish, before repeating the process for the second movie. Let's run through the procedure quickly and see what happens. We should see a message that says \"Movie Poster Image downloaded\" and then another message that says the same thing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finally, we're almost done, but we need to handle the fact that it can fail and throw an error, so let's create another on listener and just listen for an error, and then we'll have a function that takes an error parameter and then we'll reject the promise with the actual error that we get, and now that we've rejected the promise with the actual error that we get, we just need to catch it, so simply specify the catch method, then we'll receive the error, and then we'll console.log the error to record and display the error message itself.\n",
      "Finally, we can identify which specific movie an error occurred in by simply replacing the single quotes with backticks, allowing us to use the javascript syntax inside it to designate the individual ‚Äúmovie.id‚Äù of the error so that future debugging becomes really very simple. So this is how the final code should look like.\n",
      "Let's do a final check to see if we have a great and curated console error message. So far, everything is working fine, and we've learnt a lot and gone to the depths of scraping from the ground up.\n",
      "\n",
      "The complete source code is available here:\n",
      "https://github.com/aviyeldevrel/devrel-tutorial-projects/tree/main/web-scraping-with-nodejs\n",
      "In this article, we learned about scraping from the ground up, including Nodejs and the fundamentals of javascript, why and when to scrape a website, the most common problems with scraping, different scraping methods such as to request method and browser automation method, and finally how to scrape the data from the IMDB website in extensive detail, as well as how to export scraped data into CSV and JSON files. If you wish to pursue a career in web scraping, this article may be very useful.\n",
      "Follow @aviyelHQ or sign-up on Aviyel for early access if you are a project maintainer, contributor, or just an Open Source enthusiast.\n",
      "Join Aviyel's Discord => Aviyel's world\n",
      "Twitter =>https://twitter.com/AviyelHq\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with NodeJS: a comprehensive guide [part-2]\n",
      "URL: https://dev.to/aviyel/web-scraping-with-nodejs-a-comprehensive-guide-part-2-43lk\n",
      "Answer: let's take a closer look at what happened and why it didn't work. This is a frequent mistake that rookie scrapers make when playing with requests; it happens to everyone. You may have also noticed the gibberish encoded response; this is due to the IMDB's usage of the gzip compression algorithm.\n",
      "\n",
      "This is done to reduce the size of the response data so that it may be returned faster and save a lot of bandwidth at the same time. Let's get started with this. We can address this by adding an extra parameter to the request function called gzip and setting it to true.\n",
      "\n",
      "So this will inform the request library to decompress this actual request from IMDB using gzip compression. Let's fast test it out and see if it works, so run your scrapper again.\n",
      "\n",
      "As you can see, we already have a readable <!DOCTYPE> HTML response, as well as everything we had before, and the title and ratings work perfectly.\n",
      "\n",
      "Let's go back to the dev tools, which we already have open, and search for the content encoding on the response headers from the website, which you can see here it states content encoding with the value of gzip, indicating that this website indeed uses gzip compression algorithm.\n",
      "\n",
      "An in-depth look at jQuery selectors and data parsing\n",
      "Let's learn a little more about jquery selectors and where to go for more information about them. It's quite normal and acceptable to not remember all of them. Jquery selectors This is why you should always have the documentation on hand. Let's go to the browser and search for jquery selectors. As you can see, there are a lot of results. Let's go to the w3 school link. It's not the official one, but it's a very popular website that has all of the selectors that we need. This is what we'll use to search for selectors that we need to scrape some parts of. website.\n",
      "\n",
      "\n",
      "Now, let's get started scraping. Go to IMDB movie page and open the dev tools. Now that we have the movie open, our editor running, and our dev tools open, we can start scraping. There may be some errors, but we'll fix them as we go. By using this method, you'll be able to see the entire thought process behind it, so let's get started.\n",
      "First, we have the title and the movie rating. Let's assume we want to retrieve the movie poster. To do so, go to the movie poster and check elements, looking for anything unique about it, such as an id or a class name that isn't repeated, or just discover the selector.\n",
      "\n",
      "So the first thing that pops up is the div with the class name of ipc-poster, which we can easily get and then access the element and then the image within it. First, let's clear everything in the console, and because this website already has jquery implemented, we can check the jquery code that we're going to write directly in the console. If the website doesn't have jquery implemented, you can inject it and also check it in the console In the later section of this article there is a demo showing how to inject the jquery library inside the browser itself, but just so you know, if you don't see the console tab with the console, you can easily click on escape to toggle between them on and off.\n",
      "\n",
      "Let's write the code for this image selector. First, we'll select the div with the class name ipc-poster or ipc-media, and then we'll select the child within it, which is an IMG tag, which is the image. We can now expand this element and see that we have an image, so as you can see on the image below it appears that this selector is working perfectly fine.\n",
      "\n",
      "\n",
      "Now that we have a proper and working selector, we need to choose the source of the image to get the actual link of the image, so for that, we can use the attribute function and pass into the parenthesis the exact attribute name that we want to select, in our case, it will be source ‚Äúsrc‚Äù let's try it out so type attribute and then src enter and it should work.\n",
      "\n",
      "Finally, let's put this code to the test in our editor by copying the code from the browser console and pasting it into a new variable called imagePoster inside our code editor. This should work the same, so let's console log the imagePoster and remove the breakpoint because we don't need it any longer (you can keep it if you want to check it). So, the code in \"index.js\" should look like this.\n",
      "Now go to the debug and start the debugger\n",
      "\n",
      "As you can see, the title The Godfather has a 9.2 rating, and we also have a link to the image source. Before we go any further, if you have a white space issue in the movie title, you can easily remove it by using the .trim() after the text() method, which will also eliminate the white space between the text.\n",
      "Let's keep going and proceed to the next step. Okay, so now that we have the title, the poster, and the rating, let's see how much amounts of rating this movie has. Let's right-click on the rating amount and inspect the element first.\n",
      "\n",
      "What we can see is that there is a div with another tag of div with the class AggregateRatingButton__TotalRatingAmount but nothing useful. We need to go a little higher because we only have the ipc-button__text class at the moment. So we can finally check this and grab onto it, acquire the link, and then obtain the text within it. As a result, let's clear the console and start writing code for it.\n",
      "So first get the div with the class ipc-button__text, then get the last child, which is another div tag, and finally the last div child, so we can finally obtain the text, and click enter and check if it works.\n",
      "\n",
      "and, as you can see from the image above, it works; as you can see, it displays the precise rating number. Now, let's quickly put this into action, so open the editor and create a new variable called ratingAmount, then paste the following code into it.\n",
      "We can immediately test it by logging the ratingAmount in the console and hit play to the debugger.\n",
      "\n",
      "As you can see, it's functioning perfectly, so let's continue and try to get the movie's release year as well. To do so, repeat the previous method and inspect the element once more to see what we have.\n",
      "\n",
      "So, there's a list tag with the class ipc-inline-list__item, and inside it, we'll select a tag and quickly test it in our console to see whether it's functioning.\n",
      "\n",
      "Finally, it works, and we can easily add it to our code editor and use it in our code. Let's copy this and check that it's the text that we want, but in your case, you might see some white space that can be easily removed using the trim function, and we'll also use slice to chuck the exact year only that we need.\n",
      "So make a variable called releaseYear, paste the code in it, and slice it up.\n",
      "Let's put it to the test right away.\n",
      "\n",
      "It works perfectly, and we have the title, the rating, the poster, the number of ratings, and the year of release. Now that this is starting to look pretty good, let's attempt something a little more difficult: let's try to get all of the movie's genres in an array rather than as text; we want to have an array with crime and drama.\n",
      "Again, inspect the element and check it out, as you can see in the image below.\n",
      "\n",
      "As you can see, all of the links to certain genres, such as crime, have a genre link in common, so this prefix /search/title/?genres is present right here in the drama, and we can latch on to it and choose all of the links that have the prefix of the href tag /search/title/?genres.\n",
      "\n",
      "Let's try it out and see if it works; if it does, that's great; if it doesn't, and we're receiving more links than we need then it is because the IMDB page can have numerous links with these types of prefixes.\n",
      "So first, let's empty off the terminal tab and start writing the code. We want to get all the li tags with data-testid=\"storyline-genres that have a tag with href equal to /search/title/?genres, but we don't want them to start with the prefix genres only but with /search/title/?genres. So, let's head over to the jquery selectors documentation and look for the starting keyword.\n",
      "\n",
      "As you can see, we have something right here: all elements with the title attribute value beginning with tom. All we have to do now is add a ^ sign in front of our equal sign, and it will get all the elements that start with the /search/title/?genres prefix. Let's add it and then close it and get all the text just for testing purposes.\n",
      "\n",
      "As you can see, it's not displaying the results properly. To fix this, we need to narrow down the search and try to select only the links within a specific parameter, more specifically within these li links. We can easily add this selector so that we can refine our search to get only the links within this parameter. You might be wondering how we do this. First, let's clear the console. Now, right in front of our selector, we need to add the li selector for the data-testid of storyline-genres, and this says that we want to select all the href elements that start with /search/title/?genres that are contained inside this list and let's test it out. Copy and paste the following code into the console.\n",
      "As you can see, the selector works perfectly, and crime, drama is perfect for the time being. But how do we put this into an array instead of using the text function? We'll use each function, which will iterate through all of the selectors that it finds, and by the way, you can check the documentation for cheerio on GitHub. Let's try it out, but this time instead of text, we'll use each function, which has two parameters: one for the index and the other for the actual element we'll utilize. close it and now that we've done that, we'll make an array called movieGenres and start it as an empty array, and then we'll access the specific element and get their text and insert it into the movieGenres array. We can do this easily by making it like genre equal access the element and then the text inside it, and then we'll just push it to the array movieGenres, and then let's console log movieGenres and yeah it works perfectly, we have an array of crime and drama. This is what your code should look like.\n",
      "If your code throws an error or doesn't work, you'll need to load jquery into the browser. To do so, open the console and paste the following code.\n",
      "\n",
      "Again, test the previous code in the console; you should see something similar to this.\n",
      "\n",
      "Finally, let's add the code to our scraper and see if it works. Also, copy this code and paste it into the code editor, then console log movieGenres and run it again. As you can see, it works well, and we have an array of crime, drama parsed precisely like in our chrome console.\n",
      "\n",
      "This is what your index.js source code should look like.\n",
      "In this section, we'll rapidly learn a new way for scraping several movies from an array of given URLs and saving the data we collect as JSON in a file, so let's get started. The first thing you'll want to do is turn the constant into an array.\n",
      "\n",
      "So now we have multiple URLs, let's open another movie link on the IMDB website, copy the URL, convert it to an array, and input another movie, and that's all there is to it.\n",
      "\n",
      "So, right now, we have two movies that we want to scrape for information. To do so, simply go inside the defined asynchronous function and follow/paste the code as shown below.\n",
      "This loop will iterate over each of the URLs that we created previously. Now that the movie's URL is going to be URLs instead of URL, we need to alter the URI and assign URLs, and of course, this should work. We should give it a try right now and see what we have.\n",
      "\n",
      "As you can see, the first request is made, and it outputs \"The Godfather,\" the rating, and everything else we've got, and then it waits for the rest of the request to finish, and then it makes the second request to The Godfather Part 2 movie, and it outputs it to the console, so everything is working as it should. Let's prepare the data and save it as JSON into another variable. Let's start by defining moviesParseData as an empty array, then push in that array exactly before the console log, so that we have an array with all of the scraped movie results. We're using moviesParseData to add another entry to this array, and we want it to be an object. So, let's push the title, the rating, the poster, the number of ratings, the year of release, and, of course, the genres. As a side note, this is a shorthand strategy, but for the time being, let's keep it simple because it looks cleaner.\n",
      "Okay, now let's test what we did. We should anticipate this array to be populated with all of the details of both movies. Let's comment out all of the console log and then console log moviesParseData to access all of the movies. We need to put a breakpoint in that console log so that the debugger stops right at that console log. so now, let's test it out and it does indeed stops right at the breakpoint line all right then let's take a look at the console so we have an array hence expand the array and then we have the length of two objects exactly what we expected as you can see the title The Godfather and The Godfather part 2 and its rating, poster, and everything else.\n",
      "\n",
      "Right now, we have two movies that we scraped, and we'll continue by saving this data to a file. First, let's close the debugger, and to save files to the system, we'll use a native library from node.js called fs( file system). Currently, what we want to do is import the library, which we can easily do without having to install anything because it's already implemented inside the node.js itself.\n",
      "Now that we've got the library and the movie's data in an array, we can create the method to save it to a file with ease. We're going to use the writeFileSync fs method. Three parameters are required by this function. The first is the actual path to the file that you want to write, the second is the content, and the third is the type of information that you want to save the actual encoding. Let‚Äôs save it as movies.json and the content inside it to be the moviesParseData in actual string form, so we'll use JSON stringify and pass in the moviesParseData array. This will transform the array into an actual JSON string and finally, we are going to save it in utf-8 encoding/format.\n",
      "Before we start testing this, let's look at why we used the writeFileSync function. We're using this function with the sync after it because it tells node.js that we want to wait for this line to finish before moving on to the next one, which is similar to writing await in front of it but it doesn't work like this because the fs writeFile function doesn't return a promise, so await and async only work with promises.\n",
      "The next step is to start/run the debugger, and we should expect it to produce a movies.json file in our directory. So check it out, and everything is just as we expected it to be. We have the movies.json file, and all of the JSON material is placed in this file.\n",
      "This is what your final code should look like.\n",
      "Let's first learn how to export the scraped data to a CSV file if you're working with them and need to use them in any way. A CSV file is a comma-separated value file in which the first line represents the keys and the subsequent lines represent the rows with the values for those specific keys, with each value separated by a comma. While creating a CSV file is not a difficult task, it is preferable to use a well-maintained library that does the job with two or three lines of code rather than writing and maintaining your own.\n",
      "So you can easily develop a CSV generator, but if you want to get things done quickly and easily, which is what most people want, you should use something like json2csv, which is a npm package that you can use straight with node. So go to json2csv and then to its npm and github repository.\n",
      "\n",
      "As you can see, we can install it by simply typing the following command into our code editor's terminal.\n",
      "Now that we have this library, we can finally utilize it in our code, so go to the index.js file. Close the terminal, and instead of saving the data as JSON like we did before, we want to convert it into CSV data and then save it to the file, so for now, comment out the console log and return to the documentation to see how to import/use this library, as shown in the image below.\n",
      "\n",
      "Copy the import command and paste it at the top of the code inside your code editor. Now, let's return to the documentation and look at an example. We have a great set of examples that could work for our situation.\n",
      "\n",
      "So we have an array of objects and we want to convert them to CSV data. In our case, we have two movie lists with multiple data in each of them, and we're going to convert those to CSV data. By looking at this example, you'd need to specify the fields of the JSON object that you want to have converted, and then basically input your data into it. So let's go ahead and test this out ourselves.\n",
      "So, first, right before the console log, define a constant variable named csvDataKeys, and let's say we only want the title and the rating, imagePoster, and rating amount only from the data we have, hence copy and paste the exact following code into your index.js file right at the bottom before console log.\n",
      "We'll use the moviesParseData array to hold all the data, and if we did everything correctly, we should have a working CSV and instead of logging moviesParseData console log the actual CSV data, so let's start and run the debugger and see what we have. Of course, as you can see, it works properly; we have the title and the rating, imagePoster and ratingAmount, as the keys and the actual results of the moviesParseData.\n",
      "\n",
      "The only thing left is to save them in a file, which we can do easily with the command fsWriteFileSync, but instead of writing the data as movies.json, we'll write it as CSV, so uncomment the line and replace it with movies.csv, and instead of JSON.stringify, output the CSV and save the file, and your final code should look like this.\n",
      "After that, we should have a file generated, so let's run the debugger, and sure enough, we have a movies.csv file with all of the data we require.\n",
      "\n",
      "\n",
      "One small tip: if you don't want to provide any fields, or if you don't want to specify all of the fields, and you want to obtain everything from the array, you can simply remove everything from the array, and the code should look like this.\n",
      "Follow @aviyelHQ or sign-up on Aviyel for early access if you are a project maintainer, contributor, or just an Open Source enthusiast.\n",
      "Join Aviyel's Discord => Aviyel's world\n",
      "Twitter =>https://twitter.com/AviyelHq\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with NodeJS: a comprehensive guide [part-1]\n",
      "URL: https://dev.to/aviyel/web-scraping-with-nodejs-a-comprehensive-guide-part-1-16g8\n",
      "Answer: This blog is about the Node js course on web scraping, it is divided into three-part series, where you will learn how to scrape any type of website and by using a real-world example. This blog will teach you strategies and practices that you won't find anywhere else. You'll be able to quickly grasp all of the ideas and move from the examples and also, you will be able to make your scraper by the end. This blog post is aimed to assist anyone interested in learning web scraping with NodeJS because the issue with any other blog tutorials is a little challenging and you don't always find everything you need in one place.\n",
      "The sole and only requirement you need is that you have to have a basic understanding of Javascript or are at least familiar with it, as this will be the only programming language we are going to utilize. I would also recommend reviewing the async and await syntax in es6 and higher, as we'll be using it a lot.\n",
      "Also, if you're wondering what sorts of tools we will be using then, we will be utilizing vscode which is a free code editor that also supports NodeJS debugging without the need to install extra complicated plugins. We'll also utilize a variety of libraries, as well as different libraries, but we'll mostly use puppeteer, which was built and is maintained by the Google team, and also we will be using NighmareJS as well.\n",
      "Before we begin creating our scraper program, we must first set up our environments by downloading our editors and NodeJS, as well as complete some basic project setups before we can begin writing code. So first, go to the official VS code website,code.visualstudio.com, and download and install the VS code editor which is available free of charge. The key reason to use Visual Studio Code is that it is free and comes with a built-in debugger for Nodejs, making debugging a hundred or even thousand times easier.\n",
      "\n",
      "After installing our VS code editor, we must ensure that we have Nodejs installed to run our Nodejs application on our machine. Nodejs can be downloaded from nodejs.org and is compatible with both macOS and Windows, with a simple installation process. All you need to do now is download the package and follow the simple installation instructions.\n",
      "\n",
      "Also, if you want to see what version of Nodejs you have installed, go to the terminal/command line and type node -v which will tell you what version of Nodejs is currently installed on your computer. If this command doesn't work, make sure you restart your computer after installing Nodejs.\n",
      "Now that you've completed this, go to our VScode and open an empty folder in it, then open the terminal inside it and run the npm init -y command to initialize our project. So now you can see that a new \"package.json\" file has been produced within that folder, and we can just start installing and using our libraries right away and start making use of them. Also, inside that folder, create a new file called index.js, which will be our entry file, and now we're ready to write some code inside it. This is how your file structure should now look.\n",
      "\n",
      "We'll be building a simple IMDB scraper that parses the data from the IMDB movie page. This is just one of many real-world examples we've included in this particular section of the blog to give you gist and an idea of what can be accomplished in a short amount of time, so don't worry about getting too many details in this example because we'll go into more depth in the upcoming examples.\n",
      "So we'll use NodeJS to send a direct request to the IMDB website and expect a response that looks exactly like the image below. To begin, navigate to the IMDB website, right-click, and then select View page source.\n",
      "\n",
      "\n",
      "As you can see, this is the exact HTML content with Javascript and CSS that we will scrape using our NodeJS scraper. Following that, we will use Cheerio JS, a Nodejs library that can handle HTML content and has a syntax that is nearly identical to jQuery. It would be preferable if you were already familiar with the jQuery syntax. To begin, we must import both the request-promise library, which is in charge of managing requests to the IMDB website and the Cheerio JS library, which will be used to parse the HTML contents.\n",
      "Now, make sure you're in the index.js file that we previously created, and begin importing the actual libraries inside it.\n",
      "The next thing we're going to do is go to the IMDB website and copy the URL of whatever movie you want because we're going to send the request to that specific URL, so simply create a variable named URL and paste that specific copied URL link inside it.\n",
      "\n",
      "Because we cannot write the asynchronous function in the index unless it is declared as async, we must now write the simple async function that will be accessed when the NodeJS scrapper is fired. Simply create a nameless asynchronous function that will be executed automatically. Before we write anything inside that async function, we must first install the request-response and cheerio libraries. To do so, go to the terminal and type the following command.\n",
      "After installing the package, it should look something similar to this.\n",
      "\n",
      "So, now that we have everything installed and ready to go, we can use the request library. To do so, create a variable called response and inside it simply wait for the request to be completed while also passing the URL as its parameters. To test the response, simply console log it; we should be able to see the raw response. So, to run it, go to the debugging tab and press the run button.\n",
      "\n",
      "As you can see, it worked; we got what we needed, which means the script was successful, and we can now begin passing our response to the cheerio library and using it to go through each of the HTML properties and find out exactly what we need.\n",
      "First, let's get rid of the console log and implement the cheerio library.\n",
      "We simply created a $ (dollar) variable responsible for the cheerio with the actual IMDB response. Now we can begin writing the scraping for the movie title. First, go to the movie that you want to scrap. So, right-click on the title and select Inspect Element.\n",
      "\n",
      "Then we have the div element and inside it, there is an h1 as well as span element, which contains the title of the movie as well as the ratings of the movie. We can select the element by using the jQuery syntax-like selector as shown in the code below.\n",
      "If you select the debug option again, you should see something similar to this.\n",
      "\n",
      "So, now that you have enough information to get started with web scraping, let's delve into much more detail.\n",
      "So, before you begin creating a scrapper for your website, you should ask yourself\n",
      "So, if you ever find yourself in a situation like this, where you believe you will not benefit from an official API due to the reasons stated above, or if there is a specific website that does not even have an API, you should consider creating a scrapper.\n",
      "What we did previously is a perfect example, where we wrote a straightforward IMDB scrapper. Because IMDB does not have an official API that is accessible to the public, we relied on scraping the data. Of course, the scrapper that we wrote is very basic, but it demonstrated the possibility and power of scrapping with Nodejs. To give you a hot tip, we will explore the IMDB scrapper and write an even more complex scrapper later on.\n",
      "Before we begin, we must understand when it is appropriate to scrape data from a website. Keep in mind that web scraping is not always an ethical solution, nor is it always a black hat solution; it falls somewhere in the middle. That is to say, web scraping is not illegal, but it can get you in trouble if you violate someone else's website or organizational policies. So, before you plan to scrape a website, you should look at the terms and services of that particular website and see if they have anything related to scraping the data from their website; if they do, it means they may not want you to do it, and if they don't, it means they don't care if you scrape their website or not. Also, before you scrape, you should ask for permission to scrape it. Also, before you start scraping other people's or companies' websites, you should respect their data by using official APIs whenever possible, not spamming their website with an excessive number of requests, and finally, if you want to monetize the scrapped data, always seek legal advice and make sure what you're doing with it is completely legal.\n",
      "\n",
      "The most difficult and inconvenient aspect of web scraping is the maintenance and stability of the scrapper. These are the issues that you may have to deal with when building a scrapper. Scrapers can be useful for a variety of things such as extracting data, parsing, and so on. Let's say you wrote a scrapper and it works fine until it doesn't and you encounter some random error, which is exactly the problem, so it can work for one day, one month, or even one year before failing. The main issue with this is that the website that you are currently scraping can constantly change, its structure can change, and their system can change, and also the URLs of the website As a result, you have no control over it, and your scrapper may fail at any time due to this issue. When writing or coding scrappers, the logic and workflow are based on the current website you are attempting to scrape and its structure, so if the website decides to change its entire structure, you may have to change the entire structure as well as the logic of the scrapper as well. Of course, if you still want to make it work, you may be wondering how to solve this type of problem. The short answer is that you cannot change this type of problem because you have no control over the website you are attempting to scrape; you must simply deal with the problem that arises. This is why you must learn how to develop a scrapper quickly and efficiently, as well as how to debug and fix problems. This type of problem can occur on both a small and large scale, so you must be prepared at all times.\n",
      "\n",
      "In this section, we'll go over the request-promise library, what you can do with it, and when it's best to use it. So, what exactly are we able to do with the request library? We're incorporating this library into our earlier project that we did. We use this library because the request library allows us to submit requests to the server in the simplest and fastest way possible. Before we begin, let's look at some examples. So, when you visit a website, a basic GET request to the server is sent first, followed by the initial content, the HTML response. So, with the request library, you can do the same thing, but instead of using the browser, you can write the action in Nodejs and it will do everything for you.\n",
      "Let's take another example: when you want to login and enter your username and password into a specific website, a POST request is sent to the server, which sends the details of your entered account to the server for confirmation. This can also be done manually in Nodejs by simulating every or any request the browser makes to any website; all we have to do is provide the right parameters to it. In the case of the IMDB scraper, we used it as a GET request to obtain HTML and parse it.\n",
      "Since you control every parameter that you send to the server, it can be a little overwhelming at times. Let's use the previously described login process as an example. So, as previously described, the login process can consist of a single simple POST request to the server with the username and password depending on the website, followed by a single response with some cookies or a token in such case the request method is ideal, or the login system can consist of multiple requests on a simple login form on some websites can automatically send multiple requests for security reasons or because of how they were originally built on and In that case, you do not want to use the request library but of course, it is feasible, but it is very time-consuming and can be extremely frustrating, and many things can go wrong, such as missing simple parameters in the request headers and the server you are currently attempting to reach refuses to accept it. It all depends on the situation, but it is strongly discouraged to use this library if you have a large number of requests to send. Hence, if the website is more complex and automatically sends AJAX requests with different parameters and tokens, the best method would be to use the headless browser, which we will cover in detail in the following upcoming sections.\n",
      "Therefore, only in simpler times, you should use the request library, but when the website has loads of security behind it and is dynamically rendered, you should probably use another method or even a headless browser method.\n",
      "\n",
      "In this section, we'll deep dive into browser automation and how it might be applied for developing a scraper. But first, let's define browser automation. Browser automation, in our case with the help of NodeJs, essentially means controlling the browser using code. Now that you know that certain browser engines support this, you can see that you can't just automate your regular browser; instead, you'll need a browser that allows you to manage it using code, and we'll look at a plethora of examples in the upcoming topics ahead.\n",
      "\n",
      "Before we get started, let's quickly go over the benefits and drawbacks of using Browser automation. For starters, it's much more beginner-friendly, and it's very easy to understand the action of the steps that you need to take because they're the same as when you browse the internet; all you have to do is write the specific code and scripts that your automated browser will follow. In most circumstances, implementing the scrapper with the automated browser is much cleaner, and you may wind up writing less code than you would with the request approach, but this, of course, depends on the page that needs to be scraped and what you need from it. The first disadvantage of this request approach is that you are essentially relying on the API availability of the individual browser you are automating. Others browsers have limited capabilities, and some aren't very stable, and some aren't even updated anymore, which is why you should be cautious and thoroughly study the browser before using it in your scrapper. So, before you decide whatever type of browser you want to automate, the browser's documentation will usually give detailed information.\n",
      "To begin, you must understand that there is no right or incorrect option. Any website may be done using requests, and the other way around. It all depends on how long it will take, how much code you will write, and how successful it will be. The browser automated scrapper will use more bandwidth and resources to load the page content from the website than the request method because the browser will load every CSS file, every javascript file, and every image that is on the website, whereas the request method will only get the HTML code for the website itself and will not load the external contents like files and libraries. So, if bandwidth and a few milliseconds of delay aren't important to you, browser automation is an excellent and perfect option for you. Browser automation makes things a lot easier while also saving you a lot of time.\n",
      "Before you begin, you must first decide which libraries to use. There are two excellent libraries available: Puppeteer and NightmareJS. There are many more libraries, although many of them are closed or abandoned.\n",
      "Puppeteer is built on the Chrome browser and is also known as a headless version of Chrome. It was created specifically for automation, testing, and testing chrome extensions, among other things, but in our case, we will be using this library for scraping. This library is developed and maintained by the Google Chrome team and is a fully functional and up-to-date headless browser.\n",
      "NightmareJS, on the other hand, is the electron browser's driver. It's a lot of fun to learn and even more fun to use, but it's not particularly suitable for complex scrappers. When compared to the puppeteer library, it has a lot of limitations. One of the library's biggest flaws is that it doesn't allow numerous tabs and links to open at once. As a result, libraries like this may break your scrapper or drive you to make compromises when you need them.\n",
      "So, before you start scraping, let's go over a few things you might need to know. When you're running the scrapper and testing it, you can turn on the visual browser to see each action as it happens in real-time. This helps you understand and debug when you have a problem or when you're building a new scrapper. A competent headless browser will provide you with practically all APIs, allowing you to automate almost everything a user can do but by using the power of only coding and programming.\n",
      "In this segment of the course, we'll delve a little deeper into the IMDB scraper that we constructed in the first session. We'll make it a little more complex as we go, and we'll learn new things along the way. With the request method, we'll learn how to spoof or fake user headers. So the main question is \"why do we need to spoof them?\" It's because we want it to appear that the scraper is a browser that's making the request. Request headers are extra parameters that the browser sends to the server automatically. They usually contain cookie information, such as whether you're logged in or not, and other types of browser information.\n",
      "So, let's get started with how to check these. First, open the browser and right-click to open the developer tools. Right now, we need to go to the network tab to see all of the requests that are happening in this tab.\n",
      "\n",
      "We may see a number of requests and their types here. There's the document, as well as images, graphics, style sheets, javascript, and a whole lot more.\n",
      "\n",
      "Let's take a look at the initial request that's being made, as you can see here. We can see the general information and the response headers, but we need to look at the request headers, which are a little farther down. Now we need to go to GitHub's request promise website and look at their documentation to see how we can include those in our request.\n",
      "\n",
      "Here => https://github.com/request/request-promise\n",
      "What we need to look for is a way to add those extra parameters throughout the request, and if we look closely enough, we'll see the header options.\n",
      "\n",
      "We'll copy the header and paste it into our VS code editor. Right now, we only have the URL as a parameter, and we need to alter it so it's an actual object, so delete the URL and build an object, but we'll still need to input the URI URL and then paste in the headers option.\n",
      "Right now, we have precisely what we had before with the addition of a user parameter to the request with the value request promise. Of course, request promise isn't a user agent, but it's something we can simply adjust based on the documentation provided.\n",
      "Let's go ahead and obtain the request headers now. Let's go back to the dev tools and look at the first request, making sure it's the one that's responsible for the IMDB page and not an image or a javascript file. Then, just like before, look at the request header and copy everything for now.\n",
      "\n",
      "Copy everything and return it to the editor. What we have now are the request headers that are sent by the browser when we enter the IMDB page. What we need to do now is convert them all to javascript objects and pass them on instead of the previous useragent. Let's format them properly and replace them, indent them properly. Now we have control over all of the requests that are sent by the browser are being sent by us. Finally, we only need to worry about the cookie because we don't need it in this situation, so let's erase it, and we're done.\n",
      "Now we have the user agent, which is specific to the computer that you're using to code this. You can simply modify this on the user agent and check phoney ones on the internet and paste them right here; you don't need to paste in your actual browser info. Finally, let's put this to the test to see if it still works. Go to the debug tab and select debug play.\n",
      "\n",
      "Now, let's hope for the best and head to the debug console, where you can see that it does not work, as it does not print the movie's title or rating. So, we'll use what we learned before and set a debugger right at the console.log line to see what's going on. Let's run it again, and it stops right at the console.log line, and we can see what variables we have. We have the rating, which is an empty string, and the title, which is also an empty string, which means it didn't find the selectors we were looking for because the response changed, as you can see and it is completely nonsensical.\n",
      "\n",
      "So, when we requested with only the URL, all of the other options were default, but now that we've added our own, everything is the default. We get this response because we forgot to add the gzip option to some of the default parameters for the request function.\n",
      "Follow @aviyelHQ or sign-up on Aviyel for early access if you are a project maintainer, contributor, or just an Open Source enthusiast.\n",
      "Join Aviyel's Discord => Aviyel's world\n",
      "Twitter =>https://twitter.com/AviyelHq\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping Sprott U Fund with BS4 in 10 Lines of Code\n",
      "URL: https://dev.to/cincybc/web-scraping-sprott-u-fund-with-bs4-in-10-lines-of-code-17fo\n",
      "Answer: I started my second career as a Nuclear Fuel Uranium trader around a decade ago. A few years in, I was frustrated with my company's refusal to upgrade systems beyond 7 spreadsheets with redundant information scattered throughout, so I started my journey learning about databases, data engineering, and learning how to automate things with Python. One of the datapoints I scrape currently as background, contextual data (until I get the time to put it into a component!) on my uranium-focused dashboard is data scraped from the market newcomer, Sprott Uranium Fund's daily updated website. Here is tutorial on how I do it using Python Package bs4.\n",
      "First we import our packages\n",
      "Then we request the website using the requests package. If the response comes back successful 200, we use BeautifulSoup to parse it.\n",
      "Congratulations! You now have the webpage locally in your computer's memory. But how do we extract their share price and the volume of Uranium the fund is currently holding?\n",
      "You can go to that URL and open up the Developer's view to look at elements, look at the source code for the whole page in your browser, or use BeautifulSoup's prettify() function to see it in your Jupyter Notebook with print(soup.prettify().\n",
      "You'll find the share price and Uranium volume about an 1/5 of the way down the page. Here is a sample of what I'm looking at:\n",
      "The values are stored in a div class called \"fundHeader_value.\" To get all of them and extract the ones with the share price and Uranium stocks, we use BeautifulSoup findall function storing it in a variable called fund_values (a list).\n",
      "The share price is the 4th value in that list, so you use Python list slice and call the contents function to get it in a way you can manipulate it in Python.\n",
      "If you print the variable shareprice, you'll get a lot of stuff you don't want in there.\n",
      "First thing, is that we want the contents of the first item in this list, so shareprice[0]. We then want to get rid of the other stuff around it, namely white spaces and key returns. To make sure we're manipulating a string object, we can tell Python to recognize it as a string with str(shareprice[0]). Python has a very powerful method for \"stripping\" away whitespace with .strip(), so we call that after our string str(shareprice[0]).strip().\n",
      "That gives us $US11.81 as a string. If that's what you want, you can stop there, but if you want to put it into a chart or store it as a number in a database, you need to also get rid of the $US. Luckily, Python has another method for \"replacing\" the part of the string you don't want with nothing. You just have to put .replace('$US','') on it and it returns 11.81.\n",
      "That was a long explanation for one line of text, but it shows how concisely Python can get things done!\n",
      "How about the Uranium volume? Easy...Rinse and repeat. The only difference is that it has commas instead of $US and is the 6th item in the list of fund_values.\n",
      "So there you have it, you have scraped the fund's website in 10 lines of code (12 if you count the extra 2 for the Uranium Volumes).\n",
      "Raise my dopamine levels with a Like. I'll try to write more technical stuff here.\n",
      "Here is the full code: (Find it here in Github as well)[https://github.com/CincyBC/bootstrap-to-airflow]\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with CSS Selectors using Python\n",
      "URL: https://dev.to/serpapi/web-scraping-with-css-selectors-using-python-4gdp\n",
      "Answer: This blog post is ongoing with somewhat regular updates. It's about understanding CSS selectors when doing web scraping, and what tools might be handy to use.\n",
      "At SerpApi we've encountering different types of selectors, some of them are quite complex that include complex logic, for example, logic may include selectors like :has(), :not() among other selectors, and we want to share a back a little of our knowledge gained during our journey while building our APIs.\n",
      "We want to point out that this blog post is not a complete CSS selectors reference, but a mini-guided tour of frequently used and more advanced type of selectors and how to work them while web scraping with code examples.\n",
      "A basic familiarity with bs4 library, or whatever HTML parser package/framework you're using as usage of CSS selectors in different languages, frameworks, packages are not much different.\n",
      "Install libraries:\n",
      "CSS selectors are patterns used to select match the element(s) you want to style extract from HTML page.\n",
      "Let's start with easy one, SelectorGadget Chrome extension. This extension allows to quickly grab CSS selector(s) by clicking on desired element in your browser, and returns a CSS selector(s).\n",
      "SelectorGadget is an open-source tool that makes CSS selector generation and discovery on complicated sites a breeze.\n",
      "Uses cases:\n",
      "When using SelectorGadget it highlights element(s) in:\n",
      "\n",
      "\n",
      "\n",
      "Since SelectorGadget isn't a magical all around tool, sometimes it can't get the desired element. This happens when website HTML tree is not well structured, or if the site is rendered via JavaScript.\n",
      "When it happens, we use Elements tab via Dev Tools (F12 on a keyboard or CTRL+SHIFT+C) to locate and grab CSS selector(s) or HTML elements by their:\n",
      "‚úçSyntax: element_name\n",
      "Type selectors matches elements by node name. In other words, it selects all elements of the given type within a HTML document.\n",
      "‚úçSyntax: .class_name\n",
      "Class selectors matches elements based on the contents of their class attribute. It's like calling a class method PressF().when_playing_cod().\n",
      "‚úçSyntax: #id_value\n",
      "ID selectors matches an element based on the value of the elements id attribute. In order for the element to be selected, its id attribute must match exactly the value given in the selector.\n",
      "‚úçSyntax: [attribute=attribute_value] or [attribute], more examples.\n",
      "Attribute selectors matches elements based on the presence or value of a given attribute.\n",
      "The only difference is that this selectors uses curly braces [] instead of a dot (.) as class, or a hash (or octothorpe) symbol (#) as ID.\n",
      "‚úçSyntax: element, element, element, ...\n",
      "Selector list selects all the matching nodes (elements). From a web scraping perspective this CSS selectors is great (in my opinion) to handle different HTML layouts because if one of the selectors is present it will grab all elements from an existing selector.\n",
      "As an example from Google Search (carousel results), the HTML layout will be different depending on country where the search is coming from.\n",
      "When country of the search is not the United States:\n",
      "\n",
      "When country of the search is set to the United States:\n",
      "\n",
      "Following examples translates to this code snippet (handles both HTML layouts):\n",
      "‚úçSyntax: selector1 selector2\n",
      "Descendant combinator represented by a single space () character and selects two selectors such that elements matched by the second selector are selected if they have an ancestor (parent, parent's parent, parent, etc) element matching the first selector.\n",
      "‚úçSyntax: selector|element:nth-child()\n",
      "The :nth-child() pseudo-class matches elements based on their position among a group of siblings.\n",
      "‚úçSyntax: selector|element:has(selector|element)\n",
      ":has() is a pseudo-class that checks if parent element(s) contains certain child element(s)\n",
      "‚úçSyntax: selector|element:contains(selector|element|text)\n",
      "contains() method is not completely related to CSS selectors but rather to XPath. It's returns true or false if there's a value in a substring of searched (first) string. A little confusing, let's show an example.\n",
      "Outputs:\n",
      "‚úçSyntax: selector|element:not(selector|element|text)\n",
      "The :not() pseudo-class is used to prevent specific items from being selected.\n",
      "The :not pseudo-class could used (chained) with contains() method to create a boolean expression which is really handy.\n",
      "Continuing with the previous example, we can select element that doesn't contains $ symbol in the text :not(:contains(\\$))::text:\n",
      "Outputs:\n",
      "Here's a more practical usage where we need to select everything (only category) that doesn't contains $ in the text string:\n",
      "Other useful CSS selectors:\n",
      "Additional useful CSS selectors you can find on W3C Level 4 selectors, W3Schools CSS selectors reference, and MDN CSS selectors documentation.\n",
      "To test if the selector extracts correct data you can:\n",
      "Place those CSS selector(s) in the SelectorGadget window and see what elements being selected:\n",
      "\n",
      "Use Dev Tools Console tab via $$(\".selector\") method (creates an array (list()) of elements):\n",
      "Which is equivalent to document.querySelectorAll(\".selector\") method (according to Chrome Developers website:\n",
      "Output from the DevTools Console for both methods are the same:\n",
      "\n",
      "Betting only classes might be not a good idea since they could probably change.\n",
      "A little more realible way would be to use attribute selectors selectors (mentioned above) they are likely to change less frequently.\n",
      "Attribute selectors examples: (HTML from Google organic results):\n",
      "\n",
      "Many modern websites use autogenerated CSS selectors for every change that is being made to certain style component, which means that rely exclusively on them is not a good idea. But again, it will depend on how often do they really change.\n",
      "The biggest problem that might appear is that when the code will be executed it will blow up with an error, and the maintainer of the code should manually change CSS selector(s) to make the code run properly.\n",
      "Seems like not a big deal, which is true, but it might be annoying if selectors are changing frequently.\n",
      "This section will show a couple of actual examples from different websites to get you familiarize a bit more.\n",
      "\n",
      "Test CSS container selector:\n",
      "\n",
      "Code:\n",
      "\n",
      "Testing .post-card-title CSS selector in Devtools Console:\n",
      "Code:\n",
      "\n",
      "Test CSS selector with either SelectorGadget or DevTools Console:\n",
      "\n",
      "Code:\n",
      "Join us on Twitter | YouTube\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping: Intercepting XHR Requests\n",
      "URL: https://dev.to/anderrv/web-scraping-intercepting-xhr-requests-1hke\n",
      "Answer: Have you ever tried scraping AJAX websites? Sites full of Javascript and XHR calls? Decipher tons of nested CSS selectors? Or worse, daily changing selector? Maybe you won't need that ever again. Keep on reading, XHR scraping might prove your ultimate solution!\n",
      "For the code to work, you will need python3 installed. Some systems have it pre-installed. After that, install Playwright and the browser binaries for Chromium, Firefox, and WebKit.\n",
      "As we saw in a previous blog post about blocking resources, headless browsers allow request and response inspection. We will use Playwright in python for the demo, but it can be done in Javascript or using Puppeteer.\n",
      "We can quickly inspect all the responses on a page. As we can see below, the response parameter contains the status, URL, and content itself. And that's what we'll be using instead of directly scraping content in the HTML using CSS selectors.\n",
      "Our first example will be auction.com. You might need proxies or a VPN Since it blocks outside of the countries they operate in. Anyway, it might be a problem trying to scrape from your IP since they will ban it eventually. Check out how to avoid blocking if you find any issues.\n",
      "Here is a basic example of loading the page using Playwright while logging all the responses.\n",
      "auction.com will load an HTML skeleton without the content we are after (house prices or auction dates). They will then load several resources such as images, CSS, fonts, and Javascript. If we wanted to save some bandwidth, we could filter out some of those. For now, we're going to focus on the attractive parts.\n",
      "\n",
      "As we can see in the network tab, almost all relevant content comes from an XHR call to an assets endpoint. Ignoring the rest, we can inspect that call by checking that the response URL contains this string: if (\"v1/search/assets?\" in response.url).\n",
      "There is a size and time problem: the page will load tracking and map, which will amount to more than a minute in loading (using proxies) and 130 requests :O. We could do better by blocking certain domains and resources. We were able to do it in under 20 seconds with only 7 loaded resources in our tests. We will leave that as an exercise for you ;)\n",
      "For a more straightforward solution, we decided to change to the wait_for_selector function. It is not the ideal solution, but we noticed that sometimes the script stops altogether before loading the content. To avoid those cases, we change the waiting method.\n",
      "While inspecting the results, we saw that the wrapper was there from the skeleton. But each houses' content is not. So we will wait for one of those: \"h4[data-elm-id]\".\n",
      "Here we have the output, with even more info than the interface offers! Everything is clean and nicely formatted üòé\n",
      "We could go a step further and use the pagination to get the whole list, but we'll leave that to you.\n",
      "Another typical case where there is no initial content is Twitter. To be able to scrape Twitter, you will undoubtedly need Javascript Rendering. As in the previous case, you could use CSS selectors once the entire content is loaded. But beware, since Twitter classes are dynamic and they will change frequently.\n",
      "What will most probably remain the same is the API endpoint they use internally to get the main content: TweetDetail. In cases like this one, the easiest path is to check the XHR calls in the network tab in devTools and look for some content in each request. It is an excellent example because Twitter can make 20 to 30 JSON or XHR requests per page view.\n",
      "\n",
      "Once we identify the calls and the responses we are interested in, the process will be similar.\n",
      "The output will be a considerable JSON (80kb) with more content than we asked for. More than ten nested structures until we arrive at the tweet content. The good news is that we can now access favorite, retweet, or reply counts, images, dates, reply tweets with their content, and many more.\n",
      "Stock markets are an ever-changing source of essential data. Some sites offering this info, such as the National Stock Exchange of India, will start with an empty skeleton. After browsing for a few minutes on the site, we see that the market data loads via XHR.\n",
      "Another common clue is to view the page source and check for content there. If it's not there, it usually means that it will load later, which probably requires XHR requests. And we can intercept those!\n",
      "\n",
      "Since we are parsing a list, we will loop over it a print only part of the data in a structured way: symbol and price for each entry.\n",
      "As in the previous examples, this is a simplified example. Printing is not the solution to a real-world problem. Instead, each page structure should have a content extractor and a method to store it. And the system should also handle the crawling part independently.\n",
      "We'd like you to go with three main points:\n",
      "Even if the extracted data is the same, fail-tolerance and effort in writing the scraper are fundamental factors. The less you have to change them manually, the better.\n",
      "Apart from XHR requests, there are many other ways to scrape data beyond selectors. Not every one of them will work on a given website, but adding them to your toolbelt might help you often.\n",
      "Thanks for reading! Did you find the content helpful? Please, spread the word and share it. üëà\n",
      "Originally published at https://www.zenrows.com\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping Software Reviews & Tips To Help You Scrape The Internet\n",
      "URL: https://dev.to/zoltan/web-scraping-software-reviews-tips-to-help-you-scrape-the-internet-58lo\n",
      "Answer: Web scraping (also known as screen scraping and web harvesting) is the technique and process of extracting data from websites through software or a script.\n",
      "The final goal is to gather information across several sources and transform it into structured data for further analysis, use, or storage.\n",
      "Although there are several ways to do web scraping, the best way to do it is by combining the power of web scraping software and programming languages to ensure scalability and flexibility.\n",
      "Today, we‚Äôll explore how to pick the right tool for your project, as well as our best tips for scraping virtually any website on the internet.\n",
      "https://www.scraperapi.com/blog/web-scraping-software-reviews/\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping in Node.js!\n",
      "URL: https://dev.to/code_jedi/web-scraping-in-nodejs-2lkf\n",
      "Answer: Web Scraping is a way to collect all sorts of publicly available data like prices, text, images, contact information and much more from the world-wide-web. This can be useful when trying to collect data that might take a person a lot of time to collect and organize manually.\n",
      "Some of the most useful use cases of web scraping include:\n",
      "Scraping product prices from e-commerce websites such as amazon, ebay or alibaba.\n",
      "Scraping social media posts, likes, comments, followers or bios.\n",
      "Scraping contacts from websites like yellowpages or Linkedin.\n",
      "While there are a few different libraries for scraping the web with Node.js, in this tutorial, i'll be using the puppeteer library.\n",
      "Puppeteer is a popular and easy to use npm package used for web automation and web scraping purposes.\n",
      "Some of puppeteer's most useful features include:\n",
      "You can read more about puppeteer here\n",
      "For this tutorial, I will suppose you already have npm and node_modules installed, as well as a package.json and package-lock.json file.\n",
      "If you don't, here's a great guide on how to do so: Setup\n",
      "To install puppeteer, run one of the following commands in your project's terminal:\n",
      "npm i puppeteer\n",
      "Or\n",
      "yarn add puppeteer\n",
      "Once puppeteer is installed, it will appear as a directory inside your node_modules.\n",
      "The web scraping script will get the first synonym of \"smart\" from the web thesaurus by:\n",
      "Getting the HTML contents of the web thesaurus' webpage.\n",
      "Finding the element that we want to scrape through it's selector.\n",
      "Displaying the text contents of the scraped element.\n",
      "Before scraping, and then extracting this element's text through it's selector in Node.js, we need to setup a few things first:\n",
      "Create or open an empty javascript file, you can name it whatever you want, but I'll name mine \"index.js\" for this tutorial. Then, require puppeteer on the first line and create the async function inside which we will be writing our web scraping code:\n",
      "index.js\n",
      "Next, initiate a new browser instance and define the \"page\" variable, which is going to be used for navigating to webpages and scraping elements within a webpage's HTML contents:\n",
      "index.js\n",
      "Scraping the first synonym of \"smart\"\n",
      "To locate and copy the selector of the first synonym of \"smart\", which is what we're going to use to locate the synonym inside of the web thesaurus' webpage, first go to the web thesaurus' synonyms of \"smart\", right click on the first synonym and click on \"inspect\". This will make this webpage's DOM pop-up at the right of your screen:\n",
      "Next, right click on the highlighted HTML element containing the first synonym and click on \"copy selector\":\n",
      "Finally, to navigate to the web thesaurus, scrape and display the first synonym of \"smart\" through the selector we copied earlier:\n",
      "First, make the \"page\" variable navigate to https://www.thesaurus.com/browse/smart inside the newly created browser instance.\n",
      "Next, we define the \"element\" variable by making the page wait for our desired element's selector to appear in the webpage's DOM.\n",
      "The text content of the element is then extracted using the evaluate() function, and displayed inside the \"text\" variable.\n",
      "Finally, we close the browser instance.\n",
      "index.js\n",
      "Now if you run your index.js script using \"node index.js\", you will see that it has displayed the first synonym of the word \"smart\":\n",
      "GIF\n",
      "Scraping the top 5 synonyms of smart\n",
      "We can implement the same code to scrape the top 5 synonyms of smart instead of 1:\n",
      "index.js\n",
      "GIF\n",
      "The \"element\" variable will be: \"#meanings > div.css-ixatld.e15rdun50 > ul > li:nth-child(1) > a\" on the first iteration, \"#meanings > div.css-ixatld.e15rdun50 > ul > li:nth-child(2) > a\" on the second, and so on until it reaches the last iteration where the \"element\" variable will be \"#meanings > div.css-ixatld.e15rdun50 > ul > li:nth-child(5) > a\".\n",
      "As you can see, the only thing that is altered in the \"element\" variable throughout the iterations is the \"li:nth-child()\" value.\n",
      "This is because in our case, the elements that we are trying to scrape are all \"li\" elements inside a \"ul\" element,\n",
      "so we can easily scrape them in order by increasing the value inside \"li:nth-child()\":\n",
      "li:nth-child(1) for the first synonym.\n",
      "li:nth-child(2) for the second synonym.\n",
      "li:nth-child(3) for the third synonym.\n",
      "li:nth-child(4) for the fourth synonym.\n",
      "And li:nth-child(5) for the fifth synonym.\n",
      "Final notes\n",
      "While web scraping has many advantages like:\n",
      "Saving time on manually collecting data.\n",
      "Being able to programmatically aggregate pieces of data scraped from the web.\n",
      "Creating a dataset of data that might be useful for machine learning, data visualization or data analytics purposes.\n",
      "It also has 2 disadvantages:\n",
      "Some websites don't allow for scraping their data, one popular example is craigslist.\n",
      "Some people consider it to be a gray area since some use cases of web scraping practice user or entity data collection and storage.\n",
      "Hopefully this article gave you some insight into web scraping in Node.js, it's practical applications, pros and cons, and how to extract specific elements and their text contents from webpages using the puppeteer library.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping Windows Form\n",
      "URL: https://dev.to/rifat070/web-scraping-windows-form-ap3\n",
      "Answer: I want to scrape a website that requires login by using windows form. The first page of my software will have textboxes for login. After login I want to show some data from that page. Can anybody help?\n",
      "Thanks in advance :)\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping vs. API: Do You Know What the Finest Way of Scraping Data is?\n",
      "URL: https://dev.to/xbyteio/web-scraping-vs-api-do-you-know-what-the-finest-way-of-scraping-data-is-569k\n",
      "Answer: You will find data everywhere, however, getting hands on that is another problem‚Äî even if it‚Äôs legal.\n",
      "Web scraping is a huge part of working on innovative projects. However, how do you have your hands on the big data from across the internet?\n",
      "Manual data gathering is unacceptable. It‚Äôs extremely time-consuming and doesn‚Äôt provide all-inclusive and accurate results. However, between dedicated web scraping software as well as a website‚Äôs committed API, which route makes sure the finest data quality without sacrificing morality or integrity?\n",
      "What is Data Harvesting?\n",
      "Data harvesting is a procedure of scraping publicly accessible data straight from online sites. Rather than depending on official information sources, such as prior surveys and studies organized by main companies as well as credible organizations, data harvesting helps you take data harvesting in your hands.\n",
      "You just require a website, which publicly provides the data types you‚Äôre after, the tool to scrape it, as well as a database for storing it.\n",
      "The first as well as last steps are very straightforward. Actually, you can pick a random site using Google as well as store data in the Excel spreadsheet. Scraping data is where the things get complicated.\n",
      "Keeping That Ethical and Legal\n",
      "In terms of authority, given that you don‚Äôt use black-hat methods to get data or violate any site‚Äôs privacy policy, you‚Äôre safe. You need to avoid doing everything illegal with data that you harvest including harmful apps and unnecessary marketing campaigns.\n",
      "Legal data harvesting is a bit more complex. Primarily, you need to respect a site owner‚Äôs rights above their data. In case, they follow Bot Exclusion Standards for any parts of their site, then avoid it.\n",
      "So, they don‚Äôt need anybody to extract their data without clear permission, even though it‚Äôs publicly accessible. Furthermore, you need to avoid downloading in large amounts data all together, as it could crash a site‚Äôs servers as well as might get you labeled as a DDoS attack.\n",
      "Tools of Web Scraping\n",
      "Web scraping is as near as it gets to take data harvesting counts in your hands. They‚Äôre the most customized alternative and make data scraping procedure easy and accessible, all whereas providing you unlimited access of the completeness of a site‚Äôs accessible data.\n",
      "Web scrapers or web scraping tools are software produced for data scraping. They are available in data-friendly programming languages like Ruby, PHP, Python, and Node.js.\n",
      "How Different Web Data Scraping Tools Work?\n",
      "Web data scrapers automatically load as well as read the whole website. That‚Äôs way, they don‚Äôt have access of surface-level data, and however, they can read a site‚Äôs HTML codes, JavaScript, and CSS elements.\n",
      "You could set a scraper to get a particular data type from different sites or train it to read as well as duplicate all the data, which isn‚Äôt protected or encrypted by the Robot.txt file.\n",
      "Web data scrapers work using proxies to evade getting blocked by website security as well as anti-bot and anti-spam tech. They utilize proxy servers for hiding their identity as well as mask IP addresses to look like normal user traffic.\n",
      "However, note that to completely covert while extracting, you have to set tools to scrape data at slower rates‚Äîone, which matches the speed of a human user.\n",
      "Ease of Use\n",
      "Although depending heavily on the complex programming libraries and languages, web data scraping tools are very easy to utilize. They don‚Äôt need you to be any data science or programming expert to take the maximum out of them.\n",
      "Moreover, web scrapers create data for you. The majority of web scrapers repeatedly convert data into different user-friendly formats. Also, they compile that into ready-to-use and downloadable packets to get easy access.\n",
      "API Data Scraping\n",
      "API means Application Programming Interface. It‚Äôs not a web scraping tool but a feature, which software and website owners can select to implement. APIs work as an intermediate, helping websites as well as software to converse as well as exchange information and data.\n",
      "Today, the majority of websites, which handle a huge amount of data have a devoted API like YouTube, Twitter, Facebook, or Wikipedia. However, as a web scraper is the tool, which helps you browse as well as extract the remote corners of the websites for data, APIs are well-structured in the data extraction.\n",
      "How Does API Data Scraping Work?\n",
      "APIs don‚Äôt instruct data harvesters to obey their privacy. They impose it in their code. The APIs include rules, which create structure as well as put limits on the user experiences. They control all the data types you can scrape that data resources are open to do harvesting, as well as the kind of frequencies of requests.\n",
      "You may think about APIs as the app or website‚Äôs customized communication protocol. This has definite rules to trail and requires to speak the language before communicating with that.\n",
      "How to Utilize APIs for Data Scraping?\n",
      "To utilize an API, you require a decent knowledge level in the query‚Äôs language a website utilizes to ask about data using the syntax. Most of sites utilize JSON (JavaScript Object Notation) in the APIs, therefore you require a few to improve your knowledge in case you will depend on the APIs.\n",
      "However, it doesn‚Äôt finish there. Because of a huge amount of data as well as variable objectives that people have, APIs generally send raw data. Whereas the procedure isn‚Äôt complex as well as only needs a beginner-level database understanding, you will require to convert data into SQL or CVS before doing anything with that.\n",
      "As they‚Äôre the official tool provided by a site, you don‚Äôt need to worry about having a proxy server or having your IP blocked. And in case, you‚Äôre bothered that you could cross any moral lines as well as extract data that you weren‚Äôt permitted to, APIs provide you only the data access an owner needs to provide.\n",
      "Web Scraping vs. API: It‚Äôs Time to Use Both\n",
      "According to your present skill level, your targeted websites, as well as your objectives, you might need to utilize both APIs as well as data scraping tools. In case, a site doesn‚Äôt have any dedicated API, then using any web data scraper is the only option you have. However, websites having an API‚Äîparticularly if they are charging for accessing data, frequently make extraction using any third-party tools is near to impossible!\n",
      "For more details about different web scraping services or web scraping APIs, you can contact X-Bye Enterprise Crawling or ask for a free quote!\n",
      "For more visit: https://www.xbyte.io/web-scraping-vs-api-do-you-know-what-the-finest-way-of-scraping-data-is.php\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping using axios and Cheerio\n",
      "URL: https://dev.to/drsimplegraffiti/i-scraped-dev-to-using-axios-and-cheerio-26ko\n",
      "Answer: Hello folks, today I will be sharing information on Web Scraping. Web scraping is simply the process of extracting content and data from a website. This post is only for Educational purpose‚ùó\n",
      "\n",
      "üë®‚Äçüíª Nodejs\n",
      "üë®‚Äçüíª Developer Tool knowledge (DevTools)\n",
      "üë®‚Äçüíª Document Object Model knowledge\n",
      "ü•¶ Make a new directory in my case nodescraping and initiate a node js app\n",
      "npm init -y\n",
      "\n",
      "üéØ Result: Creates your package.json file\n",
      "ü•¶ Install dependencies\n",
      "npm i express axios cheerio\n",
      "üéØ Result:\n",
      "ü•¶ Install Dev dependencies (for development purposes), nodemon restarts our node app automatically when files change..\n",
      "npm i nodemon -save-dev\n",
      "üéØ Result:\n",
      "ü•¶ Edit your start script\n",
      "üéØ Result:\n",
      "ü•¶ create a file app.js and import the packages\n",
      "ü•¶ I will be using the axios package to fetch the website. I will be using a site called Dev.toüòÅ. Be at liberty to use any website of your choice. We will be scraping and exporting our result into a plain text file CSV.\n",
      "ü•¶ Right-click, to inspect the website to select elements (class, is) and their respective attributes (a, li).\n",
      "üéØ This gives us the ability to inspect the classes we want to select.\n",
      "ü•¶ I want to target the following. Blog title, link, author, and read the time.\n",
      "Always use . before the class name you want to target.\n",
      "In the logic above, i am target the child element of the class crayons-story.\n",
      "The .text() method is converting the result to text.\n",
      "ü•¶ I repeated the whole process to select the Blog link, author, and read the time.\n",
      "ü•¶ Final logic is:\n",
      "View source code here: here\n",
      "ü•¶ Run server npm run dev\n",
      "üéØ Result:\n",
      "This is a quick guide on how to scrape websites, there are other packages that can be used to perform the same function such as puppeteer, fetch, request and so on.\n",
      "Web scraping by Thomas W.Smith\n",
      "Web scraping by Traversy Media\n",
      "Cheerio Docs\n",
      "Thanks for reading\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping: Serverless Node.JS e Puppeteer\n",
      "URL: https://dev.to/alissonb13/web-scraping-serverless-node-js-e-puppeteer-3kp4\n",
      "Answer: Existem situa√ß√µes onde precisamos coletar informa√ß√µes de uma p√°gina web e essa pr√°tica √© chamada de web scraping. Geralmente esse processo √© simples e, provavelmente, n√£o se faz necess√°rio execut√°-lo o tempo todo.\n",
      "Atualmente temos √† nossa disposi√ß√£o a arquitetura serverless, onde conseguimos hospedar fun√ß√µes sem preocupa√ß√£o com infraestrutura. Esse modelo de arquitetura torna bastante agrad√°vel o desenvolvimento de funcionalidades que ser√£o executadas sob demanda, que √© o caso padr√£o de um web scraping.\n",
      "No ecossistema Node.JS, o Puppeteer √© uma biblioteca bastante utilizada para a implementa√ß√£o de web scrapings. Para ser mais honesto, essa biblioteca √© bem mais poderosa e possui muito mais funcionalidades do que simplesmente coletar informa√ß√µes de web sites. O Puppeteer √© capaz de gerar screenshots e PDFs de p√°ginas web, automatizar o envio de formul√°rios e fazer testes de UI. E o que o torna t√£o poderoso √© a sua API de alto n√≠vel capaz de controlar inst√¢ncias do Chrome ou Chromium sobre o protocolo DevTools.\n",
      "Bom... agora que j√° mencionei sobre a arquitetura serverless e o Puppeteer, podemos meter a m√£o no c√≥digo e escrever uma fun√ß√£o lambda que ir√° acessar https://stackoverflow.com/jobs e capturar as informa√ß√µes das vagas de emprego e retorn√°-las em formato json.\n",
      "Para esse exemplo iremos utilizar Serverless Framework, provider AWS Lambda com runtime Node.JS utilizando Typescript e, obviamente, o Puppeteer. Estou assumindo que voc√™ j√° tem o serverless instalado, bem como as credenciais da AWS configuradas na sua m√°quina.\n",
      "Eu n√£o gosto muito de utilizar templates, pois geralmente trazem um monte de coisas n√£o t√£o necess√°rias assim, principalmente para esse projeto em quest√£o. Ent√£o, eu come√ßo criando toda lambda function com os seguintes passos:\n",
      "Dentro do arquivo serverless.yml precisamos fornecer as informa√ß√µes do nosso serverless.\n",
      "\n",
      "O service recebe o nome que queremos dar para a nossa lambda function.\n",
      "O provider √© basicamente a infra onde a lambda ir√° executar, nesse caso √© a AWS, como podemos ver na propriedade name. A propriedade region recebe o valor da regi√£o da AWS que voc√™ ir√° executar seu c√≥digo, em layers temos um arn para o pacote chrome-aws-lambda, as outras propriedades armazenam informa√ß√µes sobre o ambiente de execu√ß√£o, tamanho de mem√≥ria RAM e timeout.\n",
      "Os plugins s√£o ferramentas facilitadoras e nesse caso temos dois: serverless-plugin-typescript que configura todo TS no nosso projeto e serverless-offline que ir√° facilitar a execu√ß√£o local do projeto.\n",
      "Na parte de functions temos a nossa fun√ß√£o, que pode receber qualquer nome, mas no handler deve apontar para o path do arquivo e fun√ß√£o que ir√° executar. √â importante notar a parte de events, que nada mais √© do que o evento que ir√° disparar a chamada para a a nossa fun√ß√£o, e nesse caso √© um evento http com uma requisi√ß√£o GET para o endpoint /scraping.\n",
      "No nosso app.ts iremos escrever o c√≥digo que ir√° executar quando a nossa fun√ß√£o for chamada.\n",
      "Nesse momento, o caminho natural ao desenvolver um projeto com Node.JS seria instalar o puppeteer no projeto, utilizando o yarn add puppeteer (ou npm install puppeteer). Por√©m quando estamos falando de fun√ß√µes lambda temos algumas limita√ß√µes, e uma delas √© o tamanho da aplica√ß√£o, que deve ser no m√°ximo 50MB.\n",
      "Quando instalamos o Puppeteer na aplica√ß√£o, ele instala tamb√©m uma vers√£o do Chrome que √© utilizado para manipular o conte√∫do das p√°ginas web, sendo assim o limite de 50MB √© facilmente ultrapassado, j√° que a vers√£o do chrome que √© instalada com o Puppeteer chega a um pouco mais de 200MB de tamanho. Mas para resolver esse problema podemos utilizar o pacote chrome-aws-lambda, que ir√° fornecer todo o ferramental para podermos trabalhar com o puppeteer sem que o nosso projeto fique inflado a ponto de ultrapassar o limite de 50MB. Nesse momento iremos instalar tamb√©m um pacote chamado puppeteer-core que cont√©m o puppeteer mas sem a inst√¢ncia do chrome embutida.\n",
      "Depois de instalar os pacotes chrome-aws-lambda e puppeteer-core, podemos importar no c√≥digo da nossa fun√ß√£o. Vou explicar detalhadamente cada trecho de c√≥digo da fun√ß√£o abaixo:\n",
      "\n",
      "Na linha 12 uma inst√¢ncia do Chrome √© inicializada passando alguns par√¢metros, e temos que destacar dois deles:\n",
      "Em seguida inicializamos uma nova p√°gina. Mas aqui, tenho uma dica legal pra passar. A maioria dos c√≥digos de exemplos que encontramos na internet exibe o c√≥digo await browser.newPage(), fazendo com que uma nova aba seja aberta nonavegador. Mas se pararmos para pensar quando o browser foi iniciado ele j√° abriu uma p√°gina, ent√£o n√≥s s√≥ precisamos pegar ela utilizando (await browser.pages())[0]. De todo modo, precisamos acessar essa page para navegar at√© uma URL, que nesse caso est√° declarada em uma constante na linha 10.\n",
      "O objeto page nos fornece o acesso √† fun√ß√£o .evaludate(), onde conseguimos utilizar javascript para acessar os elementos da p√°gina e extrair as informa√ß√µes. Essa fun√ß√£o retorna uma promessa de um tipo gen√©rico, ent√£o voc√™ pode estruturar as informa√ß√µes de retorno como quiser. No nosso caso, estamos retornando um array do tipo Job.\n",
      "Ap√≥s retornar o nosso conte√∫do podemos ent√£o fechar a nossa inst√¢ncia do Chrome, ou caso voc√™ queira fazer ainda mais procedimentos, pode utilizar o await page.close() para fechar uma p√°gina que n√£o ir√° utilizar mais.\n",
      "Agora que entendemos o que est√° no serverless.yml e app.ts, podemos executar a nossa fun√ß√£o. E agora tenho outra dica: quando estamos trabalhando com o chrome-aws-lambda localmente ele n√£o tem acesso √† uma inst√¢ncia do chrome para trabalhar, ent√£o precisamos instalar o puppeteer como depend√™ncia de desenvolvimento utilizando o comando yarn add puppeteer -D (ou npm install puppeteer -D). Internamente o chrome-aws-lambda se resolve e consegue encontrar a inst√¢ncia de acordo com o environment.\n",
      "Ent√£o, para que n√£o fiquem d√∫vidas em rela√ß√£o aos pacotes instalados, temos o seguinte packge.json:\n",
      "\n",
      "OBS: lembrando que todos plugin declarado no serverless.yml deve ser instalado na aplica√ß√£o tamb√©m, e nesse caso estamos utilizando como depend√™ncias de desenvolvimento.\n",
      "Para executar a aplica√ß√£o basta utilizar o comando serverless offline e para fazer o deploy basta executar serverless deploy e ele ir√° subir o c√≥digo para a nuvem do provedor e na regi√£o declarada.\n",
      "Ao executar o comando serverless offline o que esperamos de retorno √© algo parecido com essa imagem:\n",
      "\n",
      "Podemos perceber uma URL GET exatamente com o endpoint que configuramos no serverless.yml, basta fazer uma solicita√ß√£o utilizando postman, insomnia ou at√© mesmo no pr√≥prio browser e poderemos ver o retorno em formato JSON.\n",
      "Bom... acho que √© isso! :)\n",
      "Na pr√≥xima publica√ß√£o estou querendo trazer algo mais elaborado mostrando um pouco sobre a configura√ß√£o de um schedule que ir√° disparar a execu√ß√£o da fun√ß√£o, detalhando um pouco mais sobre os recursos da AWS.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping YouTube search video results with Nodejs\n",
      "URL: https://dev.to/serpapi/how-to-scrape-youtube-search-result-with-nodejs-1kjj\n",
      "Answer: \n",
      "First, we need to create a Node.js* project and add npm packages puppeteer, puppeteer-extra and puppeteer-extra-plugin-stealth to control Chromium (or Chrome, or Firefox, but now we work only with Chromium which is used by default) over the DevTools Protocol in headless or non-headless mode.\n",
      "To do this, in the directory with our project, open the command line and enter npm init -y, and then npm i puppeteer puppeteer-extra puppeteer-extra-plugin-stealth.\n",
      "*If you don't have Node.js installed, you can download it from nodejs.org and follow the installation documentation.\n",
      "üìåNote: also, you can use puppeteer without any extensions, but I strongly recommended use it with puppeteer-extra with puppeteer-extra-plugin-stealth to prevent website detection that you are using headless Chromium or that you are using web driver. You can check it on Chrome headless tests website. The screenshot below shows you a difference.\n",
      "\n",
      "SelectorGadget Chrome extension was used to grab CSS selectors by clicking on the desired element in the browser. If you have any struggles understanding this, we have a dedicated Web Scraping with CSS Selectors blog post at SerpApi.\n",
      "The Gif below illustrates the approach of selecting different parts of the results.\n",
      "\n",
      "Declare constants from required libraries:\n",
      "Next, we \"say\" to puppeteer use StealthPlugin and write what we want to search:\n",
      "Next, we write down a function for scrolling page:\n",
      "Next, we write down a function for getting organic results data from the search page:\n",
      "And finally, a function to control the browser, and get information:\n",
      "Now we can launch our parser. To do this enter node YOUR_FILE_NAME in your command line. Where YOUR_FILE_NAME is the name of your .js file.\n",
      "Alternatively, you can use the YouTube Video Results API from SerpApi.\n",
      "The difference is that you can still get the same results without using browser automation, which saves time. Also, you don't need to write a parser from scratch, choose the right CSS selectors, which can change. And in the end, there is a possibility that at some point the request may be blocked as suspicious. Instead, you just need to iterate over the structured JSON and get the data you want.\n",
      "First, we need to install google-search-results-nodejs. To do this you need to enter in your console: npm i google-search-results-nodejs\n",
      "Declare constants from required libraries:\n",
      "Next, we write down what we want to search and the necessary parameters for making a request:\n",
      "Next, we wrap the search method from the SerpApi library in a promise to further work with the search results:\n",
      "And finally, we declare and run the function getResult that gets videos info from all pages and return it:\n",
      "If you want to see some projects made with SerpApi, please write me a message.\n",
      "Join us on Twitter | YouTube\n",
      "Add a Feature Requestüí´ or a Bugüêû\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping Walkthrough with Python\n",
      "URL: https://dev.to/awwsmm/web-scraping-walkthrough-with-python-85c\n",
      "Answer: Web scraping is the process of extracting data from a web page's source code, rather than through some API exposed by the owner(s) of that page. It can be a bit tricky at first, but it allows you to easily pull and organise lots of information from the web, without having to manually copy and paste anything.\n",
      "To do some basic web scraping today, I'll use the Python library BeautifulSoup. If you haven't used this package before, you'll need to install it. The easiest way to do that is with the Python package manager pip. First, check if you have pip on your machine by trying to install a library with it:\n",
      "If you have Python but don't have pip (if the above throws an error), install pip by itself using the instructions found here. macOS and most Linux distributions come with Python by default, but if you're on Windows and you need to install Python, try the official website.\n",
      "Python 2.7 is deprecated as of 1 January 2020, so it might be better to just get Python 3 (if you don't yet have it). I don't have Python 3 yet (because I just factory reset my Mac not too long ago), so I'm installing it first using these instructions, which essentially just boil down to:\n",
      "Now, we can check that both Python 2 and Python 3 are installed, and that pip was installed alongside Python 3:\n",
      "Finally, let's get BeautifulSoup using pip3:\n",
      "Note that, at this point, you could use the \"normal\" Python interpreter with the python3 command, or you could use the more feature-rich IPython by installing:\n",
      "Throughout this tutorial, I'll be using IPython.\n",
      "My motivation for this project was that I wanted to create an \"average profile\" of a developer at a given level in a given area, based on job postings on Indeed and similar websites. While doing something like that is a bit involved and might involve some regex, a good place to start would be to simply see how often a given technology is listed in job postings: more mentions == more important, right?\n",
      "BeautifulSoup lets you access a page's XML / HTML tags by their type, id, class, and more. You can pull all <a> tags, for instance, or get the text of all <p> tags with a particular class. So to pull data out in a regular way, we need to dissect the structure of the pages we want to scrape. Let's start by doing a search for JavaScript developers in New York City:\n",
      "\n",
      "Note the URL of this web page:\n",
      "https://www.indeed.com/jobs?q=javascript+developer&l=New+York+City\n",
      "If we go to the second page of results, it changes to:\n",
      "https://www.indeed.com/jobs?q=javascript+developer&l=New+York+City&start=10\n",
      "...and the third page of results:\n",
      "https://www.indeed.com/jobs?q=javascript+developer&l=New+York+City&start=20\n",
      "Right, so there are 10 results per page and each page after the first has an additional parameter in the URL: &start=..., where ... is a positive multiple of 10. (As it turns out, we can append &start=0 to the URL of the first page and it returns the same results.) Okay, so we know how to access pages of results... what's next? How about we inspect the structure of the first results page:\n",
      "\n",
      "One thing I notice is that the links to each job ad seem to have an onmousedown which changes predictably. The first one is\n",
      "...the second is\n",
      "...and so on. I would bet that we can pull all <a> tags with an onmousedown containing \"return rclk(this,jobmap[\" and that would give us all the links to all the jobs listed on this page. Let's put that in our back pocket for now and open one of these ads -- let's see if we can figure out where the job specifications are within these pages:\n",
      "\n",
      "It looks like the main body of the ad is contained in a <div> with class=\"jobsearch-JobComponent-description\". That sounds like a pretty specific div. I'll just go ahead and assume that's the same on every page, but you can check if you like. So now that we know the structure of the URLs we want to visit, how to find links to job ads on those pages, and where the text of the ad is contained in those subpages, we can build a web scraping script!\n",
      "Let's start by just looping over search pages. Our URL will look something like:\n",
      "...but we need to append a non-negative multiple of 10 to the end. An easy way to do this in Python is to create a range loop:\n",
      "That looks good! Note that we had to convert the integer to a string with Python's str() method.\n",
      "What we really want to do is actually visit these pages and extract their content. We can do that with Python's urllib module -- specifically urllib.request.urlopen() (Python 3 only). We can then parse the page with BeautifulSoup by simply calling the BeautifulSoup constructor. To test this, let's temporarily reduce our loop range to just one page and print the contents of the page with soup.prettify():\n",
      "I trimmed the output using string slicing, limiting it to 500 characters (the source code of this page is pretty long). You can see just in that short snippet, though, our original search: q=javascript&amp;l=New+York+City.\n",
      "Great! So, this seems to work. Let's use select() now to grab all of the job ad links on this page. Remember that we're looking for all of the <a> tags with an onmousedown containing \"return rclk(this,jobmap[\". We have to use a special syntax to achieve that result, see below:\n",
      "We append \"https://www.indeed.com\" to the beginning of each link because, in the source code of the page, all the hrefs are relative. If we grab one of these links (say the third one) and paste it into the browser, we should hopefully get a job ad:\n",
      "\n",
      "...looking good! Okay, what's next? Well, we want to, again, open these subpages with BeautifulSoup and parse the source code. But this time, we want to look for <div>s with a class that contains jobsearch-JobComponent-description. So let's use string slicing again and print the first, say, 50 characters of each page, just to make sure that all of these URLs are working:\n",
      "Again, great! Everything's working so far. The next thing to do would be to try to extract the text of the main body of each ad. Let's use the same *= syntax in select() that we used previously to find <div>s in these subpages which have a class attribute which contains jobsearch-JobComponent-description:\n",
      "BeautifulSoup.select() returns the HTML / XML tags which match the search parameters that we provide. We can pull attributes from those tags with bracket notation (as in adlink['href']) and we can pull the text contained within opening and closing tags (for instance, between <p> and </p>) with get_text(), as we did above. The subSOUP.select() statement returns a list of <div> tags, with class attributes that contain the substring \"jobsearch-JobComponent-description\", then we use a for ... in loop to get each <div> in that list (there's only one) and print the text contained within <div> ... </div> with get_text().\n",
      "The result is this list of jumbled text. It doesn't make any sense because we cut each description off after only 50 characters. But now we have our fully-functional Indeed job ad scraper! We just need to figure out what to do with these results to complete our task.\n",
      "The easiest thing to do is to come up with a list of keywords we're interested in. Let's look at the popularity of various JavaScript frameworks. How about:\n",
      "...that's probably a good start. If you're familiar with processing text data like this, you'll know that we have to convert everything to lowercase to avoid ambiguity between things like \"React\" and \"react\", we'll have to remove punctuation so we don't count \"Angular\" and \"Angular,\" as two separate things, and we can easily split this text into tokens on spaces using split(). Let's first split the text of each ad, convert each word to lowercase, and see what our list of words looks like:\n",
      "...and so on. Let's pick out some weird ones:\n",
      "...right, so we'll have to split on spaces as well as ., ,, and :. Elsewhere in the list, we have:\n",
      "which will, of course, be damaged by splitting on ., but I think the benefits outweigh the costs here. We also have lots of hyphenated words like\n",
      "...so we probably shouldn't split on hyphens or dashes. We do however have one or two\n",
      "...so we'll want to split on / as well. Finally, there's nothing we can do about typos like:\n",
      "...at the moment, so we'll have to leave those as-is. To make this solution a bit more robust, we want to split on multiple separators, not just the space character. So we need Python's regular expression library re:\n",
      "Right. So now what weirdos do we have?\n",
      "So, still a few edge cases. Easy-to-fix ones include removing trailing 's from words and adding ?, (, and ) to the list of separator characters (as well as whitespace characters like \\n, \\t, and \\r). (One more quick scan reveals that we should add ! to the list of separator characters as well, obviously.) We can also ignore words that are only a single character or less. Fixing the problems with times (11:59pm) and salaries ($70,000 - $80,000) are a bit more involved and won't be covered here. For now, we'll just ignore those. So let's check out our improved scraper:\n",
      "Beautiful! Now, what can we do with it?\n",
      "Instead of simply printing a list of words, let's add them to a dictionary. Every time we encounter a new word, we can add it to our dictionary with an initial value of 1, and every time we encounter a word we've seen before, we can increment its counter:\n",
      "I added a \"Scraping\" echo to the user so we can be sure our script is progressing. Note that the resulting dictionary is not ordered! If we want to order it by value, there are a few different ways we can do that, but the easiest one is probably to just turn it into a list of tuples, flipping the keys and values so we can easily sort by key (number of occurrences of a particular word):\n",
      "We sort by reverse=True so it's sorted high-to-low, and the most common words are at the top of the list. Let's see the result:\n",
      "Of course, the reason we want to pick specific words out (like \"angular\", \"react\", etc.) is because we'll get a bunch of useless filler words (like \"to\", \"and\", etc.) otherwise. Let's define a list of \"good\" words, check our word against the list, and only count ones that we care about. Finally, I'll also get rid of the [:50] slice which we used for debugging, and expand my search to the first 100 pages of results. Here is the final script:\n",
      "I made some small aesthetic changes... can you see where they are? I also made sure to remove \".js\" or \"js\" from the end of any framework names so they're not counted as separate things. I removed the \"magic number\" 10 from the script and put it in a descriptive variable (ads_per_page). Also, I created a variable (max_pages) which says I should only look at 100 pages of results, so in total, I'll look at the 1000 most recent \"Javascript\" ads posted on Indeed in the NYC area.\n",
      "This is going to take a while, so I'll go grab some coffee and come back...\n",
      "...so, what does the result look like?\n",
      "So, out of 1000 ads scraped, 556 mentioned \"react\", 313 mentioned \"angular\", and so on. Quite a bit of insight from a quick script!\n",
      "With some more work, this could be turned into a website / app where developers (or anyone) looking for a job could find out what the average requirements are (\"...56% of ads requested experience with React...\"), what the average salary is (\"...$55,000 +/- $2,000...\"), and benchmark themselves against those averages. Such a tool would be really useful in salary negotiations, or when trying to decide what new technologies / languages to learn to advance your career. Data could be kept current by tracking ad posting dates and throwing out stale information (older than, say, a week).\n",
      "This information would also be useful to employers, giving them a better idea of where to set salaries for certain positions, levels of experience, and so on. Indeed was just the first step, but this scraping could easily be expanded to multiple job posting websites.\n",
      "This prototype only took a few hours' work for one person with limited Python experience. I would imagine that a small team of people could get this app up and running in just a few weeks. Thoughts? Does anyone know of anything similar?\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Webscraping in Python with Flask and BeautifulSoup 4\n",
      "URL: https://dev.to/blazephoenix/webscraping-in-python-with-flask-and-beautifulsoup-4-1pkl\n",
      "Answer: \n",
      "Web scraping is a term used for the process of extracting HTML/XML data from websites. Once extracted, it can be parsed into a different HTML file or saved locally in text/spreadsheet documents.\n",
      "A lot of websites that aggregate data from other websites on the internet. Some examples could be websites that give you the best deals on the same product after comparing across multiple platforms (Amazon, Flipkart, Ebay, etc.), and also sites that collect datasets to apply ML algorithms to.\n",
      "I would recommend you to limit your thinking to how something could benefit you especially when you know little to nothing about it. It helps to be a generalist when you‚Äôre just starting out. Learn everything, you never know when you‚Äôll need it! You can always settle and specialize in one area eventually, when you‚Äôre well aware of the options you have.\n",
      "If Python is installed correctly, you should see, 3.6.8 in the terminal.\n",
      "VSCode is a free code editor with lots of features that make writing and debugging code much easier.\n",
      "Now we need to import a few libraries which will help us build our web scraper.\n",
      "This is basically the command line but within the editor so we don‚Äôt have to have two windows and keep switching between them.\n",
      "You could call it the Alfred to Python‚Äôs Batman. Hehe.\n",
      "This installs the beautifulsoup library which will help us scrape webpages.\n",
      "Flask is a lightweight framework to build websites. We‚Äôll use this to parse our collected data and display it as HTML in a new HTML file.\n",
      "The requests module allows us to send http requests to the website we want to scrape.\n",
      "In your file, type the following code:\n",
      "The first line imports the Flask class and the render_template method from the flask library. The second line imports the BeautifulSoup class, and the third line imports the requests module from our Python library.\n",
      "Next, we declare a variable which will hold the result of our request\n",
      "We send a GET request to https://webscraper.droppages.com and convert the HTML to plain text and store that in the source variable.\n",
      "Next we declare a soup variable and store the value we get after passing source to BS. ‚Äòlxml‚Äô is the markup we want our rendered code to have.\n",
      "At this point, we have our code working. You can check it out by passing soup to a print function, like this print(soup) after the previous line and running python webscraper.py in the terminal.\n",
      "Right now, we are pulling the entire web page rather than specific elements on it. To get specific elements, you can try these by yourself.\n",
      "But before you do that, you should be aware of what exactly you want to get. You can either run the last command again or open the web page in the browser and inspect it by right clicking on the page. Some knowledge of HTML DOM and CSS is required here. You can head over to W3Schools or MDN for a quick crash course on both.\n",
      "You can pass regular CSS notation in the brackets to be more specific about the elements you want.\n",
      "Now, we are only actually just outputting HTML along with the text inside it. What if we just want the text?\n",
      "That‚Äôs easy.\n",
      "We simply pass .text at the end of it. Just like we did with source. Here‚Äôs an example.\n",
      "Here, we tell Python to store the text of the div in the 4th article element which is in the main element, in the head variable.\n",
      "You can check the output by passing head to print() and running python webscraper.py in the terminal.\n",
      "Try getting the names of one of the authors if you can.\n",
      "You can get an author like this,\n",
      "Notice how you also get the date along with the name. That‚Äôs because both of them share the same element. There is a way to get the author name separately by using Python string methods like split and slice. But we won‚Äôt cover that here.\n",
      "Next up, we will use flask to re-render our received data the way we want on a local server.\n",
      "In your file, type the following code,\n",
      "Create a new templates folder in your main webscraper folder and call it index.html\n",
      "The flask part is a little complicated to explain but to put it simply, we created a simple server that will take our index.html from the templates folder and serve it on a local server‚Ää‚Äî ‚Äälocalhost://5000\n",
      "Now, we can combine multiple variables we declared in all the previous code using soup and pass the text to our HTML and use CSS to style them the way we want!\n",
      "You can use this code for the index.html file,\n",
      "Now, we can use all the code we learned so far to create custom variables and pull specific data from our site. If we are well versed with the structure of our target site, we can use shortcuts like these.\n",
      "Now we‚Äôll pass these variables into our HTML while it gets rendered so we can see the data on our webpage.\n",
      "Now open the terminal and run python webscraper.py\n",
      "If you‚Äôre wondering how it‚Äôs so easy, well, it‚Äôs not. This was just a single page, and a simple one at that, with no classes or IDs added to the HTML. But this is a good start.\n",
      "Wonder how you can scrape multiple pages?\n",
      "The answer is multiple for, while, try, except and if-else loops!\n",
      "Hello, this was my very first technical article. If you find any errors in the code or the way I approached the tutorial, feel free to correct me. I'm excited to be part of this community as I grow with it and intend to contribute meaningful content. Thank you for reading!\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping in JavaScript and Node.js using Puppeteer\n",
      "URL: https://dev.to/inspiratory/web-scraping-in-javascript-and-node-js-using-puppeteer-8p1\n",
      "Answer: Internet is a source for all kinds of useful (and useless) data. Most people manually access that data by using a web browser. You can visit a website using a web browser to do things like checking out social media, getting the latest news, or check stock/cryptocurrency prices.\n",
      "Another way of accessing data is to use an API. API is short for Application Programming Interface. A Web API defines the way we can programmatically access and interact with a remote resource. This way, we can consume data on the web without using a web browser. For example, we can use an API of a money exchange to programmatically fetch the latest prices for a stock without visiting the website.\n",
      "Web scraping is the act of extracting data from a website either by manual or automated means. Manually extracting data can be time-consuming, given the amount of data that is out there. Unfortunately, not every online resource has an API that you can interact with. For those cases, we can automate a browser to access a website programmatically.\n",
      "We can control a browser programmatically by using JavaScript. Automating our interactions with the web by programming a browser enables us to build tools that can scrape data from websites, fill out forms for us, take screenshots or download files with a single command.\n",
      "There are many libraries in the JavaScript ecosystem that would allow us to control a browser programmatically. The package that we will be using for this purpose is called Puppeteer. It is a well-maintained library that is developed by the teams at Google.\n",
      "Puppeteer allows us to control a Chrome (or a Chromium) browser programmatically. When we control a browser without any graphical User Interface (UI), it is said to be running in a headless mode.\n",
      "This post assumes that you are comfortable using the JavaScript async-await pattern that is used for writing asynchronous programs. JavaScript has a couple of patterns that are used for dealing with the asynchronous program flow, such as callback functions and Promises. async-await is an asynchronous programming structure that got introduced into JavaScript after Promises. It makes working with asynchronous code a lot easier. Using async-await, we can write code that almost reads like synchronous code. Using async-await makes working with Puppeteer much easier.\n",
      "This post will also assume a basic knowledge of Node.js, HTML, CSS, and JavaScript DOM APIs. If you are not comfortable with any of these subjects make sure to check out my book Awesome Coding that teaches you these fundamentals and a lot more! You can find the source code for the program we write here at: https://github.com/hibernationTheory/awesome-coding/tree/master/sections/05-puppeteer/puppeteer-03-project-wiki\n",
      "Let's install Puppeteer to start working with it. This post will assume that you have Node.js and npm installed on your machine. We will begin with creating a new folder for our project and running the npm init command in that folder to create a package.json file in there.\n",
      "Now that we have the package.json file created. We can install the puppeteer library by running this command:\n",
      "This installation might take a while since it downloads a version of the Chromium browser compatible with this library.\n",
      "After downloading the file we can create a file called main.js and start coding inside it.\n",
      "Here is an example of a Puppeteer program that programmatically launches a headless browser to visit a website and then takes a screenshot of that site to save it onto the computer.\n",
      "We start our code by importing the puppeteer library. After that, we define an async function called main and then call it at the end of our program. The main logic of our program resides inside the main function.\n",
      "Inside the function body, we first launch a browser instance by calling puppeteer.launch(). Whenever we launch a browser, we should remember to close it to not to cause any memory leaks from our program. A memory leak means the program that is not working is still consuming the resources of the system. We close the browser by calling browser.close().\n",
      "We launch a new page inside that browser by calling browser.newPage(). We then visit the [example.com](http://example.com) domain inside that page by using the page.goto method. We take a screenshot of the page by using the page.screenshot method and save that screenshot into the same folder that we have called the program from. We then ensure that we are closing the browser and exit the program.\n",
      "Now that we know the basics of Puppeteer let's build a simple project to put our knowledge into use.\n",
      "Using our Puppeteer knowledge, we will build a program that will fetch a random Wikipedia article every time it runs.\n",
      "Let's look at how we would manually perform such a task to understand how we would automate it. In this case, we need to visit the website for Wikipedia (https://en.wikipedia.org) and click on the link named Random Article to take us to a random article page. On each article page, there is a title and an introductory paragraph.\n",
      "We will need to follow the same steps with our Puppeteer program. We will visit the URL for random results and fetch the HTML elements with the title and the description. We would then need to display these results on the screen.\n",
      "The URL for the Random Article page is https://en.wikipedia.org/wiki/Special:Random. We can get this value by right-clicking on this link and selecting Copy Link Address. We will start by writing a program that will visit this URL and take a screenshot.\n",
      "Everytime we run this program we are capturing a new screenshot from the visited URL.\n",
      "We can inspect the HTML structure of an article page in a Chrome browser by clicking View > Developer > Inspect Elements. We would see that the title of the article is defined inside an h1 tag. This means that we can get the title data by running the code below inside the developer console when we are on an article page.\n",
      "We can use Puppeteer to execute this code in the context of a webpage. We can use the page.evaluate function for this purpose. page.evaluate takes a callback function as an argument that gets evaluated in the current web page context. What we return from this callback function can be used in the Puppeteer application.\n",
      "Here we are capturing the value of the h1 tag in the webpage context and returning that value to the Puppeteer context.\n",
      "page.evaluate can be a little unintuitive since its callback function can't refer to any value in the Puppeteer context. For example, we can't do something like the following example when using the page.evaluate function:\n",
      "This program would throw an error. The selector variable doesn't exist inside the webpage context, so we can't refer to it from there. If we wanted to pass data to the webpage context, we could do so by providing it as an argument to the page.evaluate and its callback function.\n",
      "In this example, we are passing the selector variable as the second argument to the page.evaluate function as well as an argument to the callback function.\n",
      "In our program, let's get the first paragraph of the article as well. Looking at the HTML structure, it seems like the p element we are looking for is inside an element with the class value mw-parser-output. That element, in turn, is inside the element with the id value mw-content-text. We can select all p elements inside that container with this CSS selector: #mw-content-text .mw-parser-output p.\n",
      "We are now getting both the title and the first paragraph from the article page. We are returning them to the Puppeteer context as an array. We are using array destructuring to unpack these values. Let's also get the URL of the current page by using the window.location.href variable.\n",
      "This is looking pretty great. We can format these values that we are capturing using a template literal and display them on the screen using console.log.\n",
      "This code works great so far, but I am noticing that the description text is sometimes empty. Looking at the article page, this seems to happen when the first p element has a class called mw-empty-elt. Let's update our code to check to see if the first element's class name is equivalent to mw-empty-elt. If so, we would use the second p element instead. We can use the document.querySelectorAll function to get an array of all HTML elements that match the given CSS selector.\n",
      "This program is now in a pretty good spot! We have added the logic to choose the second paragraph if the first one has the class name mw-empty-elt.\n",
      "And that is pretty much it for this project! One thing to note is how we rely on specific ID and class names to be present on the webpage for our program to work. If the HTML and CSS structure of the website we are scraping is to be updated, we would also need to update our program.\n",
      "Performing manual operations in a programmatic way gives us a lot of leverage. If we have a program that can access a single website, it can be a simple matter to scale it to access thousands of websites.\n",
      "This can be problematic when interacting with the web. If we were to load thousands of pages from a single domain in a short amount of time, it could potentially overwhelm the servers hosting those pages. It can even be interpreted as an attack by the website. Our IP can temporarily get blocked from accessing their resources or even get banned. We need to be mindful when using websites programmatically. We might want to add artificial delays in between our operations to slow our program down. We also need to be careful about what data we can access programmatically. Some websites try to limit programmatic access to protect their data, or there can even be legal implications for accessing and storing certain kinds of data.\n",
      "Automating a browser is not the only way to access data on a webpage. There are many web applications out there that expose an API to connect developers with their resources. An API is an Application Programming Interface that we can use to interface with a resource programmatically. Using APIs, developers can build applications on top of popular services such as Twitter, Facebook, Google, or Spotify.\n",
      "In this post, we used Puppeteer in Node.js to scrape data from websites. We have used the JavaScript async-await structure to manage the asynchronous data flow. We have also used CSS selectors to grab data from the HTML structure inside a web page using DOM API methods such as document.querySelectorAll\n",
      "Web scraping is the act of using programs like Puppeteer to access and harvest data from websites programmatically. There can be legal implications for web scraping, so you should do your own research before engaging in such an action.\n",
      "Automating a browser is not the only way to access data on a webpage. There are many web applications out there that expose an API to connect developers with their resources. An API is an Application Programming Interface that we can use to interface with a resource programmatically. Using APIs, developers can build applications on top of popular services such as Twitter, Facebook, Google, or Spotify.\n",
      "Hope you enjoyed this post! Feel free to check out my book Awesome Coding that teaches coding using JavaScript and Node.js with cool and useful projects such as this!\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping 101! Introduction to Web Scraping in Python.\n",
      "URL: https://dev.to/phylis/web-scrapping-101-introduction-to-web-scrapping-in-python-56cn\n",
      "Answer: Web Scraping makes it easier and faster when you have to pull a large amount of data from websites.\n",
      "It is an automated method used to extract large amounts of data from websites.\n",
      "The data on the websites are unstructured therefore Web scraping helps collect these unstructured data and store it in a structured form.\n",
      "Job listings: Details regarding job openings, interviews are collected from different websites and then listed in one place so that it is easily accessible to the user.\n",
      "Social Media Scraping: used to collect data from Social Media websites such as Twitter to find out what‚Äôs trending.\n",
      "Email address gathering:companies that use email as a medium for marketing, use web scraping to collect email ID and then send bulk emails.\n",
      "E-Commerce pricing:e-commerce sellers use web scraping to collect data from online shopping websites and use it to compare the prices of products.\n",
      "Academic Research: Since data is an integral part of any research, be it academic, marketing or scientific, Web Scrapping helps you gather structured data from multiple sources in the Internet with ease.\n",
      "Requests\n",
      "It is a Python library used for making various types of HTTP requests like GET, POST.\n",
      "BeautifulSoup\n",
      "It is the most widely used Python library for web scraping that creates a parse tree for parsing HTML and XML documents.\n",
      "Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\n",
      "Selenium\n",
      "It is a Python library originally made for automated testing of web applications.\n",
      "It is a web driver made for rendering web pages and also to scrape dynamically populated web pages.\n",
      "lxml\n",
      "It is a high performance, blazingly fast, production-quality HTML, and XML parsing Python library.\n",
      "Works well when scrapping large datasets and also allows you to extract data from HTML using XPath and CSS selectors.\n",
      "Scrapy\n",
      "It is not just a library but a web scraping framework.\n",
      "Scrapy provides spider bots that can crawl multiple websites and extract the data.\n",
      "To extract data using BeautifulSoup and Requests libraries following steps are followed:\n",
      "Sending an HTTP GET request to the URL of the webpage that you want to scrape, which will respond with HTML content using the Request library of Python.\n",
      "Fetching and parsing the data using Beautifulsoup and maintain the data in some data structure such as Dict or List.\n",
      "Analyzing the HTML tags and their attributes, such as class, id, and other HTML tag attributes. Also, identifying your HTML tags where your content lives.\n",
      "Storing the data in the required format eg CSV, XLSX or JSON\n",
      "Understanding and Inspecting the Data\n",
      "Inspection of the page which you want to scrape is the most important job in web scraping\n",
      "Without knowing the structure of the webpage, it is very hard to get the needed information.\n",
      "In this article, we will scrap data available Here\n",
      "To inspect data that you wish to scrape, right-click on that text and click on inspect to examine the tags and attributes of the element.\n",
      "Install the Essential Python Libraries\n",
      "Importing the Essential Libraries\n",
      "Import the \"requests\" library to fetch the page content and bs4 (Beautiful Soup) for parsing the HTML page content.\n",
      "Collecting and Parsing a Webpage\n",
      "In the next step, we will make a GET request to the url and will create a parse Tree object(soup) with the help of BeautifulSoup and Python built-in \"lxml\" parser.\n",
      "And our output will be:\n",
      "\n",
      "We can also print some basic information from it:\n",
      "print the title of the webpage.\n",
      "Output\n",
      "Print links in the page along with its attributes, such as href, title, and its inner Text.\n",
      "Sample Output.\n",
      "\n",
      "Finding all instances of a tag at once.\n",
      "Sample output\n",
      "\n",
      "Searching for tags by class and id\n",
      "We can use the find_all method to search for items by class or by id.\n",
      "In this case we‚Äôll search for any h tag that has the class entry_title:\n",
      "Searching for elements by id:\n",
      "We can also search for items using CSS selectors.\n",
      "Example:\n",
      "Lets now use CSS selectors to find all the p tags in our page that are inside of a div;\n",
      "Output\n",
      "Now we have understood the python web scrapping basics.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Javascript and Node.js\n",
      "URL: https://dev.to/anderrv/web-scraping-with-javascript-and-node-js-2d\n",
      "Answer: Javascript and web scraping are both on the rise. We will combine them to build a simple scraper and crawler from scratch using Javascript in Node.js.\n",
      "Avoiding blocks is an essential part of website scraping, so we will also add some features to help in that regard. And finally, parallelize the tasks to go faster thanks to Node's event loop.\n",
      "For the code to work, you will need Node (or nvm) and npm installed. Some systems have it pre-installed. After that, install all the necessary libraries by running npm install.\n",
      "We are using Node v12, but you can always check the compatibility of each feature.\n",
      "Axios is a \"promise based HTTP client\" that we will use to get the HTML from a URL. It allows several options such as headers and proxies, which we will cover later. If you use TypeScript, they \"include TypeScript definitions and a type guard for Axios errors.\"\n",
      "Cheerio is a \"fast, flexible & lean implementation of core jQuery.\" It lets us find nodes with selectors, get text or attributes, and many other things. We will pass the HTML to cheerio and then query it as we would in a browser environment.\n",
      "Playwright \"is a Node.js library to automate Chromium, Firefox and WebKit with a single API.\" When Axios is not enough, we will get the HTML using a headless browser to execute Javascript and wait for the async content to load.\n",
      "The first thing we need is the HTML. We installed Axios for that, and its usage is straightforward. We'll use scrapeme.live as an example, a fake website prepared for scraping.\n",
      "Nice! Then, using cheerio, we can query for the two things we want right now: paginator links and products. To know how to do that, we will look at the page with Chrome DevTools open. All modern browsers offer developer tools such as these. Pick your favorite.\n",
      "\n",
      "We marked the interesting parts in red, but you can go on your own and try it yourselves. In this case, all the CSS selectors are straightforward and do not need nesting. Check the guide if you are looking for a different outcome or cannot select it. You can also use DevTools to get the selector.\n",
      "\n",
      "On the Elements tab, right-click on the node ‚û° Copy ‚û° Copy selector.\n",
      "But the outcome is usually very coupled to the HTML, as in this case: #main > div:nth-child(2) > nav > ul > li:nth-child(2) > a. This approach might be a problem in the future because it will stop working after any minimal change. Besides, it will only capture one of the pagination links, not all of them.\n",
      "We could capture all the links on the page and then filter them by content. If we were to write a full-site crawler, that would be the right approach. In our case, we only want the pagination links. Using the provided class, .page-numbers a will capture all and then extract the URLs (hrefs) from those. The selector will match all the link nodes with an ancestor containing the class page-numbers.\n",
      "As for the products (Pok√©mon in this case), we will get id, name, and price. Check the image below for details on selectors, or try again on your own. We will only log the content for now. Check the final code for adding them to an array.\n",
      "\n",
      "As you can see above, all the products contain the class product, which makes our job easier. And for each of them, the h2 tag and price node hold the content we want. As for the product ID, we need to match an attribute instead of a class or node type. That can be done using the syntax node[attribute=\"value\"]. We are looking only for the node with the attribute, so there is no need to match it to any particular value.\n",
      "There is no error handling, as you can see above. We will omit it for brevity in the snippets but take it into account in real life. Most of the time, returning the default value (i.e...., empty array) should do the trick.\n",
      "Now that we have some pagination links, we should also visit them. If you run the whole code, you'll see that they appear twice - there are two pagination bars.\n",
      "We will add two sets to keep track of what we already visited and the newly discovered links. We are using sets instead of arrays to avoid dealing with duplicates, but either one would work. To avoid crawling too much, we'll also include a maximum.\n",
      "For the next part, we will use async/await to avoid callbacks and nesting. An async function is an alternative to writing promise-based functions as chains. In this case, the Axios call will remain asynchronous. It might take around 1 second per page, but we write the code sequentially, with no need for callbacks.\n",
      "There is a small gotcha with this: await is only valid in async function. That will force us to wrap the initial code inside a function, concretely in an IIFE (Immediately Invoked Function Expression). The syntax is a bit weird. It creates a function and then calls it immediately.\n",
      "As said before, we need mechanisms to avoid blocks, captchas, login walls, and several other defensive techniques. It is complicated to prevent them 100% of the time. But we can achieve a high success rate with simple efforts. We will apply two tactics: adding proxies and full-set headers.\n",
      "There are Free Proxies even though we do not recommend them. They might work for testing but are not reliable. We can use some of those for testing, as we'll see in some examples.\n",
      "Note that these free proxies might not work for you. They are short-time lived.\n",
      "Paid proxy services, on the other hand, offer IP Rotation. Meaning that our service will work the same, but the target website will see a different IP. In some cases, they rotate for every request or every few minutes. In any case, they are much harder to ban. And when it happens, we'll get a new IP after a short time.\n",
      "We will use httpbin for testing. It offers several endpoints that will respond with headers, IP addresses, and many more.\n",
      "The next step would be to check our request headers. The most known one is User-Agent (UA for short), but there are many more. Many software tools have their own, for example, Axios (axios/0.21.1). In general, it is a good practice to send actual headers along with the UA. That means we need a real-world set of headers because not all browsers and versions use the same ones. We include two in the snippet: Chrome 92 and Firefox 90 in a Linux machine.\n",
      "Until now, every page visited was done using axios.get, which can be inadequate in some cases. Say we need Javascript to load and execute or interact in any way with the browser (via mouse or keyboard). While avoiding them would be preferable - for performance reasons -, sometimes there is no other choice. Selenium, Puppeteer, and Playwright are the most used and known libraries. The snippet below shows only the User-Agent, but since it is a real browser, the headers will include the entire set (Accept, Accept-Encoding, etcetera).\n",
      "This approach comes with its own problem: take a look a the User-Agents. The Chromium one includes \"HeadlessChrome,\" which will tell the target website, well, that it is a headless browser. They might act upon that.\n",
      "As with Axios, we can provide extra headers, proxies, and many other options to customize every request. An excellent choice to hide our \"HeadlessChrome\" User-Agent. And since this is a real browser, we can intercept requests, block others (like CSS files or images), take screenshots or videos, and more.\n",
      "Now we can separate getting the HTML in a couple of functions, one using Playwright and the other Axios. We would then need a way to select which one is appropriate for the case at hand. For now, it is hardcoded. The output, by the way, is the same but quite faster when using Axios.\n",
      "We already introduced async/await when crawling several links sequentially. If we were to crawl them in parallel, just by removing the await would be enough, right? Well... not so fast.\n",
      "The function would call the first crawl and immediately take the following item from the toVisit set. The problem is that the set is empty since the crawling of the first page didn't occur yet. So we added no new links to the list. The function keeps running in the background, but we already exited from the main one.\n",
      "To do this properly, we need to create a queue that will execute tasks when available. To avoid many requests at the same time, we will limit its concurrency.\n",
      "If you run the code above, it will print numbers from 0 to 3 almost immediately (with a timestamp) and from 4 to 7 after 2 seconds. It might be the hardest snippet to understand - review it without hurries.\n",
      "We define queue in lines 1-20. It will return an object with the function enqueue to add a task to the list. Then it checks if we are above the concurrency limit. If we are not, it will sum one to running and enter a loop that gets a task and runs it with the provided params. Until the task list is empty, then subtract one from running. This variable is the one that marks when we can or cannot execute any more tasks, only allowing it below the concurrency limit. In lines 23-28, there are helper functions sleep and printer. Instantiate the queue in line 30 and enqueue items in 32-34 (which will start running 4).\n",
      "We have to use the queue now instead of a for loop to run several pages concurrently. The code below is partial with the parts that change.\n",
      "Remember that Node runs in a single thread, so we can take advantage of its event loop but cannot use multiple CPUs/threads. What we've seen works fine because the thread is idle most of the time - network requests do not consume CPU time.\n",
      "To build this further, we need to use some storage (database) or distributed queue system. Right now, we rely on variables, which are not shared between threads in Node. It is not overly complicated, but we covered enough ground in this blog post.\n",
      "We'd like you to part with four main points:\n",
      "We can build a custom web scraper using Javascript and Node.js using the pieces we've seen. It might not scale to thousands of websites, but it will run perfectly for a few ones. And moving to distributed crawling is not that far from here.\n",
      "If you liked it, you might be interested in the Python Web Scraping guide.\n",
      "Thanks for reading! Did you find the content helpful? Please, spread the word and share it. üëà\n",
      "Originally published at https://www.zenrows.com\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with F#\n",
      "URL: https://dev.to/tunaxor/web-scrapping-with-f-1fd7\n",
      "Answer: NOTE: The content of this post is based on this code, check it for the full example.\n",
      "https://github.com/AngelMunoz/Escalin\n",
      "Hey there, this is the next entry in Simple Things in F#\n",
      "If you've ever wanted to pull data periodically from a website, or you are a QA automation person looking to do E2E (end to end) testing, then Playwright might be an option for you. Similar to Cypress or PhantomJS, Playwright is a library that allows you to automate ineractions with websites, you can even take screenshots and PDFs!\n",
      "Playwright offers access to the following browsers\n",
      "Normally these tools are made with javascript in mind (playwright is no exception) but, Playwright offers .NET libraries as well so if you like to use F#, VB or even C#, you can do some web scrapping, E2E with playwright.\n",
      "We will focus on F# here so you are required to have the .NET SDK installed on your machine, also you will need the playwright global cli tool (there's an npm version as well if you prefer to install that\n",
      "Once installed we can create a new console project in the following way:\n",
      "In this case I made a poject called Escalin, once created the project we'll install these dependencies.\n",
      "SCRIPTING: You can actually use playwright with F# scripts as well but you will need to install the playwright browsers first on that machine either by creating a dummy project and run the dotnet tool or using playwright npm tool to download them\n",
      "Once we have our dependencies ready, we can start digging in with the code in VSCode using Ionide, Rider or Visual Studio.\n",
      "For today's exercise we will do a web scrapping of my own blog, and get a list of the post summaries in the index page and save them as a json file\n",
      "To do that, we will need to do the following:\n",
      "Let's start with the namespaces and a few types we will need to get our work done.\n",
      "Also, our main's goal is to have something like this:\n",
      "That means we will need to create the following functions\n",
      "in the case of Async.AwaitTask and Async.RunSynchronously it's not necessary since they are FSharp.Core implementations, we'll also use the pipe operator |> to apply the last's function result as a parameter for the next function.\n",
      "The pipe operator is very useful in F# it could also make it to javascript at some point\n",
      "if we want to visualize that in another way, we can think of it as this:\n",
      "64 |> addNumbers 10 is equivalent to addNumbers 10 64\n",
      "Let's get started with getBrowser\n",
      "NOTE: I changed the parameters here vs the source code be more readable\n",
      "In this case, we're not doing much rather than just creating a browser instance and returning it, think about it as a simple helper function that you can also modify to pass in browser options and other things if you need them further down the line.\n",
      "We are also taking the task as the parameter, so we can use the pipe operator easily the downside here I guess is that we have to do let! playwright = getPlaywright but I don't think too much about it, the benefit is that we can make our main function more legible and gives us a clear indication of how we want to proceed.\n",
      "The next is getPage\n",
      "This function is also short, we just open a new page and go yo a particular URL and ensure we did it correctly, once we're done that we just return the page\n",
      "The next function is getPostSummaries that will find all of the post summaries in the page we just visited on the last function.\n",
      "Before we get to the next one, we need to check what is convertElementToPost doing, how did we go from an element read only list to a post array? let's make a list of things we need to do in order to get a post so the code doesn't look too alien\n",
      "All of this, based on knowing that the content might come like this\n",
      "Phew! that was intense right? string handling is a mess specially if I'm around, that's what my mind could produce but hey as long as it works! the other web scrapping thing we did here was at the beggining, once we knew we were inside a card, we could safely query elements and know they were going to be only children of that card after we processed the text we're ready to go.\n",
      "Let's get to the last step in our main writePostsToFile, this will just take the post array task we returned on the last function chain and then just write that to the disk.\n",
      "Once we're done with all of that we just apply the result to Async.AwaitTask given that F#'s Async/Task aren't the same,\n",
      "check Async and Task docs to have a better overview\n",
      "F# doesn't really have an async main so that's why we run that last task synchronously and return 0 at the end\n",
      "The result should look like this\n",
      "NOTE: that gif contains old code but produces the same output\n",
      "GIF\n",
      "The process I went through to get to this code was basically to go to my blog, inspect it with my browser and start analyzing the website's structure, once I kind of knew what was the ideal path to do it and what where the classes/elements I needed to look for I started with the web scrapping part.\n",
      "Keep in mind that playwright has many many options, you can perform clicks, text inputs get screenshots, pdfs, do mouse events and a lot of things that can help you archieve your goals either by doing Testing or doing some web scrapping as I just showed you.\n",
      "F# is a pretty concise language, and just think about if for a minute, async and parallel programing could be some of the most complex to mentalize yet we just did both and even mixed them in a way that really felt natural or at least I hope it felt that way for you as well isn't that amazing?\n",
      "have fun and I will see you again in the next entry!\n",
      "From F# 6.0 and .NET6 Ply is not needed anymore, task {} has been incorporated into F# Core\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping tutorial with Scraper API\n",
      "URL: https://dev.to/smusca/web-scraping-tutorial-with-real-time-crawler-22m3\n",
      "Answer: Web scraping is an irreplaceable tool in nowadays marketing world, used by companies to stay competitive and create more sales. Analysing scraped data allows companies to compare their offered content, merchandise, prices and availability. In most cases, comparing prices to your competitors and lowering them by at least in 1$, can boost up the sales for a long time. On the other hand, if you are not a big company, it might get tricky of where you should start, and what you should do to get all the data that you need.\n",
      "To start with, you should decide if you want to write a web crawler yourself, or want to use an already made web scraping tools for that. Writing your own crawler could be useful, if you know the specific content needed, getting data in a specific language or using the exact methods. Downside of this - you need to have people, who could code and who would create a useful tool for you. Also, it needs combining your own crawler code, buying proxies and exporting all gathered data in easy-to-read way. If you do not have a team that could provide you with such tool, there is always a possibility to use pre-made tools. Pre-made web scraping tools downloads specific web pages and extracts data that is required, such as a list of items available, their prices, availability, and other details. Let‚Äôs consider one of the tools and check how it works - Scraper APIs .\n",
      "This tool works in a pretty simple way - user makes a request about what data is needed, the crawler receives the request and tries to access the data. If it is successful, crawler then sends data back to the user.\n",
      "\n",
      "If you want to see it in action, you can always try out their sample on their web page - you can try it out with a search engine, or with e-commerce search for websites. As it uses ASIN (Amazon Standard Identification Number), all you need is to paste a product number in the field, and get data about the product in JSON or HTML formats.\n",
      "\n",
      "You can extract data from product pages, product offer listing pages, reviews, questions & answers, search results or from any URL in general. There are also two options in how to retrieve data - with the callback data delivery method, you don't need to check your task status ‚Äì Scraper API lets you know once the data is ready. With real-time data delivery, the data is retrieved on the same connection. Proxies enable data collection without IP bans, this way assuring anonymity as well. The same goes for the Scraper API as it uses both data center and residential proxies.\n",
      "As for search engines, you can make a request in two languages - Python or PHP. You can also write your command in shell. All you need is a keyword, domain, language and country, which will be used for search results. Scraper API supports any number of requests done for any location and any keyword. High accuracy is ensured by the use of natural geo-located IP addresses. This is how your request would look.\n",
      "\n",
      "Your results would be extracted in to given link. In this case - The result is a SERP (search result page) in JSON format. It has a HTML-code inside JSON. So, we need to parse JSON to see the result page.\n",
      "All the parameters needed to form a payload, and all the examples of how the crawler works, can be found on Oxylabs learning hub. They also explain their other tools as well as how to write a code for specific searches or extractions.\n",
      "All in all, Scraper API is an easy to use convenient tool for both e-commerce and search engine crawling, which can help your business to become more profitable and gather data that is needed.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping using Python! Create your own Dataset\n",
      "URL: https://dev.to/prashant2018/web-scraping-using-python-create-your-own-dataset-50n5\n",
      "Answer: Machine Learning requires a lot of data and not always it is easy to get the data you want. Have you ever wondered how Kaggle and other such websites provide us with huge datasets? The answer is web scraping. So, let us see how we can extract data from the web.\n",
      "Let‚Äôs assume we are building a model which requires movie information such as title, summary, and rating of a number of movies. When it comes to movies, we know IMDB has the largest database. Let us dig into it.\n",
      "There‚Äôs a pattern in everything. We need to observe and find a pattern in the HTML code of the web page to extract relevant data. Let‚Äôs go step by step. We will be doing everything using python and scrape the data from the following URL :\n",
      "https://www.imdb.com/search/title?release_date=2019&sort=user_rating,desc&ref_=adv_nxt\n",
      "1. Install dependencies\n",
      "2. Download the webpage\n",
      "‚ÄúRequests‚Äù is a great HTTP library to make request calls. We will use it to download the webpage of the given URL.\n",
      "3. Inspecting elements and finding the pattern\n",
      "Now the data we have downloaded is exactly the same you see when you right-click and do inspect element in the browser. Let‚Äôs right-click on the rating and see how we can extract it.\n",
      "\n",
      "When we look closely we will see the class ‚Äúratings-bar‚Äù contains the rating of the movie. If we inspect other movies, we will find all the movies have the same class name for the ratings on that page. Here, we found a pattern to extract all the ratings from the page. Similarly, we can extract summary, title, genre, etc.\n",
      "Not only using class but you can select a specific part of the HTML code using id, tags, etc as well.\n",
      "Let‚Äôs jump into the code!\n",
      "BeautifulSoup allows us to extract data(more precisely parse data) from HTML using the class name, id, tags, etc. Isn‚Äôt it Beautiful? :-D\n",
      "To select the content from the page we use CSS Selectors. CSS Selectors allows us to select different classes, ids, tags, and other html elements easily. CSS Selector for Class is \".\" and for ID is \"#\". To select a class we need to prefix a \".\" to the class name we want to extract and similarly, for ID we need to prefix \"#\".\n",
      "This ‚Äúrating_list‚Äù is the list of object containing all the <div> elements containing ‚Äúratings-bar‚Äù as class name. We need to get the text from within the div element.\n",
      "Here‚Äôs how a single rating object looks like:\n",
      "We need to get the rating value from the <strong> tag. We can extract the tags using find(‚ÄòtagName‚Äô) method and get the text using getText().\n",
      "And we are done. Similarly, you can extract Titles, Summary, Genre using the above method with the appropriate class name and tag names.\n",
      "You can store the data to CSV or excel file and use it for your Machine Learning model.\n",
      "Full Code present on my Github:\n",
      "https://github.com/prashant2018/Medium-Article-Code-Snippets/tree/master/Web-Scraping-Using-Python\n",
      "Follow me on Twitter:\n",
      "https://twitter.com/prash2018\n",
      "--------------------------------------------------------------------------------\n",
      "Query: web scraping with python\n",
      "URL: https://dev.to/jonasjohnson/web-scraping-with-python-ik4\n",
      "Answer: I‚Äôve recently run into problems scraping the new google site. Anyone else?\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping with Python and AWS Lambda: A modern approach\n",
      "URL: https://dev.to/adelmofilho42/web-scraping-with-python-and-aws-lambda-a-modern-approach-1iok\n",
      "Answer: In December 2020, AWS started to support Lambda functions as container images, which is a real breakdown that allows us to deploy way more complex projects with the same you-pay-only-for-what-you-use pricing and serverless architecture.\n",
      "Web scraping workloads have real benefits from this Upgrade due to an easier installation of selenium.\n",
      "The Dockerfile bellow is based on the oficial lambda container image for python 3.8 (it is really awful to create this image from scratch).\n",
      "The python script below configures the Selenium with a Chrome headless. Note the path of the chrome driver at the driver definition - such path comes from the work directory of the base image.\n",
      "Finally, build and run the container image!\n",
      "In order to test your new web scraping containerized lambda function, run the following command.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web-scraping IMDb with R\n",
      "URL: https://dev.to/lorena/web-scraping-imdb-with-r-26np\n",
      "Answer: You don't need to be a data scientist to work with data. In fact, you most probably use a lot of data every day in both your professional and personal life. For example, price comparison sites, movie rankings, sales leads, and trending topics are all examples of data that is available online in some sort of table form.\n",
      "If you want to collect or analyse this data, copy-pasting each record is most likely out of the question. This manual task is not only time-consuming, but also expensive and prone to error ‚Äì not to mention it's also downright boring.\n",
      "Here's where automatic web-scraping can help! Web scraping is a method of automatically gathering data from websites in a structured manner and storing it into a local database or spreadsheet.\n",
      "There are many no-code tools for web scraping, like browser plug-ins (e.g. Webscraper) and software (e.g. Parsehub). However, if you need more advanced scraping settings and have basic coding skills, I recommend the Python libraries Beautiful Soup or Selenium, and the R package rvest. The latter is the one I used for scraping IMDb and you can find the commented code on my GitHub. \n",
      "Before I proceed to the fun part, note that the legality of web scraping is not clearly defined around the world, so you should check the website's terms of use before scraping it!\n",
      "Now let's dive in. I wanted my data analysis to answer three questions:\n",
      "I used IMDb as a reference, because it contains all the information I need. On the website I selected the movies released between 01.01.-31.12.2018, sorted by popularity, and limited my search to the first page, so the top 50 movies.\n",
      "The top 5 popular movies in 2018 were:\n",
      "Scraping the genre tags of each movie is pretty straightforward:\n",
      "However, this returns a list of genres for each movie, because the movies are labeled with multiple genres. The text data needs to be cleaned a bit:\n",
      "Now, another tricky thing is that the genres of each movie are enumerated alphabetically, not in order of importance. To simplify my work, I selected only the first genre:\n",
      "At a glance, I noticed that 3 out 5 are action-hero movies, so I visualized closer at the genre distribution:\n",
      "\n",
      "My initial observation was confirmed: Action and Drama are the most popular genres, followed by Biography. I guess most people enjoy, on one hand, movies that transport them into wild worlds and simulate experiences out of the ordinary, and on the other hand, movies that depict dramatic life stories and relate to some extent to their real life.\n",
      "Next, I analyzed the distribution of movie duration:\n",
      "The plot shows that most popular movies last on average 104 minutes (median 117 minutes). The longest movie is Avengers: Infinity War (149 minutes) and the shortest movie (excluding TV-shows) is A.I. Rising (85 minutes). From the histogram it is clear that the bars on the left represent the TV-shows (under 60 minutes).\n",
      "\n",
      "I also analyzed the runtime distribution by genre. First, I aggregated the movies by genre:\n",
      "Then, I visualized the average duration for each genre:\n",
      "I found that among genres Biographies are longest (on average 127 minutes) and Crimes are shortest (on average 85 minutes). This is not entirely surprising, since I think that, first, it is quite a challenge to pack a lifetime in a biographical movie, and second, there's only so much nerve-wrecking tension a person can take following a crime. However, I was expecting the average duration of Animations to be shorter than 110 minutes, because they are produced mainly for children, who have a short attention span and low patience to sit through a two-hour movie. But then again, we are talking about the most popular movies of last year on IMDb, which means that adults made up the large audience.\n",
      "\n",
      "This is a simple web scraping project that can reveal a lot of information about people's movie preferences. It would be interesting to also analyze the total gross and see which movies and genres have sold best in 2018. Now you could try to scrape and analyze this information with your preferred tool and let me know what you found out!\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping in PHP using Goutte - part 2\n",
      "URL: https://dev.to/sayopaul/web-scraping-in-php-using-goutte---part-2-5e66\n",
      "Answer: In the last article, we got introduced to web scraping and we looked into Goutte, a wonderful PHP web scraping library . In this article, we would be putting our knowledge to practice by scraping the website of the Punch . To be more specific, we would be scraping the punch to get the lastest news https://punchng.com/topics/news headlines üòé .\n",
      "Let's get right into it üí™ !\n",
      "NB : This is for testing purposes only, I do not in any way intend to reproduce the material gotten from the Punch and I do not advice you to do so as that would be copyright infringement .\n",
      "First things first, we set up Composer autoloading, import the Goutte namespace and we instantiate a new Goutte Client:\n",
      "The next step is to send a request via the $client object . The $client object returns a crawler instance . It is this instance that we use to apply our filters .\n",
      "On the front page of the Punch news page are article boxes . Each article has its own box and a heading ( The headline ) with the class \".seg-title\" . We want to select all the headlines (.seg-title) on the page and then take each of them one by one . We do it with this:\n",
      "Notice the method each() ? The each() method allows us to iterate over the current selection(node list) when it contains more than one node . As we mentioned above, we are selecting each of the headlines (.seg-title) hence we have more than one node and we want to iterate through them . Underground, the each() method accepts an instance of an anonymous function, loops through the current node list and then passes a node on each iteration to the closure thus allowing us to access the current node ( $node ) in the closure .\n",
      "Alright, the next thing we want to do is extract the text from the current node .\n",
      "We get the textual content of the node by calling the method text() . The next thing we do is print out the headline and there we have it ! We would always get the latest 10 news headlines on the punch printed out to us whenever we run this script . Like I said in the previous article, when it comes to scraping, almost anything is possible ( even logging in and filling forms ) . The limit is your mind üòä . I honestly wish we could go deeper but sadly that's all for now üòÖ .\n",
      "For more information, please do well to read the docs of DomCrawler, CssSelector and Goutte .\n",
      "Do you have any web scaping needs ? You can hire me to help you out here\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping in PHP using Goutte\n",
      "URL: https://dev.to/sayopaul/web-scraping-in-php-using-goutte--4p6h\n",
      "Answer: Today I would be talking about something very common, Web Scraping . Depending on your needs or a client's needs, situations may arise when you may need to extract data from a webpage .\n",
      "What is Web Scraping ?\n",
      "According to WebHarvy, Web Scraping (also termed Screen Scraping, Web Data Extraction, Web Harvesting etc.) is a technique employed to extract large amounts of data from websites . In its simplest form, web scraping is getting the contents of a webpage via a script . Alright, let's move on to web scraping in PHP . Recently, I needed to scrape a site for a client in PHP so I looked for articles that talked about web scraping in PHP and I found out that there were few and most of them were pretty outdated .\n",
      "However, in my research, I came across Goutte ; a (wonderful) screen scraping and web crawling library for PHP . At its core, Goutte is a wrapper around three of Symfony's components ( God bless Fabien üôå) ; BrowserKit, CssSelector and DomCrawler . It is important for us to understand what each of these components does as it helps us to understand just how powerful Goutte is .\n",
      "BrowserKit ;\n",
      "Simply put, the BrowserKit component simulates the behaviour of a real browser . It is the foundational element of Goutte.\n",
      "DomCrawler;\n",
      "The DomCrawler component eases the navigation of the DOM ( Document Object Model ) . The DomCrawler allows us to navigate the dom like this:\n",
      "We can also traverse through nodes on the DOM using some of the methods that it provides . For example, if we want to get the first paragraph in the body of the page we could do this:\n",
      "The eq() method is zero indexed and it takes a number specifying the position of the element we want to access .\n",
      "There are other methods such as siblings(), first() [an alias of eq(0), underground it just calls eq(0) ], last() etc .\n",
      "CssSelector;\n",
      "The CssSelector is a wonderful component that allows us to select elements via their CSS selectors . It does this by converting the CSS selectors to their XPath equivalents . So for example say we wanted to select an element with a class called \"fire\" we could do this:\n",
      "The CssSelector component is so amazing that it even supports CSS such as ;\n",
      "The above means that we are looking for a div element with an inline style attribute of \"style=max-height:175px; overflow: hidden;\"\n",
      "For more information, please do well to read the docs of DomCrawler, CssSelector and Goutte .\n",
      "Alright now that we have a bit of an idea about the three major components, it is time for us to bring everything together and actually scrape something . As you may have realised by now,when it comes to scraping, there is no laid down way to do it . You are free to explore and try out so many ways to get your data . The only limit you have is your creativity . There are times where I have had to combine the CssSelector and DomCrawler in order to get what I want [ actually, a lot of times ] .\n",
      "In the next post we are going to put everything that we have learnt so far in to play by scraping the website of the Punch .\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Puppeteer üé≠\n",
      "URL: https://dev.to/kirillinoz/web-scraping-with-puppeteer-kj7\n",
      "Answer: Internet is a wide place full of information. Here you can find everything from cute kitten videos up to scientific researches. This information isn't only useful to us, but it could become vital for our websites and applications.\n",
      "There are a few ways to access the necessary data, Rest APIs, public databases and web scraping. Puppeteer is an awesome tool for getting the last one done. In this post, I want to help you discover this tool for yourself and show you what it's capable of.\n",
      "Let's get the first question out of the way.\n",
      "Puppeteer is a Node library which provides a high-level API to control Chrome or Chromium over the DevTools Protocol.\n",
      "That's what the official documentation says. Complicated isn't it? In simpler terms, we're dealing with an invisible browser which is controlled via code.\n",
      "After installing the Puppeteer NPM package, you'll have to write some code and show Puppeteer where and which information you would like to get.\n",
      "Note that most of Puppeteer's methods are asynchronous, so you'll have to use an async function or then method. For upcoming examples, I'll stick to the first one.\n",
      "I think that the best method to learn something new is by practicing. So let's take an example for web scraping with Puppeteer and break it down. I'll try to explain every step as best as I can. But if you're stuck or you want to find out more about a specific method, I encourage you to check out the official documentation.\n",
      "For starters, we need to check if the website allows web scraping at all. For this, we will check if the website contains a robot.txt file. If not, we are good to go. Otherwise, you'll see which restrictions are placed by the owner of the website.\n",
      "In this example we will be scraping some data from the official English Premier League website, to be more precise, a table of football players and their total amount of goals per season.\n",
      "With the following code we will launch our browser with a predefined viewport. And navigate to the website while leaving some time for all components to load at the end of the code.\n",
      "Note: websites may differ according to the client's viewport that's why it's important to predefine it.\n",
      "Now let's scrape the table we're seeing on the website. For that, we will use $$eval to find all the table components using HTML elements as the selector and then getting the innerText in each of the selected elements.\n",
      "Now we just have an array filled up with all the strings that we could find in the table. We have empty strings and unnecessary spacing. We should also split this array into smaller arrays containing the data for each individual player and then create an object out of each array to make it easier to read and access for the front-end.\n",
      "Now that we've collected our data about each and every footballer in the table, we could also use some images of each football player. The website, we were currently on, doesn't provide us with any photos, so let's start a new session and go to the well-known website which finds billions of photos across the whole internet. I'm talking about Google Images.\n",
      "First, we start a new session and open a new page. Then we use the array seasonData to get the name of each footballer. The name needs some adjustment because we will pass it into an URL and any space has to be replaced with +. Afterwards we will need to select the first image on the page. This can be done using XPath which gives the shortest unique path to every element on a website. Then we select our element and get the image URL. In the end, we should add it as a property to our player object.\n",
      "Now using Node's file system we can save our seasonData array to a JSON file, pass the data to the front-end or create a Rest API.\n",
      "This example is just the tip of the iceberg. You can do other things with the Puppeteer library such as interact with elements, take screenshots and more. If you want to find out more about it, check out the official documentation.\n",
      "I hope I could waken interest in you to learn more about this awesome JavaScript library.\n",
      "Thank you for your time! ‚ù§\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Python : Beautiful Soup\n",
      "URL: https://dev.to/biomathcode/web-scraping-with-python-beautiful-soup-39kh\n",
      "Answer: >\n",
      "Article is about web scraping from single HTML document using beautiful soup python module. Step-by-step guide to do web scraping with beautiful soup. You should have basic knowledge about html tags and python programming language.\n",
      "There are three main python modules that are used for web scraping:\n",
      "1. Beautiful Soup - This article\n",
      "Beautiful Soup parses a single HTML document\\ so you can get data out of it in a structured way.\n",
      "2. Scrapy\n",
      "Scrapy is a comprehensive scraping framework\\ that recursively follows links based on rules you can set and automates a lot of the most onerous minutiae of scraping large amounts of data.\n",
      "3. Selenium\n",
      "Selenium is an entirely different tool, a browser Automator\\ that has many purposes besides scraping, but can be used to make scraping more efficient, mostly by rendering JavaScript and other dynamically populated data into HTML that is then readable by Scrapy or BeautifulSoup without having to perform direct HTTP requests or use something like Splash to render the JavaScript.\n",
      "Here is step-by-step process to do web scraping. You would have to change the code to get the desired html tags, and target them to get the desired data from the website. The process will remain the same, mostly.\n",
      "Before getting Started install bs4 :\n",
      "In terminal using pip\n",
      "conda\n",
      "This is the website that you want to scrape data from. For this tutorial I will be scrapping positive affirmations from a url.\n",
      "Url: - https://www.loudlife35.com/2019/06/500-positive-affirmations-that-will.html\n",
      "\n",
      "Here, you can see my target html tag is a \\ with the style attributes of font-size (15pt).\n",
      "Before you start scraping a web page, open up chrome dev tools with ctrl+I\\. See the html document structure. Html is made of nested tags, so the target tag of you will be nested deep within the document. Also, you might want to get multiple tags from a website.\n",
      "## Step 2: Read the web page with python\n",
      "The requests module allows you to send HTTP requests using Python.\n",
      "The HTTP request returns a Response Object with all the response data (content, encoding, status, etc).\n",
      "Make a request to a web page, and print the response text.\n",
      "There is not further use of request module if you want to learn more about the request module and sending other types of http requests such as DELETE, PUT, POST\n",
      "read here: - https://www.w3schools.com/python/module_requests.asp\n",
      "beautifulSoup provides us which methods such as find and find_all to search for specific tags that we want to find. The data you want from a website will be inclosed by some html tags that's what we need to find on a webpage.\n",
      "importing BeautifulSoup from bs4\n",
      "Parsing the html using beautifulSoup\n",
      "There are two method that are provided in the beautiful Soup module to find specific tags.\n",
      "find_all method is used to find all the similar tags that we are searching for by providing the name of the tag as argument to the method. find_all method returns a list containing all the HTML elements that are found. Following is the syntax:\n",
      "example:\n",
      "To find all the p tags do this:\n",
      "find method is used to find the first matching tag.\n",
      "As, i have mentioned above html tags that we want are the tag with style attribute of font-size. We can find all of the records with the find_all method in beautiful soup. The method takes first parameter - tag and the second - attributes.\n",
      "Now what we get is a list of all the span tags.\n",
      "Our data contains - html tags, numbers.\n",
      "records[0], look like this.\n",
      "To get the content with in the html tags, beautiful soup have two methods.\n",
      ".get_text() --> The get_text() method returns the text inside the Beautiful Soup or Tag object as a single Unicode string.\n",
      ".contents --> A tag‚Äôs children are available in a list called .contents.\n",
      "We can remove the numbers by regular expression. But i don't know that so i just used a python way of doing that.\n",
      "Nah, But seriously regular expression are great when you are scraping the web.\n",
      "'A loving relationship now brightens my life'\n",
      "Create a list.\n",
      "Loop through all the target htmls tags. A bit of removing the unwanted characters.\n",
      "And append it to the list.\n",
      "Using pandas library we can create a dataset and export the data frame as .csv file.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping through Python\n",
      "URL: https://dev.to/vamsi_dhananjay/web-scraping-through-python-4465\n",
      "Answer: \n",
      "Web scraping is the process of automatically extracting data and collecting information from the web.\n",
      "It could be described as a way of replacing the time-consuming, often tedious exercise of manually copy-pasting website information into a document with a method that is quick, scalable and automated. Web scraping enables you to collect larger amounts of data from one or various websites faster.\n",
      "It‚Äôs possible to do web scraping with many programming languages.\n",
      "However, one of the most popular approaches is to use Python and the Beautiful Soup library\n",
      "When we scrape the web, we write code that sends a request to the server that‚Äôs hosting the page we specified. The server will return the source code ‚Äî HTML, mostly ‚Äî for the page (or pages) we requested.\n",
      "So far, we're essentially doing the same thing a web browser does ‚Äî sending a server request with a specific URL and asking the server to return the code for that page.\n",
      "But unlike a web browser, our web scraping code won't interpret the page's source code and display the page visually. Instead, we'll write some custom code that filters through the page's source code looking for specific elements we‚Äôve specified, and extracting whatever content we‚Äôve instructed it to extract.\n",
      "Unfortunately, there‚Äôs not a cut-and-dry answer here. Many websites don‚Äôt offer any clear guidance one way or the other. Some websites explicitly allow web scraping. Others explicitly forbid it.\n",
      "Before scraping any website, we should look for a terms and conditions page to see if there are explicit rules about scraping.\n",
      "We can also check the robots.txt file for any website to see which pages are permitted to be scraped and which pages disallow scraping. Ex: https://www.facebook.com/robots.txt. Here, Facebook clearly specifies which pages are allowed to be accessed and which pages are not.\n",
      "The first thing we‚Äôll need to do to scrape a web page is to download the page. We can download pages using the Python requests library.\n",
      "The requests library will make a GET request to a web server, which will download the HTML contents of a given web page for us.\n",
      "We can use the BeautifulSoup library to parse the web page, and extract the text.\n",
      "We first have to import the library, and create an instance of the BeautifulSoup class to parse the web page.\n",
      "Then, We can print out the HTML content of the page using various BeautifulSoup methods.\n",
      "Let's scrape the content from YouTube's robot.txt file.\n",
      "First, we have to import requests library and send a GET request to download data from the desired location.\n",
      "Now, we have to import BeautifulSoup to parse the web page and print all the data that we have access to.\n",
      "Now, we can read the output and find out which pages YouTube allows to be scraped and which it does not\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping Using Selenium and Python: The Step-By-Step Guide for Beginner\n",
      "URL: https://dev.to/younglbrownh/web-scraping-using-selenium-and-python-the-step-by-step-guide-for-beginner-15lb\n",
      "Answer: The easiest websites to scrape data from are static pages that all content is downloaded upon request. Sadly, these types of sites are gradually fading out, and dynamic websites are gradually taking over.\n",
      "With dynamic sites, all content on a page is not provided upon loading a page ‚Äì the content is dynamically added after specific JavaScript events, which poses a different problem to scraping tools designed for static websites. Fortunately enough, with tools like Selenium, you are able to trigger JavaScript events and scrape any page you want, no matter how JavaScript-rich a page is.\n",
      "With Selenium, you are not tied to a single language like other tools. Selenium has support for Python, Ruby, Java, C#, and JavaScript. In this article, we will be making use of Selenium and Python to extract web data. Before we go into that in detail, it is wise if we look at Selenium and instances when you should make use of it.\n",
      "Selenium was not initially developed for web scraping ‚Äì it was initially developed for testing web applications but has found its usage in web scraping. In technical terms, Selenium or, more appropriately, Selenium WebDriver is a portable framework for testing web applications.\n",
      "In simple terms, all Selenium does is automate web browsers. And as the team behind Selenium rightfully put it, what you do with that power is up to you! Selenium has support for Windows, macOS, and Linux. In terms of browser support, you can use it for automating Chrome, Firefox, Internet Explorer, Edge, and Safari. Also important is the fact that Selenium can be extended using third-party plugins.\n",
      "With Selenium, you can automate the filling of forms, clicking buttons, taking a snapshot of a page, and other specific tasks online. One of these tasks is web extraction. While you can use it for web scraping, it is certainly not a Swiss Army Knife of web scraping; it has its own downside that will make you avoid using it for certain use cases.\n",
      "The most notable of its downsides is its slow speed. If you have tried using Scrapy or the combo of Requests and Beautifulsoup, you will have a speed benchmark that will get you to rank Selenium slow. This is not unconnected to the fact that it makes use of a real browser, and rendering has to take place.\n",
      "For this reason, developers only use Selenium when dealing with JavaScript-rich sites that you will find it difficult to call underlying APIs. With Selenium, all you do is automate the process, and all events will be triggered. For static sites that you can quickly replicate API requests, and all content is downloaded upon loading, you will want to use the better option, which is Scrapy or the duo of Requests and Beautifulsoup.\n",
      "Selenium is a third-party library, and as such, you will need to install it before you can make use of it. Before installing Selenium, make sure you already have Python installed. To install Python, you can visit the Python official download page. For Selenium to work, you will need to install the Selenium package and then the specific browser driver you want to automate. You can install the library using pip.\n",
      "For browser drivers, they have support for Chrome, Firefox, and many others. Our focus in this article is on Chrome. If you don‚Äôt have Chrome installed on your computer, you can download it from the official Google Chrome page. With Chrome installed, you can then go ahead and download the Chrome driver binary here.\n",
      "Make you download the driver for the version of Chrome you have installed. The file is a zip file with the actual driver inside of it. Extract the actual Chrome driver (chromedriver.exe) and place it in the same folder as any Selenium script you are writing.\n",
      "As it is in the coding tutorial tradition, we are starting this selenium guide with the classical hello world program. The code does not scrape any data at this point. All it does is attempt to log into an imaginary Twitter account. Let take a look at the code below.\n",
      "the username and password variables‚Äô values are dummies. When you run the above code, it will launch Chrome and then open the Twitter log in page. The username and password will be inputted and then sent.\n",
      "Because the username and password are not correct, it displays an error message, and after 5 seconds, the browser is closed. As you can see from the above, you need to specify the specific web browser, and you can see we did that on line 7. The get method sends GET requests. After the page has loaded successfully, we use the\n",
      "method to find the username and input elements and then use\n",
      "for filling the input fields with the appropriate data.\n",
      "Sending web requests using Selenium is one of the easiest tasks to do. Unlike in the case of other tools that differentials between POST and GET requests, in Selenium, they are sent the same way. All that‚Äôs required is for you to call the get method on the driver passing the URL as an argument. Let see how that is done in action below.\n",
      "Running the code above will launch Chrome in automation mode and visit the Twitter homepage and print the HTML source code of the page using the\n",
      ". You will see a notification below the address bar telling you Chrome is being controlled by an automated test software.\n",
      "From the above, Chrome gets launched ‚Äì this is the headful approach and used mainly for debugging. If you are ready to launch your script on a server or in a production environment, you wouldn‚Äôt want Chrome launched ‚Äì you will want it to work in the background. This method of running the Chrome browser without it launching is known as the headless Chrome mode. Below is how to run Selenium Chrome in headless mode.\n",
      "Running the code above will not launch Chrome for you to see ‚Äì all you see is the source code of the page visited. The only difference between this code and the one before it is that this one is running in the headless mode.\n",
      "There are basically 3 things involved in web scraping ‚Äì sending web requests, parsing page source, and then processing or saving the parsed data. The first two are usually the focus as they present more challenges.\n",
      "You have already learned how to send web requests. Now let me show you how to access elements in other to parse out data from them or carry out a task with them. In the code above, we use the\n",
      "method to access the page source. This is only useful when you want to parse using Beautifulsoup or other parsing libraries. If you want to use Selenium, you do not have to use the\n",
      "method. [su_list icon=\"icon: hand-o-right\" icon_color=\"#0E86D4\"]Below are the options available to you.\n",
      "For each of the\n",
      "methods, there is a corresponding method that retrieves a list of elements instead of one except for\n",
      ". Take, for instance, if you want to retrieve all elements with the ‚Äúthin-long‚Äù class, you can make use of the\n",
      "instead of\n",
      ". The difference is the plurality of the element keyword in the function.\n",
      "With the above, you can find specific elements on a page. However, you do not just do that for doing sake; you will need to interact with them either to trigger certain events or retrieve data from them. Let take a look at some of the interactions you can have with elements on a page using Selenium and Python.\n",
      "With the above, you have what is required to start scraping data from web pages. I will be using the above to scrape the list of US states their capital, population (census), and estimated population from the Britannica website. Take a look at the code below.\n",
      "Looking at the above, we put into practice almost all of what we discussed above. Pay attention to the trs variable. If you look at the source code of the page, you will discover that the list of states and the associated information are contained in a table. The table does not have a class neither does its body.\n",
      "Interestingly, it is the only table, and as such, we can use the find.element_by_tag_name(‚Äútbody‚Äù) method to retrieve the tbody element. Each row in the tbody element represents a state and its information, each embedded in a td element. we called the find.elements_by_tag_name(‚Äútd‚Äù) to retrieve the td elements.\n",
      "The first loop is for iterating through the tr elements. The second one is for iterating through the td elements for each of the tr elements. Element.text was used for retrieving text attached to an element.\n",
      "From the above, we have been able to show you how to scrape a page using Selenium and Python. However, you need to know that what you have learned is just the basics. There is more you need to learn. You will need to know how to carry out other moves and keyboard actions.\n",
      "Sometimes, just filling out a form with a text string at once will reveal traffic is bot-originating. In instances like that, you will have to mimic typing by filling in each letter one after the other. With Selenium, you can even take a snapshot of a page, execute custom JavaScript, and carry out a lot of automation tasks. I will advise you to learn more about the Selenium web browser on the official Selenium website.\n",
      "Selenium has its own setback in terms of slow speed. However, it has proven to be the best option when you need to scrape data from a rich JavaScript website.\n",
      "One thing you will come to like about Selenium is that it makes the whole process of scraping easy as you do not have to deal with cookies and replicating hard-to-replicate web requests. Interestingly, it is easy to make use of.\n",
      "Source, https://www.bestproxyreviews.com/selenium-web-scraping-python/ \n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Nodejs\n",
      "URL: https://dev.to/yoanbello/web-scraping-with-nodejs-1pfj\n",
      "Answer: I love sports, programming and learning new things. This is a simple project in which I do web scraping to the fabulous site https://www.basketball-reference.com/ and I get the results of the last day of the nba, I show them on a web page and send them by mail.\n",
      "El link del proyecto en mi github es https://github.com/yoanbello/scrapinBasketballReference.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping 101\n",
      "URL: https://dev.to/icecoffee/web-scraping-101-4fij\n",
      "Answer: What is Web Scraping?\n",
      "Ans: Try Google Search lol.\n",
      "As the name suggests it's just scraping data of the web sites using http request response cycle.\n",
      "Request response cycle is just two way communication between servers or computers browser is just an interface like messanger and servers are like users.\n",
      "Fun tip!!!\n",
      "Try writing\n",
      "curl https://www.example.com\n",
      "in your native terminal.\n",
      "That's also web scraping of sort.\n",
      "But let's just cut to the chase.\n",
      "So today we are gonna be scraping all the image from udemy.com, sounds fun :o!!!\n",
      "To get started first you need to install a couple of python modules\n",
      "requests, bs4 and lxml\n",
      "So requests package is used to make requests to the server,\n",
      "bs4 is a tool that make stuff looks pretty and easy to work with,\n",
      "and lxml well that's a parser to parse HTML response markup and goes along with bs4.BeautifulSoup() like a sauce.\n",
      "We're ready and good to go.\n",
      "Now just follow along.\n",
      "Step 1: Import all the necessary modules.\n",
      "Step 2: Declare variables name and url to make stuff easy.(totally optional)\n",
      "Step 3: Request server for HTML code via url and create a soup object of retrieved data via parser and bs4.BeautifulSoup() method.\n",
      "Step 4: Select and fetch all the image objects or nodes.\n",
      "Step 5: Parse all the images nodes one after the other to get image source which will help us to make requests for image data.\n",
      "Step 6: Select source(value of src attribute) of the image.\n",
      "At this time if you will try to print an image object it will look exactly like corresponding html code.\n",
      "Step 7: Make a request to the corresponding server to receive image data.\n",
      "So now we have image source, which can be used to in request.get() to ask some other server for \"The Image Data\". But we got to check for the type first because in HTML5 we can also use svg as source of an image that may be represented differently than the image. In short, we can surely save an svg's via this method but I just haven't added that functionality yet!\n",
      "In fact you can fetch any type of data GIF's, PNG's, JPG's, you name it.\n",
      "Step 8: Open a file in binary format to save data.\n",
      "Step 9: Write all the data received from imageNode.content attribute.\n",
      "Step 10: Bruh, you did it ! Congrats!!!\n",
      "Let the loop complete its job.\n",
      "Now you have a new script to show off.\n",
      "All the code at one place:\n",
      "You did a great job. Now you can scrape any type of data you like from a normal website.\n",
      "What to do Now???\n",
      "Try reading documentation of beautifulsoup4, requests, lxml or any other url parser you liked this stuff.\n",
      "Before that, there is one more thing.\n",
      "You can also fetch image file names from those source url's.\n",
      "I prefer to do that by regex and believe me it's weird to see those funny looking patterns at first so please don't judge .\n",
      "Hope, you liked it.\n",
      "Embrace it's power.\n",
      "GIF\n",
      "\n",
      "and make some thing beautiful.\n",
      "Have a beautiful day. ‚ú®\n",
      "Resources:\n",
      "Cover image\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with NodeJS\n",
      "URL: https://dev.to/jakedeng/web-scraping-with-nodejs-4f2c\n",
      "Answer: Recently I have came across a need of monitoring stock status of a product on a website(Trying to help my wife to buy Jellycat toys). They don't have an in-stock notification on their official website so I was trying to build a simple product stock notification app using nodejs.\n",
      "As a lot of websites don't have a public API to work with, after my research, I found that web scraping is my best option.\n",
      "There are quite some web scraping libraries out there for nodejs such as Jsdom , Cheerio and Pupperteer etc. (web scraing tools in NodeJs). In short, there are 2 types of web scraping tools:\n",
      "1. DOM Parser\n",
      "2. Headless Browser\n",
      "If you want to know more about different web scraping tools, please refer to the link above. But essentially, Headless Browser enables more possibilities interacting with dynamic web content which is a better fit for my purpose and Pupperteer is a Node library which provides a high-level API to control headless Chrome over the DevTools Protocol.\n",
      " \n",
      "The following knowledge will better help you to better understand the following content.\n",
      "1. Background in Javascript and HTML\n",
      "2. Understanding of DOM\n",
      "3. Understanding of NodeJS\n",
      " \n",
      "The idea of this application is simple: make a concurrent request to the destination URL and parse the response information for the data that contains stock status and stock level of a particular product. If the product is in stock, it will send me an email to notify me.\n",
      " \n",
      "Please ensure you have Node and npm installed on your machine. I would recommand to use any LTS Node version greater than 10.\n",
      "We will be using the following packages:\n",
      "There will be a certain level of project setup for better code reuse and readability. I referenced my project struture from this post (How to Scrape a Website Using Nodejs and Puppeteer) but it was optional.\n",
      " \n",
      "Create a node project and install all required dependencies using the following commands:\n",
      " \n",
      "I would like to start by discussing the overall design of the application before going into the details.\n",
      "index.js is the main entry of the application. await needs to be wrapped inside async function so normally I would like to have a mainEntry async function that contains all the logics and run that async function.\n",
      "mainEntry function creates a headless browser and launches a page to the desired URL. When the page is loaded, scrapper will be applied to scrape for useful information and return back to the main application. Once mainEntry receives the response data, the node mailer will be used to send an email regarding the stock info. The mainEntry function is then put into setInterval function to be executed every 5 mins in this case.\n",
      " \n",
      "Logger is the simplest module in the application, essentially we want all logs to have a timestamp on it so that we can verify that mainEntry is been executed at the set frequency. It is just a thin wrapper around console.log to include timestamp at the beginning.\n",
      " \n",
      "Mailer module is just another thin wrapper around nodemailer. By passing in the service type and authentication info, a mailer instance will be created and it is ready to be used to send out emails.\n",
      "For gmail account, if you want to log in like this, you may need to enable log in from less secure app in the gmail setting.\n",
      " \n",
      "By calling puppeteer.launch(), a browser will be created. If we set headless: false in the config, an actual browser instance UI will show up and we will be able to see all the interactions took place.\n",
      " \n",
      "After a browser instance has been created from the previous step, the browser instance will be passed into the page controller to handle page transition and scraping. In this case, all it does is to create the actual page scraper to handle the scraping logic and await the response data.\n",
      " \n",
      "Page Scraper module is the core module of the application to handle all the scraping logic of the page.\n",
      "To understand what the scraper is trying to do, we first need to understand the structure of the website page that we are trying to scrape on. Different websites would most likely to have different page structures.\n",
      "Normally, I would use 2 methods to determine how I would scrape a website:\n",
      "In this case, the div with class name 'pt0-5' contains all the product info within the tag so this is a good starting point. However this is a dynamic page and span tag that contains the actual stock status of the product and the span content may change based on the product variant selected. So if we are to scrape that particular span tag, we also need to simulate mouse click for potentially all the variants.\n",
      "\n",
      "On a second thought, since the page is dynamic, the different variants infomation is either obtained by making an AJAX request when clicked or already obtained when the page is first loaded and get updated on the mouse click event handler.\n",
      "To verify that, let us take a look at the raw HTML page before render. If we create a GET request to the URL, we will get the raw HTML page. If we search around some of the keywords we are looking for, it is easy to find there is a variable called variants that contains all the variants information including stock level and stock status.\n",
      " \n",
      "If we want to verify that, we can go back to the browser developer tool and in the console type in 'variants', we should be able to see the same content being displayed.\n",
      "Bingo! So that could be our strategy to scrape this website. Note that scraping strategy is very dependent on the website you want to scrape, so doing some research is necessary.\n",
      "\n",
      "Hopefully if we look at the code below, it should more or less make more sense to us.\n",
      "First, we await the desired page to be loaded. Since we found out the div with class p0-5 contains the information we need, we await until this tag gets loaded. This step may not be necessary since we are directly working with JS variables but I just keep it just to be safe.\n",
      "Then we return a new promise, inside the promise, we register a console event handler. That means whenever the console of that page prints out something in the headless browser, the event will be fired and call the function that get passed in. The reason we do this is because we want to capture the content of the variants variable by printing it out in the console to fire the console event.\n",
      "Inside the page.evaluate function, note that the function passed in is not interpreted by your application but interpreted by the headless browser. So that means inside the headless browser, we would like to stringify the variants varibles to strings and console.log the strings. This would cause the console event that we just created to fire.\n",
      "The reason we would want to wrap those 2 into a promise is to aviod passing in callback funtion from one level up which would potentailly produce callback hell if the application has more levels. So in the level above in pageController, all it needs to do is to await the response data to be returned.\n",
      "The response data gets returned all the way back to index.js and an email regarding the stock info will be sent out the destination email address.\n",
      "There are a lot of improvements can be made to this project. For example, the final sending example bit can be warpped into a function and the setInterval logic can be done different, because we don't need to close down the browser each time, all we need is to reload the page or recreate the page. Feel free to change it.\n",
      "I believe web scrpaing is a valuable skillset to have and it has very versatile usage as far as I am concerned.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping: How to Ensure Data Quality\n",
      "URL: https://dev.to/dataox/web-scraping-how-to-ensure-data-quality-37bl\n",
      "Answer: Web Scraping: How to Ensure Data Quality\n",
      "In our modern world, when big data is one of the biggest advantages for any business development, the high-quality and accuracy of the data is very critical, especially when you scraping information at a large scale. Poor quality of data will lead you to deficient data analysis, which is a pointless waste of your resources. That‚Äôs why it is important to know how to ensure data quality and what methods can be used to get the most accurate data scraping.\n",
      "Manual Quality Assurance Approach\n",
      "Every web scraping project based on a web crawler setup. So the stability and the code quality of the crawler may affect data quality. When the crawler is programming, it is necessary to make sure that it is proper for extraction, and there are no issues with the code. Sometimes it is necessary to practice two peers review, only after that, the crawler can be deployed.\n",
      "When the crawler starts its job, it is recommended to inspect the initial dataset manually and check data quality before final setup. The manual data review sorts out the possible issue related to the crawler, and its interaction with the website. In case of any issues, the crawler should be adjusted to resolve them before the setup is completed.\n",
      "Automated Quality Assurance Approach\n",
      "An automated quality assurance approach ensures both the correctness and coverage of the extracted content. The following key parameters might be verified automatically:\n",
      "The correct data from the appropriate web source is extracted\n",
      "The scraped content was processed and formatted as requested\n",
      "The names of the fields are matched to the specified field names\n",
      "All data positions have been scraped from all possible sources\n",
      "All the needed fields were scraped\n",
      "Automated Monitoring System\n",
      "From time to time the websites are getting updated, and as a result of some modifications the crawlers can be broken. This may affect data extraction as well as on data quality. That‚Äôs why it is recommended to have an automated system to monitor the crawling jobs and check the extracted data for errors and inconsistencies. So, there are three types of issues that can be fixed through the special monitoring tool; website changes, data validation errors, and data volume discrepancy.\n",
      "Website changes - The monitoring tool frequently checks the scraped websites to make sure that nothing has been changed since the last crawling. In the case of any changes, the relevant notification is sent to system to take the appropriate actions on crawlers modifications.\n",
      "Data validation errors - Each data has its defined value. So the goal of the monitoring tool checks whether all the data are in line with their value types, because otherwise such kind of mismatches might cause the wrong data extraction. In case of any inconsistency, the system again sends notification.\n",
      "Data volume discrepancy - Sometimes there might be discrepancy in data volume, when the extracted data volume does not match to the required quantity. The monitoring tool should have the expected records for the project, and in discrepancy happens, it sends a prompt notification.\n",
      "Upscale servers\n",
      "Overloading servers also may affect on data quality, because the crawling process requires\n",
      "significant resources. To avoid such cases, when crawlers fail because of heavy load on servers, deploy and run them on high-end servers.\n",
      "Cleansing data\n",
      "The crawled data might have useless elements like HTML tags, which should be removed, as well as duplicated records, and the related records merged. The final output should contain only clean data without any unwanted elements.\n",
      "Structuring\n",
      "Before delivering the ready data to the clients, it is necessary to make it suitable for analytic systems and necessary databases. The data should be delivered in JSON, XML, or CSV formats which are convenient for further analysis.\n",
      "Conclusion\n",
      "One of the most important aspects of web scraping is data quality maintenance. DataOx has significant experience in web crawling and knows how to provide valuable and high-standard datasets. Besides a thorough manual and automatic QA process, DataOx covers the complete range of web scraping services.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping using Django and Selenium\n",
      "URL: https://dev.to/mdrhmn/web-scraping-using-django-and-selenium-3ecg\n",
      "Answer: This is a mini side project to tinker around with Django and Selenium by web scraping FSKTM course timetable from MAYA UM as part of my self-learning prior to FYP.\n",
      "Please note that this is my first ever attempt at Selenium done in < 1 day and my solutions may or may not be the best approach. Feel free to let me know for any improvements!\n",
      "GitHub repository link: https://github.com/mdrhmn/dj-selenium\n",
      "\n",
      "If you are cloning this repo, run the following command preferably inside your virtual environment to install all dependencies:\n",
      "Using venv:\n",
      "Else, to create your Django project from scratch (make sure to have Django installed):\n",
      "And then create a virtual environment (highly recommended):\n",
      "To create virtual environment:\n",
      "To activate virtual environment (Linux/Mac OS):\n",
      "Install all dependencies:\n",
      "Next, navigate into the newly created project folder. Then, start a new Django app. We will also run migrations and start up the server:\n",
      "If everything works well, we should see an instance of a Django application running on this address ‚Äî http://localhost:8000\n",
      "\n",
      "Add app inside INSTALLED_APPS (settings.py)\n",
      "Once you‚Äôve created the app, you need to install it in your project. In project_name/settings.py, add the following line of code under INSTALLED_APPS:\n",
      "That line of code means that your project now knows that the app you just created exists.\n",
      "Add templates folder directory in TEMPLATES (project_name/settings.py)\n",
      "Add static and media folder directory in STATIC_ROOT (project_name/settings.py)\n",
      "Add desired URL for the app (project_name/urls.py)\n",
      "Create new urls.py for the app (app_name/urls.py)\n",
      "Create new template (app_name/templates/)\n",
      "Create a directory named templates and subsequently a file named app_name.html inside it:\n",
      "Create view function (FBV/CBV) in app's views.py\n",
      "Each function or class handles the logic that gets processed each time a different URL is visited.\n",
      "Add URL to app's urls.py\n",
      "Your project has a module called urls.py in which you need to include a URL configuration for the app. Inside app_name/urls.py, add the following:\n",
      "Web scraping is a technique for extracting information from the internet automatically using a software that simulates human web surfing.\n",
      "Selenium is a free (open-source) automated testing framework used to validate web applications across different browsers and platforms. It can be used for automating web browsers to do a number of tasks such as web-scraping.\n",
      "To install Selenium:\n",
      "Selenium requires a driver to interface with the chosen browser. Firefox, for example, requires geckodriver, which needs to be installed before the below examples can be run. Note that the webdriver must be located in your PATH, e. g., place it in /usr/bin or /usr/local/bin.\n",
      "Other supported browsers will have their own drivers available. Links to some of the more popular browser drivers are as follows:\n",
      "For this project, I am using Chrome's webdriver called Chromedriver. There are multiple ways to install Chromedriver:\n",
      "Depending on the use case, you can set up Selenium codes inside views.py for direct use or pair with Celery/Django-Crontab (a discussion for another time).\n",
      "The following code snippet is set up inside views.py:\n",
      "I defined a function called scrap() to contain the Selenium initialisation codes. selenium.webdriver.chrome.options allows us to specify the Selenium webdriver settings such as the following:\n",
      "The important option setting to highlight here is headless, which allows you to launch the browser without creating a visual browser window. This way, you can run tests faster and with fewer resources, and most importantly, it will allow you to run tests on systems without a graphical component.\n",
      "The first hurdle that I encountered when scraping MAYA is going through the authentication. I did some research and luckily I found a working solution from StackOverflow that allows for auto-login:\n",
      "First, I declared constants USERNAME and PASSWORD to store the SiswaMail and password environment variables set within the .env file. If you fork/clone this repository, remember to rename .settings.env as .env and fill in environment variables in the file.\n",
      "For environment variables storage, I use python-dotenv package. There are many other Python alternatives for adding .env support to your Django/Flask apps in development and deployments.\n",
      "To install python-dotenv:\n",
      "Then, add the following code to settings.py:\n",
      "At this point, parsed key/value from the .env file is now present as system environment variable and they can be conveniently accessed via os.getenv():\n",
      "Next, I defined a function called autologin() that accepts the webdriver, site URL, username and password for authentication.\n",
      "In order to extract the information that you‚Äôre looking to scrape, you need to locate the element‚Äôs XPath. An XPath is a syntax used for finding any element on a webpage.\n",
      "To locate the element‚Äôs XPath, right click and select Inspect. This opens up the developer tools. Highlight the portion of the site that you want to scrape and right click on the code. Select Copy -> Copy XPath.\n",
      "find_element_by_xpath() function is used to find an element that matches the XPath given. There are many selectors that you can use to find the right element(s) which you can refer in the official documentation.\n",
      "send_keys() types a key sequence in DOM element which in this case, is the Username and Password input fields.\n",
      "\n",
      "One of the important tips of webscraping is to understand the structure of the website. This can be done by using the browser's developer tools as previously mentioned (for Chrome, it's Inspect/Inspect Element).\n",
      "My next goal is to redirect to MAYA's Search Timetable page, which allows me to filter and access the course timetable. This can be done by first clicking the 'Timetable' button which pops up a modal, followed by clicking the 'Search Timetable' button.\n",
      "Normally, most websites would have a simple HTML structure where most elements will contain a unique but straightforward attributes such as id and class which you can manipulate for finding elements. However, this is unfortunately not the case for MAYA.\n",
      "\n",
      "If you can see clearly, much of the elements' attributes such as href for anchor tag are somewhat encrypted/hashed and linked to the user's session. Obviously this is done for security purposes.\n",
      "However, this makes it much, much harder for me to mimic the interaction properly.\n",
      "Ideally, you should use XPath that utilises the element's id and class. Instead, due to the aforementioned case, I had to resort by directly extracting the XPath from the browser itself (based on element hierarchy):\n",
      "While this works fine, it is not the best approach as it is fragile solution and will not work if the developer changes the structure in the future. In other words, it's not futureproof.\n",
      "Here, I utilised the click() method to mimic cursor-clicking inside the browser.\n",
      "This next stage is hands down the hardest part of the whole project and it clearly demonstrates the aforementioned case of obscuring ID's and classes. It involves filling up the 'Search Timetable' form shown below:\n",
      "\n",
      "Upon close inspection, I realised that the HTML structure for this page involves even more encryption or obscuring of IDs and classes. They change every time you refresh the page, so it confirms my suspicion that it is based on the user's session.\n",
      "I spent hours being stuck at this phase and was on the point of giving up. However, that's when I had an eureka moment.\n",
      "When I reinspected and reanalysed the HTML structure once again, I realised that there exists a pattern with the way the IDs and classes are configured. Although slightly differing in format (e.g. some '.'s are replaced with '_'), they all share the same 'encryption' code, so to speak.\n",
      "After a bit of brainstorming, I drafted the following algorithm to test out:\n",
      "For this part, I utilised one of the elements that I find easiest to extract. I copied its XPath and used the get_attribute() function to retrieve the encrypted ID.\n",
      "Then, I declared two variables to store the encrypted ID that was modified in different ways, based on my key observation of the format.\n",
      "My observations:\n",
      "For some unknown reason, the first three select dropdown input fields do not function like a typical select dropdown. Instead of setting the selected option as selected, the selection process is done elsewhere in another section of the code. This is most probably due to the system using some sort of CSS/JS package.\n",
      "For the 'Campus Location' select dropdown input field, since it functions normally unlike the others, I utilised from selenium.webdriver.support.ui import Select module to help click the select dropdown input field and select the desired option using select_by_visible_text() function.\n",
      "With all these things in mind, I successfully managed to fill in and submit the form using the following code:\n",
      "\n",
      "\n",
      "\n",
      "Once we have submitted the 'Search Timetable' form, we finally arrived at the desired page which is the 'Teaching Timetable' page, where all the course schedules (called 'module occurrences') are displayed.\n",
      "This final task involves extracting all the data displayed in the table, which I identified as in DataTables format. For those who don't know, DataTables is a plug-in for the jQuery Javascript library. It is a highly flexible tool, built upon the foundations of progressive enhancement, that adds all of these advanced features to any HTML table.\n",
      "\n",
      "\n",
      "This poses a few challenges:\n",
      "I managed to solve these two challenges as follows:\n",
      "Get last page number\n",
      "Extract whole table\n",
      "Iterate through table pages and rows and extract data\n",
      "And voila! All the course schedules will be extracted and written inside the maya.txt text file!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping with Strapi\n",
      "URL: https://dev.to/strapijs/web-scraping-with-strapi-31kh\n",
      "Answer: If there is one message that I am trying to convey to the Strapi community, it is that it is possible to create any type of application with Strapi. Whether it's a simple blog, a showcase site, a corporate site, an e-commerce site, an API that can, by the way, be used by a mobile application and so on.\n",
      "Due to its strong customization, you can create whatever you want with Strapi. Today, this tutorial will guide you to create an application used to scrape a website. I have been using Strapi in the past for a freelance mission which consisted of scraping several websites in order to collect public information.\n",
      "Strapi was very useful because I built an architecture allowing me to manage a scraper in just a few clicks. However, what follows may not be the best way to create a scraping app with Strapi. There are no official guidelines, it depends on your needs and your imagination.\n",
      "For this tutorial, we are going to scrape the jamstack.org site and more precisely the site generators section. This site lists headless CMSs like Strapi and others, but also site generators and frameworks such as Next.js, Gatsby or Jekyll.\n",
      "We are simply going to create an app that, via a cron, will collect the site generators every day and insert them into a Strapi app.\n",
      "To do this we will use Puppeteer to control a browser to extract the necessary information with Cheerio.\n",
      "Alright, let's get into it!\n",
      "npx create-strapi-app jamstack-scraper --quickstart\n",
      "This application will have a Sqlite database. Feel free to remove the --quickstart option to select your favorite database.\n",
      "Create the first collection type called scraper:\n",
      "Content-Types Builder > + Create new collection type.\n",
      "\n",
      "\n",
      "Great now you should be able to see your scraper by clicking on Scrapers in the side nav:\n",
      "\n",
      "\n",
      "This view is not well organized, let's change that.\n",
      "\n",
      "By creating a Scraper collection type, you'll be able to manage the behavior of your scraper directly in the admin. You can:\n",
      "\n",
      "\n",
      "Perfect! Everything seems ready concerning the content in the admin!\n",
      "Now let's create our scraper.\n",
      "\n",
      "What we have here is:\n",
      "Let's dive into the code!\n",
      "The first thing to do is make your application able to fetch your scraper with a slug.\n",
      "Add the following code in your ./api/scraper/controllers/scraper.js file:\n",
      "Only update the findOne route of your ./api/scraper/config/routes.json file with the following code:\n",
      "Awesome! Now you are able to fetch your scrapers with the associated slug.\n",
      "Now let's create our scraper.\n",
      "This file will contain the logic to scrape the site generators on Jamstack.org. It will be called by the cron but during the development, we will call it from the ./config/functions/bootstrap.js file which is executed every time the server restart. So this way we will be able to try our script each time we save our file.\n",
      "In the end, we will remove it from there and call it in our cron file every minute. The script will check if this is the time to execute it or not thanks to the frequency of your scraper you'll define for it.\n",
      "Add the following code to your ./scripts/starters/jamstack.js file:\n",
      "Update your ./config/functions/bootstrap.js file with the following:\n",
      "Now you should see your scraper (JSON) in your terminal after saving your bootstrap file.\n",
      "\n",
      "Great! Now it's time to scrap!\n",
      "First of all, we are going to create a file containing a useful function for our script.\n",
      "Create a ./scripts/scrapers/utils/utils.js file containing the following:\n",
      "The first function will use the cron-parser package to check if the script can be executed depending on the frequency you set in the admin. We'll also use the chalk package to display messages in color.\n",
      "Add theses packages by running the following command:\n",
      "Add the following code to your ./scripts/scrapers/utils/utils.js file.\n",
      "This function will check if this is the time to execute the scraper or not. Depending on the frequency you set on your scraper, the package cron-parser will parse it and verify if this is the right time to execute your scraper thanks to the next_execution_at field.\n",
      "Important: If you want to change the frequency of your scraper, you'll need to delete your next_execution_at value. It will be reset accordingly to your new frequency.\n",
      "The next function will get all site generators we already inserted in our database in order to not fetch them again during the execution of the script.\n",
      "Add the following function to your ./scripts/scrapers/utils/utils.js file.\n",
      "The next function will simply get the current date for our report and errors log\n",
      "Add the following function to your ./scripts/scrapers/utils/utils.js file.\n",
      "The last function will prepare the report of the last execution.\n",
      "Add the following function to your ./scripts/scrapers/utils/utils.js file.\n",
      "Alright! Your ./scripts/scrapers/utils/utils.js should look like this:\n",
      "Let's update our ./scripts/scrapers/jamstack.js file a little bit now. But let's add puppeteer first ;)\n",
      "Add puppeteer and cheerio to your package.json file by running the following command:\n",
      "Now save your file! You might have this message:\n",
      "\n",
      "Yep, you didn't enable your scraper so it will not go further. So in order to continue, we will need to comment this code because we want to be able to code the scrape function without having to wait or what ;)\n",
      "Comment the following portion of code:\n",
      "You can save your file and now it should be fine!\n",
      "\n",
      "Perfect! Now let's dive into scraping data!\n",
      "Update your ./scripts/scrapers/jamstack.js file with the following:\n",
      "Let me explain what is going with this modification.\n",
      "First of all, we can see that we are importing a function from another file we didn't create yet: ./scripts/scrapers/utils/query.js. These two functions will allow us to create the site generators in our database and updating our scraper (errors and report). We will create this file just after don't worry ;)\n",
      "Then we have the scrape function that simply scrapes the data we want using puppeteer and cheerio and then use the previously explained function to insert this data in our database.\n",
      "Create a ./scripts/scrapers/utils/query.js file containing the following:\n",
      "As you can see we are updating the scraper at the end with this updateScraper function. Then we will uncomment the code that allows our scraper to be executed or not depending on the frequency if it's enabled etc...\n",
      "Let's add it to our ./scripts/scrapers/jamstack.js file:\n",
      "Again, by saving your file you should have this message:\n",
      "\n",
      "Update your ./config/functions/bootstrap.js to it's default value:\n",
      "Save your file!\n",
      "Now what will happen is that, at the next minute, the scraper will fill your next_execution_at value with the corresponding one according to your frequency, here, the next minute after.\n",
      "So the next minute after the next_execution_at timestamp will be compared to the actual one and of course will be inferior or equal so your scraper will be executed. A new next_execution_at timestamp will be set to the next minute accordingly to this frequency.\n",
      "Well, I think it's over for this quick and simple tutorial!\n",
      "\n",
      "\n",
      "\n",
      "I hope that you enjoyed it!\n",
      "If you want to keep learning then feel free to register to the Academy to become a Strapi expert or simply browse our blog to uncover any subjects you'd like.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping with Puppeteer\n",
      "URL: https://dev.to/aarmora/web-scraping-with-puppeteer-3igd\n",
      "Answer: Sample code here\n",
      "Another quick video showing the basics of web scraping with Puppeteer.\n",
      "Using the techniques talked about here at javascriptwebscrapingguy.com, we‚Äôve been able to launch a way to access awesome web data. Learn more at Cobalt Intelligence!\n",
      "The post Web scraping with Puppeteer appeared first on Javascript Web Scraping Guy.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with PHP: Building a Competitor Price Monitoring Tool\n",
      "URL: https://dev.to/andreasa/web-scraping-with-php-building-a-competitor-price-monitoring-tool-1le9\n",
      "Answer: Depending on your business's niche and market, adjusting your services and prices has to come along with taking your competitors into account.\n",
      "In a lot of companies that I have seen, this is a manual task, that is completed once every quarter or at least every year.\n",
      "In this PHP web scraping tutorial, we are going to build a tiny tool, that automizes this process. Of course, the tool will need further advancements, but is always about understanding the concepts, right? :)\n",
      "Let's get started!\n",
      "We will need the following set of tools:\n",
      "Download composer here to download the composer and follow the install instructions.\n",
      "After composer has successfully been installed, install guzzle via composer:\n",
      "Next, let's install our HTML parser:\n",
      "Finally, we add the currency parser to our project:\n",
      "As we want to build a competitor price monitoring tool, let's say that this product URL is our own:\n",
      "https://www.allendalewine.com/products/11262719/diplomatico-reserva-exclusiva\n",
      "As a competitor page, we select the following:\n",
      "https://www.winetoship.com/diplomatico-rum-reserva-exclusiva.html\n",
      "Next, we have to define the CSS-Selectors that contain the price information.\n",
      "\n",
      "For our \"own\" website, the selector is .sale-price.currency. Going through the same process for the competitor, the selector is .less-price .o_price span.\n",
      "Putting the pieces together, we end up with the following script:\n",
      "You can add as many product and competitor entities as you like. The scraper then loops through all products and competitors and fetches the HTML-Markup. Our DOM-Parser then extracts the related elements from the HTML. Finally, the currency detector parses the price string into a comparable and normalized format.\n",
      "I used the following PHP web scraping tutorial to create this scraper.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Ruby\n",
      "URL: https://dev.to/dirk94/web-scraping-with-ruby-370m\n",
      "Answer: Hi guys üëã,\n",
      "I spend the last week writing a super in-depth Ruby web scraping guide.\n",
      "I show step-by-step how to build an indeed.com scraper using Ruby.\n",
      "You can find the article here\n",
      "Any feedback would be appreciated! :)\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Node and Puppeteer\n",
      "URL: https://dev.to/harshhhdev/guide-to-web-scraping-with-node-1kpe\n",
      "Answer: In this post, we'll be making our first small little web scraping app.\n",
      "Before we get started, let's just talk a little bit about web scraping and what it is. The most simlified definition for web scraping is \"extracting data from websites\", which is somewhat implied by the name. It has always been very much of a grey area. Going into a legal discussion is beyond the scope of this article, though I will recommend this blog post going into deeper detail about that.\n",
      "So, to introduce today's project, we'll be building a simple GitHub follower counter, to count how many followers a user has on GitHub through the terminal.\n",
      "First, let's make a directory for this repository.\n",
      "Open it in your code editor. If you're using Visual Studio Code you can simply do code .\n",
      "Initialise yarn(or npm)\n",
      "Install puppeteer\n",
      "First off, let's import puppeteer into our project.\n",
      "Now, let's get the terminal arguments from the user. To do this, we can use process.argv\n",
      "Next, let's create our getFollowers function.\n",
      "Inside it, let's launch the browser, open a new tab, and navigate to the URL.\n",
      "Inside it, let's evaluate the page.\n",
      "Now, let's get the follower count. If we navigate over to GitHub, and right click < view page source (or ctrl+u). We can see the code of the website.\n",
      "Inside of here, we can see that the span element, with the class of text-bold text-gray-dark has the current follower count.\n",
      "\n",
      "Back to our code, let's do\n",
      "Now, let's output the results. There is an error however. If a user does not exist, then it will show us as \"optional\" on the follower count. To prevent this, we can do...\n",
      "Next, back to our function, let's output this.\n",
      "Make sure to close the browser window as well.\n",
      "At the bottom, don't forget to call this function.\n",
      "And you should be good to go! Make sure to run node index.js followed by a username to test it out!\n",
      "_Note: a far better way to do this is to use the GitHub API. This was primarily a way on how to select and get certain elements, if you're looking to make an actual project with this, then the GitHub API is the way to go!\n",
      "Thanks for reading, Happy Thanksgiving.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Nuxtjs using Puppeteer\n",
      "URL: https://dev.to/kaperskyguru/web-scraping-with-nuxtjs-using-puppeteer-4253\n",
      "Answer: \n",
      "Web Scraping with Nuxtjs using Puppeteer is intended to demonstrate how to set up and configure puppeteer to work properly with Nuxtjs and how to use it to Scrape a job listing website and display them on your website.\n",
      "Since Puppeteer is a Server-side Node package, it becomes very difficult to set it up to work properly with a Client-Side library like Vue.js and there are no many tutorials online on how to set this up properly.\n",
      "This article is intended to demonstrate how I solve the problem in my client‚Äôs project using Nuxt.js and Puppeteer.\n",
      "Web scrapping can sound very strange at first but it‚Äôs really a very simple term to understand.\n",
      "The term web scraping is a technique that describes the extraction of data from websites and saved in any desired format for further processing.\n",
      "Web scrapping automates the manual process of extracting information from websites and storing this information electronically for further processing.\n",
      "Puppeteer is a Node library that is used to scrape web pages, automate form submission, etc.\n",
      "It is Google‚Äôs official Node library for controlling Google Chrome instance from Node.js, it can also be configured to run in headless mode and make it run in the background.\n",
      "Puppeteer can be used for several use cases but I will only list a few below.\n",
      "Before we start developing our web scrapper, we need to install and set up Nuxtjs, following the simple step in the official documentation can help speed up the process.\n",
      "Type in the following commands to set up the project and accept the default set up rules.\n",
      "After installation, let‚Äôs start by creating the different components, stores, and pages that will be needed in this project.\n",
      "Create a component called jobs to display a list of all the jobs scraped.\n",
      "Next, create a new job store in the store's folder to manage our jobs state.\n",
      "Lastly, let's create a jobs page inside the pages folder for our navigation if needed anyway.\n",
      "Of course, this is limited as your project can be complex and contains plenty of components, pages, and stores to manage different states.\n",
      "Next is to install all the necessary dependencies needed to scrape pages with nuxtjs and puppeteer.\n",
      "Run the command to install the puppeteer library and other support libraries.\n",
      "This is the difficult part, I had different issues configuring my puppeteer to work with nuxtjs because nuxtjs is both client and server-side framework.\n",
      "It becomes difficult to know where to place puppeteer or how to call it from the server-side since puppeteer is a server node library and only works on the server-side of nuxtjs.\n",
      "I will just go ahead to explain how I get it working on my project.\n",
      "First, let‚Äôs create a new script.js file in the root directory and paste in the following codes.\n",
      "Looking at the script you might understand what it does, if not, I will explain.\n",
      "It goes into node_modules/puppeteer/package.json file and delete a particular line.\n",
      "Before deleting that line, it checks if the package.json has the broswer object, if not create a new one, else move on to delete the ws property of the browser object and save the file finally.\n",
      "The script is going to run each time we run npm install.\n",
      "The ws is puppeteer's web socket that was set to a web socket that does not exist in our project.\n",
      "By deleting that line each time will run npm install puppeteer will default to using the web socket that is in our node_modules folder.\n",
      "Now, let‚Äôs add the script to our package.json file where it will be executed as a postinstall script.\n",
      "Open your package.json file and add the following code.\n",
      "You also need to add the following code into your package.json file.\n",
      "That just sets fs, path, os and tls to false because these are only needed on the server-side of things.\n",
      "Now that the hard part is off, let‚Äôs configure Webpack to deal with puppeteer correctly.\n",
      "Open your nuxt.config.js file and add the following line inside the build object.\n",
      "This configuration only requires puppeteer and adds it to externals array only when Nuxtjs is at the client-side and set fs to empty too.\n",
      "If you did everything right, your puppeteer should be ready to use with Nuxtjs to scrape pages, if you‚Äôre stuck you can grab the repository here.\n",
      "Now we can move to the easy part.\n",
      "Create a file called JobScrapper.js and paste in the following code.\n",
      "In my project, I was given a list of websites I should scrape to avoid violating any scrapping rules (Just saying üôÇ\n",
      "The init function initializes puppeteer with several configurations, creates a new page with browser.newPage(), visit our URL with await page.goto(.........), and wait for the page to load successfully with await page.waitForSelector(.....)\n",
      "This method does all the job.\n",
      "Firstly, it selects all the Jobs listed, convert it to javascript array and loop through each of them while retrieving the data needed.\n",
      "The method simply returns the job array from the resolver method and closes the browser.\n",
      "Next, we are going to set up our Vuex store to retrieve the jobs each time we dispatch the getJobs action and store them to state.\n",
      "Open the job file and add the following codes.\n",
      "Open pages/jobs.vue file and add the following codes.\n",
      "This is just one way you could dispatch the actions in each of the pages you want, but it has to be within the asyncData() hook because it is called from the server-side.\n",
      "Another way or my best way could be to dispatch the action inside nuxtServerInit action which will dispatch the action on every new page load.\n",
      "Let me show you how to do that.\n",
      "Create an index.js file inside the store folder and add the following codes.\n",
      "This will scrape the jobs and save it to state, you can then use ...mapState or ...mapGetters to retrieve the job and display it in your component.\n",
      "In my project, I use the nuxtServerInit approach and ...mapState in any of the components, I want to display the job.\n",
      "That‚Äôs all.\n",
      "Except you want to see my Job component, then clone the repository here, everything can be found there.\n",
      "P:S\n",
      "\n",
      "This method of web scraping with Nuxtjs using puppeteer has many workarounds and maybe a little difficult to understand for beginners, though it works properly because I have used it in my projects.\n",
      "I have a better approach on how to handle web scraping with Nuxtjs using Node/express and puppeteer, I will be writing about it too.\n",
      "Consider joining our newsletter to never miss a thing when it drops.\n",
      "Congratulations for making it this far, by now you should have a deep understanding of web scrapping using puppeteer in Nuxt.js.\n",
      "You should also have built and completed the JobScrapper Project.\n",
      "Keep coding üôÇ\n",
      "Originally published at https://masteringbackend.com on November 17, 2020.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping in the console\n",
      "URL: https://dev.to/markoskon/web-scraping-in-the-console-36f3\n",
      "Answer: This post contains a collection of code snippets you can paste into the browser‚Äôs console and get useful data back. It‚Äôs like scraping a web page, but instead of doing it inside a Node script, you do it in the browser‚Äôs console. Because you shouldn‚Äôt run code that you don‚Äôt understand in your console, I explain how most of the snippets work. As a result, you may learn some JavaScript tricks you haven‚Äôt seen before, learn how to accomplish basic tasks without using a framework/library (the code is not optimized for performance though), and extract some useful data from the pages you visit. Before we see the code, I want to explain some syntax you‚Äôll encounter throughout the snippets.\n",
      "In the browser, the $ function is an alias for document.querySelector. For example, if you run $('body') in the console, you‚Äôll get back the body element which is the same as running document.querySelector('body'). The $$ is the equivalent for document.querySelectorAll. I use both of those aliases to save some space in the code snippets.\n",
      "The $$ returns a NodeList that lacks many of the useful array methods such as map and filter. For that reason, I use the Array.from(notAnArray) method to transform it into an array.\n",
      "I also use the Set constructor new Set(array) that returns a Set object that contains all the unique values inside an array. Here, we face the same issue with the $$ alias, so I transform the Set to an array with the Array.from method.\n",
      "Find which elements cause overflows by adding a red outline to all the body elements. This can also help you understand how inline elements work.\n",
      "It‚Äôs not the most exciting one, but you may discover new elements to use or at least see what elements other sites use.\n",
      "You get all the elements inside the body element with $$(\"body *\") call that returns a NodeList, you make it an array, you transform that element array to an array with the tag names (strings), you keep only the unique tags with the Set constructor, and finally, you transform the Set to an array. The sort() call at the end sorts the array in alphabetical order.\n",
      "See what characters a website uses. Use this if you want to adjust the font files by creating subsets to make sure they cover those characters.\n",
      "You start by getting all the elements inside the body and filter out those elements that don‚Äôt contain human-readable text, for example, scripts, styles, iframes, etc.\n",
      "You transform the elements (with map) to an array that contains their inner texts, you keep only the values that are truthy with filter(Boolean)‚Äîthis will remove empty strings (\"\"), undefined, and more‚Äîand you transform it into an array of characters with the join(\"\") and split(\"\") array methods. join(\"\") joins the array into a string value without a separator, and split(\"\") splits that string into an array that contains individual characters.\n",
      "Keep only the unique characters (with Set), transform the characters to code points and sort them, and finally, print the result. The result is an array with the characters, along with their Unicode hex numbers.\n",
      "Or, more specifically, get the different values of the font-family CSS attributes of all body elements. You accomplish that with the help of the getComputedStyle method:\n",
      "In case you‚Äôre wondering, you cannot do the same with el.style because the CSS properties of that object are not populated from the stylesheets. el.style is used for setting properties with JavaScript. See all the differences between getComputedStyle and el.style.\n",
      "Firefox developer tools do a much better job at this task with the Fonts tab, that‚Äôs inside the Inspector tab.\n",
      "Get the font families and the different font sizes that are used in:\n",
      "The Set constructor finds the unique values from arrays that contain primitive values. In this case, we want both the family and the size of an element, so the first thought might be to create an object for each element and store that information there. If you do that, Set will not work because it will compare the objects by reference, not by the inner value to find if they are unique. For this reason, you serialize the object to a string with JSON.stringify and later transform it back to an object with JSON.parse (see the highlighted lines).\n",
      "I have a post that tackles a similar issue which is the difference between deep/shallow copy and the assignment operator. It contains useful references to other resources to learn more about the subject.\n",
      "A use case for me is that some Greek characters are identical to Latin, for example, Œ§/T or O/Œü. This code helps me find these small mistakes I make while writing text.\n",
      "Regular expressions are not the most readable code in the world, but they have some cool features. One of them is the Unicode property escapes (e.g: /\\p{property}/u). You can use them in regular expressions to find characters from a specific script, emojis, punctuation marks, and more‚Äîsee the link for more properties. Don‚Äôt forget to add the Unicode flag (u) when you use Unicode property escapes. I‚Äôm also using $& in the string replace method to refer to the matched string.\n",
      "I mark the characters with the mark Unicode character (combining low line u+0332). I initially thought to parse the HTML of the elements (not the innerText) with regular expressions and wrap the characters with <mark> elements, but, as it turns out, parsing HTML with regular expressions is probably a bad idea.\n",
      "See how a Google Font looks on a page. To do that, you create a style element, you add it inside the head element, and you use it.\n",
      "In the previous example, I use a ‚Äútrick‚Äù with Object.assign to create an element that looks like the React API for creating elements, for example:\n",
      "Vanilla JavaScript is cool, but sometimes you wish you had access to an external library to help you do the job. In the following example, you can add lodash with an external script from unpkg:\n",
      "The code above shows how to add an external script to a page with JavaScript. To add a different library from NPM, replace the :package from the following snippet with the package name, enter the URL in the browser, and unpkg will redirect you to the correct file. If not, you‚Äôll have to browse the directory to find the file yourself, and in this case, don‚Äôt forget to remove the /browse/ from the URL:\n",
      "With the following snippet, you get all the different box shadows, but you can use it for any other CSS property you‚Äôre interested in.\n",
      "Or create an object with the box shadows, colors, borders, and background images.\n",
      "Show all the elements that are usually hidden. More specifically, this snippet shows all the head elements, and from the body, it shows the scripts, styles, and noscript elements.\n",
      "In the previous snippet, you create a pre and a nested code element, and you style them. You also add the code in plain text inside the code element (see below how). The plan is to use them like this:\n",
      "You use the insertAdjacentElement method to insert the <pre> right after the original element. The alternative is to get the parent node of the element with el.parentNode and append a child with the appendChild method. You set the code element‚Äôs inner text to its inner HTML which is the original (cloned) element‚Äôs HTML. If you don‚Äôt use the cloneNode() method to create copies of the original elements, the scripts and the styles will be rendered useless, and the page will not work as before.\n",
      "Infinite scrollers, default styles on <pre> elements, and fixed elements can mess-up the result.\n",
      "I‚Äôm not sure why you‚Äôd want to use this; maybe to read the licenses and the inner thoughts of your fellow developers? My favorite comments are the DO NOT CHANGE, all in caps of course.\n",
      "The regular expression for single-line comments gives many false positives though. For example, it may return base64 encoded data that match.\n",
      "You could use either of textContent and innerText to get the text of styles and scripts, so it doesn‚Äôt matter which one you choose in this case. See all the differences between textContent and innerText.\n",
      "This is a visualization of the regular expression for single-line comments created by the Regulex app. The (?<name>thing to name) creates a named capturing group that is easier to access via match.groups.name instead of match[1].\n",
      "\n",
      "And this is a visualization of the regular expression for multiline comments:\n",
      "\n",
      "The dot special character . in regular expressions matches all characters except newlines. To match all characters including newlines, you can use [\\s\\S].\n",
      "Print the URLs and the text of the links in a table. Screen readers offer something similar with the rotor function:\n",
      "If you don‚Äôt like the table from console.table, you can use a regular console.log. In Chrome‚Äôs console, you can resize the columns of the table and sort the data by column.\n",
      "Display only the image elements inside the page‚Äôs body‚Äîit removes the body content.\n",
      "I have a more elaborate solution because the images many times have some default styling, such as absolute positions or weird widths. If you want a more consistent result, it‚Äôs better to create new image elements. This also creates image elements for the background images:\n",
      "I use append because I want to add multiple elements at once‚Äîsee all the differences between append and appendChild. You can render the images in a mosaic layout (kind of) if you add some flex styles to the body element:\n",
      "This is a visualization of the regular expression for background image URLs:\n",
      "\n",
      "This trick uses the invert CSS filter to create a dark mode if the site doesn‚Äôt offer the option. I first saw it in a post on how to create a theme switcher by Heydon Pickering.\n",
      "Ids on a page should be unique and it may be hard to notice if you don‚Äôt test with a tool like axe or Lighthouse. Duplicate ids are not always the result of your code; external libraries can cause them too. This snippet will help you identify this issue.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping with Puppeteer\n",
      "URL: https://dev.to/dberri/web-scraping-with-puppeteer-3dck\n",
      "Answer: Today, I decided to bring sort of a different topic to the blog: web scraping.\n",
      "\n",
      "Sometimes, I find myself wanting to analyse or visualize some data that is not available through an API or is not really structured, so I turn to web scraping. This is a data extraction method where the user or an automated software copies specific data from a website. I used to use Python to do that, but recently I came across puppeteer and that is the Node library we‚Äôre going to use today.\n",
      "According to the docs, puppeteer is ‚Äúa Node library which provides a high-level API to control Chrome or Chromium over the DevTools Protocol.‚Äù This means we can do things like: crawl simple web pages or even SPAs (single page applications), automate form submissions, UI testing, generate screenshots and PDFs, and more.\n",
      "Assuming you have Node.js installed in your computer, you can run this snippet as a .js file and quickly see how it works.\n",
      "If you want to see what the browser is doing instead of just waiting for the results, you can change the headless mode:\n",
      "This will open up a browser window as soon as the script starts running and you will see each step as they happen.\n",
      "Now, the cool thing about using puppeteer for web scraping is that you can use ‚Äúvanilla‚Äù JavaScript syntax to find elements and data on the page. You will have access to the document interface inside the evaluate method. Let‚Äôs try it on the Google News page:\n",
      "As I write this post, these are the headlines it returned:\n",
      "Google News has an infinite scroll behaviour. This means that when you first load the page, only a few articles are shown and the others are loaded as you scroll down the page. You can mimic this behaviour to get more data by using a combination of window.scrollBy and setInterval inside the evaluate method. Something like this (beware this can cause an infinite loop, make sure to create an exit strategy that meets your requirements):\n",
      "And that‚Äôs it. Hopefully you can see how this technique can be useful to automate boring tasks and maybe create APIs where there isn't one. On a final note, be respectful of the website you‚Äôre scraping. You should follow the rules stated in the /robots.txt for each website, make sure to agree with the terms of service and check if scraping is actually legal wherever you are doing it. And try to not DDOS them when running this in loops :)\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Python\n",
      "URL: https://dev.to/aveeksaha/web-scraping-with-python-57dm\n",
      "Answer: This is a guide on how to scrape a webpage using Urllib and BeautifulSoup.\n",
      "Code for this tutorial can be found on Github\n",
      "Web scraping is a useful tool for extracting data from websites, especially those that don‚Äôt provide an API. In this post, I‚Äôll show you how you can use web scraping to generate a dataset from a webpage.\n",
      "For this example we will be using a website called trendogate which is a website that displays trending twitter hashtags on a given day based on region. Our goal will be to retrieve hashtags that are currently trending in the US.\n",
      "To do this we will primarily be using two libraries:\n",
      "Urllib is a Python module that can be used for opening URLs. It defines functions and classes to help in URL actions. With Python you can also access and retrieve data from the internet like XML, HTML, JSON, etc.\n",
      "Urllib is going to help us retrieve the web page we want to scrape.\n",
      "To install Urllib-\n",
      "Beautiful Soup is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping\n",
      "Once we have the page we need from Urllib, we‚Äôre going to use Beautiful Soup to create a parse tree and extract the information we need from the page.\n",
      "To install Beautiful Soup\n",
      "To scrape a webpage you need to be familiar with the structure of the HTML tags in that page. So right click on the page you want to scrape and select Inspect.\n",
      "The trendogate webpage looks like this:\n",
      "\n",
      "We are interested in the trending today section. If we inspect it we can see the HTML structure.\n",
      "\n",
      "Today‚Äôs trending hashtags are in an unordered list of class list-group that looks something like this:\n",
      "First we import the libraries we‚Äôll need\n",
      "Then we‚Äôll use urllib to get the webpage. Read more about urllib\n",
      "Now we‚Äôll create a BeautifulSoup object from the HTML page we just retrieved. Read more about BeautifulSoup\n",
      "Now we have scraped all of today‚Äôs trending hashtags from the website.\n",
      "This is just one of the ways that you can scrape information from websites. Another method is to use something like Selenium that allows you to emulate a browser instance and automate tasks with code.\n",
      "Here‚Äôs an article you can refer to, if you want to get started with Selenium: https://www.toptal.com/python/web-scraping-with-python\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping Python Tutorial ‚Äì How to Scrape Data From A Website\n",
      "URL: https://dev.to/mehulmpt/web-scraping-python-tutorial-how-to-scrape-data-from-a-website-kic\n",
      "Answer: Python is a beautiful language to code in. It has a great package ecosystem, there's much less noise than you'll find in other languages, and it is super easy to use.\n",
      "Python is used for a number of things, from data analysis to server programming. And one exciting use-case of Python is Web Scraping.\n",
      "In this article, we will cover how to use Python for web scraping. We'll also work through a complete hands-on classroom guide as we proceed.\n",
      "Note: We will be scraping a webpage that I host, so we can safely learn scraping on it. Many companies do not allow scraping on their websites, so this is a good way to learn. Just make sure to check before you scrape.\n",
      "\n",
      "If you want to code along, you can use this free codedamn classroom that consists of multiple labs to help you learn web scraping. This will be a practical hands-on learning exercise on codedamn, similar to how you learn on freeCodeCamp.\n",
      "In this classroom, you'll be using this page to test web scraping: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\n",
      "This classroom consists of 7 labs, and you'll solve a lab in each part of this blog post. We will be using Python 3.8 + BeautifulSoup 4 for web scraping.\n",
      "This is the link to this lab.\n",
      "The requests module allows you to send HTTP requests using Python.\n",
      "The HTTP request returns a Response Object with all the response data (content, encoding, status, and so on). One example of getting the HTML of a page:\n",
      "Let's move on to part 2 now where you'll build more on top of your existing code.\n",
      "This is the link to this lab.\n",
      "In this whole classroom, you‚Äôll be using a library called BeautifulSoup in Python to do web scraping. Some features that make BeautifulSoup a powerful solution are:\n",
      "It provides a lot of simple methods and Pythonic idioms for navigating, searching, and modifying a DOM tree. It doesn't take much code to write an application\n",
      "Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.\n",
      "Basically, BeautifulSoup can parse anything on the web you give it.\n",
      "Here‚Äôs a simple example of BeautifulSoup:\n",
      "Looking at the example above, you can see once we feed the page.content inside BeautifulSoup, you can start working with the parsed DOM tree in a very pythonic way. The solution for the lab would be:\n",
      "This was also a simple lab where we had to change the URL and print the page title. This code would pass the lab.\n",
      "This is the link to this lab.\n",
      "In the last lab, you saw how you can extract the title from the page. It is equally easy to extract out certain sections too.\n",
      "You also saw that you have to call .text on these to get the string, but you can print them without calling .text too, and it will give you the full markup. Try to run the example below:\n",
      "Let's take a look at how you can extract out body and head sections from your pages.\n",
      "When you try to print the page_body or page_head you'll see that those are printed as strings. But in reality, when you print(type page_body) you'll see it is not a string but it works fine.\n",
      "The solution of this example would be simple, based on the code above:\n",
      "This is the link to this lab.\n",
      "Now that you have explored some parts of BeautifulSoup, let's look at how you can select DOM elements with BeautifulSoup methods.\n",
      "Once you have the soup variable (like previous labs), you can work with .select on it which is a CSS selector inside BeautifulSoup. That is, you can reach down the DOM tree just like how you will select elements with CSS. Let's look at an example:\n",
      ".select returns a Python list of all the elements. This is why you selected only the first element here with the [0] index.\n",
      "Let's keep going.\n",
      "This is the link to this lab.\n",
      "Let's go ahead and extract the top items scraped from the URL: https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/\n",
      "If you open this page in a new tab, you‚Äôll see some top items. In this lab, your task is to scrape out their names and store them in a list called top_items. You will also extract out the reviews for these items as well.\n",
      "To pass this challenge, take care of the following things:\n",
      "Use .select to extract the review count label for those product titles. (Hint: one selector for reviews could be div.ratings) Note: this is a complete label (i.e. 2 reviews) and not just a number.\n",
      "Create a new dictionary in the format:\n",
      "There are quite a few tasks to be done in this challenge. Let's take a look at the solution first and understand what is happening:\n",
      "Note that this is only one of the solutions. You can attempt this in a different way too. In this solution:\n",
      "This is the link to this lab.\n",
      "So far you have seen how you can extract the text, or rather innerText of elements. Let's now see how you can extract attributes by extracting links from the page.\n",
      "Here‚Äôs an example of how to extract out all the image information from the page:\n",
      "In this lab, your task is to extract the href attribute of links with their text as well. Make sure of the following things:\n",
      "You have to create a list called all_links\n",
      "In this list, store all link dict information. It should be in the following format:\n",
      "Make sure your text is stripped of any whitespace\n",
      "Make sure you check if your .text is None before you call .strip() on it.\n",
      "Store all these dicts in the all_links\n",
      "Print this list at the end\n",
      "You are extracting the attribute values just like you extract values from a dict, using the get function. Let's take a look at the solution for this lab:\n",
      "Here, you extract the href attribute just like you did in the image case. The only thing you're doing is also checking if it is None. We want to set it to an empty string, otherwise, we want to strip the whitespace.\n",
      "Part 7: Generating CSV from data\n",
      "This is the link to this lab.\n",
      "Finally, let's understand how you can generate CSV from a set of data. You will create a CSV with the following headings:\n",
      "You have to extract data from the website and generate this CSV for the three products.\n",
      "The for block is the most interesting here. You extract all the elements and attributes from what you've learned so far in all the labs.\n",
      "When you run this code, you end up with a nice CSV file. And that's about all the basics of web scraping with BeautifulSoup!\n",
      "I hope this interactive classroom from codedamn helped you understand the basics of web scraping with Python.\n",
      "If you liked this classroom and this blog, tell me about it on my twitter and Instagram. Would love to hear feedback!\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping com .NET Core 3.1\n",
      "URL: https://dev.to/lucaspsilveira/web-scrapping-com-net-core-3-1-228m\n",
      "Answer: Recentemente tive a necessidade de buscar algumas informa√ß√µes em sites que n√£o possu√≠am uma API para recuperar os dados necess√°rios. Ent√£o, decidi que iria utilizar algumas t√©cnicas de web scraping para buscar as informa√ß√µes que precisava. J√° havia constru√≠do alguns web scrapers com Python e PHP. Por√©m, decidi me desafiar e criar um com .NET Core 3.1 e C#. Para isso utilizei um pacote, dispon√≠vel no gerenciador de pacotes Nuget, chamado AngleSharp. Este pacote pode ser encontrado aqui.\n",
      "Para exemplificar aqui, vamos acessar o site pensador.com, e recuperar as cita√ß√µes que ficam dispon√≠veis para visualiza√ß√£o. Nosso objetivo ent√£o √© recuperar as cita√ß√µes de um determinado autor ou tema e armazenarmos em nosso dispositivo.\n",
      "Minha necessidade era construir esse banco de cita√ß√µes uma vez, sem a necessidade de ficar atualizando ou ter algum acesso remoto. Por este motivo decidi criar uma aplica√ß√£o de linha de comando, para isso utilizei o pr√≥prio template disponibilizado pelo Visual Studio.\n",
      "Ap√≥s nomear o projeto e realizar sua cria√ß√£o, √© necess√°rio instalar o pacote AngleSharp como uma depend√™ncia do projeto. Para isso √© necess√°rio clicar com o bot√£o direito do mouse em 'Dependencies', e selecionar a op√ß√£o 'Manage NuGet Packages'. Voc√™ deve visualizar uma tela semelhante com a imagem abaixo:\n",
      "Ap√≥s clicar em instalar, nosso projeto est√° pronto para come√ßarmos a realizar chamadas ao site.\n",
      "Poder√≠amos realizar as chamadas diretamente no m√©todo Main da aplica√ß√£o, mas eu prefiro separar em classes diferentes algumas opera√ß√µes. Sendo assim, decidi criar uma pasta Services e criar uma classe ScrapperService.cs dentro. Essa ser√° a classe respons√°vel por acessar o site que desejamos e trazer nossos dados j√° modelados.\n",
      "Nessa classe ScrapperService.cs vamos criar uma propriedade do tipo IBrowsingContext, que √© uma interface do pacote AngleSharp, que ser√° respons√°vel por fazer a requisi√ß√£o √†s p√°ginas web que desejamos acessar. A classe junto com o seu construtor ficar√° assim:\n",
      "Nessa etapa j√° podemos escrever o m√©todo que ir√° realizar a leitura dos nossos dados de interesse, mas antes √© importante abrirmos a p√°gina em nosso navegador para vermos como o DOM est√° organizado, para ent√£o organizarmos nossa estrat√©gia de obten√ß√£o dos dados. Ent√£o, ao abrir a p√°gina inicial de algum tema de interesse podemos perceber a organiza√ß√£o da p√°gina. Vou usar como exemplo cita√ß√µes com o tema de filosofia, a p√°gina pode ser visualizada abaixo:\n",
      "\n",
      "Ao analisarmos dessa forma, conseguimos visualizar onde est√£o os elementos de interesse para nossa aplica√ß√£o, que s√£o: as cita√ß√µes e n√∫mero total de cita√ß√µes. Necessitamos verificar o n√∫mero total de cita√ß√µes para podermos calcular o n√∫mero de p√°ginas estimado que cada tema ou autor ter√°. Assim, conseguiremos extrair todas as cita√ß√µes de cada um.\n",
      "Ap√≥s a an√°lise visual √© necess√°rio verificar a organiza√ß√£o do documento HTML, como j√° sabemos nossos elementos de interesse, podemos clicar com o bot√£o direito e inspecionar o elemento para irmos direto a ele no c√≥digo. Verificando o elemento, podemos ent√£o clicar com o bot√£o direito novamente e selecionar a op√ß√£o de copiar seletor. Esse caminho √© que utilizaremos para encontrar esse elemento em nossa aplica√ß√£o com o AngleSharp, ent√£o guarde ele em algum lugar. Abaixo uma imagem exemplificando o que foi realizado.\n",
      "\n",
      "Realizando a mesma etapa para o bloco das cita√ß√µes, temos dois seletores copiados:\n",
      "Com esses dados em m√£os podemos criar o m√©todo para buscar as cita√ß√µes na classe ScrapperService.cs, o m√©todo fica assim:\n",
      "Explicando o c√≥digo acima:\n",
      "Montamos ent√£o a url de acesso concatenando os textos conforme o padr√£o que o site utiliza, que √© algum desses dois:\n",
      "Na linha com o c√≥digo var document = await context.OpenAsync(url); √© que realizamos a leitura do documento, com aux√≠lio do pacote AngleSharp. Esse m√©todo nos retorna um objeto com a p√°gina mapeada onde √© poss√≠vel navegar atrav√©s dos seus n√≥s.\n",
      "Como j√° hav√≠amos anotado nossos seletores, podemos utilizar a fun√ß√£o QuerySelectorAll para retornar os n√≥s de interesse nesse documento. Para isso ent√£o passamos o nosso seletor de todas as cita√ß√µes como par√¢metro, adicionado da classe que todas cita√ß√µes compartilham em comum, como pode ser observado abaixo:\n",
      "Ap√≥s termos todos os n√≥s das cita√ß√µes, podemos iterar sobre eles e buscar as informa√ß√µes internas de cada n√≥. Para isso utilizamos novamente o QuerySelector, por√©m agora passando os seletores de cada informa√ß√£o que queremos. Que nesse caso s√£o: autor, frase e ID.\n",
      "Como voc√™ pode perceber, esse m√©todo retorna um objeto Quote, e esse objeto deve ser criado por n√≥s, para isso criei uma pasta Models, e ent√£o criei a classe modelo Quote.cs, que pode ser visualizada abaixo:\n",
      "Se n√≥s chamarmos o m√©todo constru√≠do acima, nossa aplica√ß√£o j√° vai retornar todas cita√ß√µes de cada p√°gina que passarmos como par√¢metro. Mas e se quisermos executar nossa aplica√ß√£o para buscarmos todas as p√°ginas de um tema ou autor? Para isso que salvamos o seletor daquela parte da p√°gina que nos informava o n√∫mero total de posts. Por√©m, se analisarmos outra p√°gina, de algum autor espec√≠fico, por exemplo, veremos que algumas informa√ß√µes est√£o diferentes:\n",
      "\n",
      "O site pensador, quando tem um artista famoso, muda o texto de total de cita√ß√µes, e tamb√©m muda a estrutura do HTML, para isso ent√£o precisamos mapear esse seletor tamb√©m. Logo, temos dois seletores distintos:\n",
      "Precisamos construir ent√£o um m√©todo que retorna o n√∫mero total de p√°ginas que cada autor ou tema ter√°. Uma forma simples de fazer isso √© dividir o n√∫mero total de cita√ß√µes pela quantidade que √© exibida na primeira p√°gina, para termos um valor aproximado. Poder√≠amos tamb√©m sempre analisarmos os elementos de pagina√ß√£o no final da p√°gina, at√© n√£o existir mais nenhum. Mas nessa implementa√ß√£o vou somente dividir o n√∫mero total de cita√ß√µes pelas exibidas na tela. Mas sintam-se livres para alterar o c√≥digo no github e implementar essa funcionalidade. Segue abaixo como o m√©todo ficou:\n",
      "Esse m√©todo ficou um pouco extenso, e poderia ser otimizado, mas deixaremos para fazer isso em uma segunda vers√£o. Para extrair as informa√ß√µes de total de cita√ß√µes e pagina√ß√£o utilizei Regex. Explicando de uma forma geral, o m√©todo acessa a primeira p√°gina do autor ou tema e verifica se est√° no padr√£o de totais de cita√ß√£o do tema ou autor, ent√£o utiliza as t√©cnicas de QuerySelector para realizar a captura dos n√≥s. Ap√≥s isso, o valor total de cita√ß√µes √© dividido pela quantidade de cita√ß√µes por p√°gina e armazenado em um inteiro para arredondarmos para baixo o total de p√°ginas.\n",
      "Agora que j√° possu√≠mos um m√©todo para estimar o n√∫mero total de p√°ginas e outro m√©todo para buscar as cita√ß√µes dessa p√°gina, s√≥ necessitamos realizar um la√ßo de repeti√ß√£o alterando a p√°gina at√© o total de p√°gina daquele autor ou tema. Para isso, podemos adicionar em nosso m√©todo Main da classe Program.cs . A classe fica assim:\n",
      "Ao executarmos o c√≥digo podemos ver como est√° funcionando nossa aplica√ß√£o:\n",
      "\n",
      "Ainda, se desejarmos buscar dados de mais de um autor ou tema poder√≠amos criar uma Lista de autores e iterar sobre ela chamando os nossos mesmos m√©todos, dessa maneira:\n",
      "Pronto! Temos uma aplica√ß√£o que realiza um Web Scraping no site pensador.com e retorna todas as cita√ß√µes dispon√≠veis de um determinado autor ou tema. Claro que normalmente n√£o √© s√≥ isso que desejamos. Muitas vezes necessitamos armazenar essas informa√ß√µes. Por esse motivo eu j√° realizei tamb√©m a integra√ß√£o dessa aplica√ß√£o com um banco de dados, onde essas informa√ß√µes ficam armazenadas. Mas como o post j√° ficou extenso, vou tratar desta etapa em um outro post aqui. Utilizaremos SQLite para armazenar e recuperar esses dados. O c√≥digo completo desse post, com a vers√£o j√° utilizando o banco de dados pode ser encontrado no meu Github. Abaixo temos um GIF da aplica√ß√£o funcionando:\n",
      "GIF\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping Don'ts\n",
      "URL: https://dev.to/crawlertera/web-scraping-don-ts-40gc\n",
      "Answer: Web scraping projects are known to fail a lot. So we thought it is more appropriate to have a list of DON'Ts rather than a list of DO's. So here goes.\n",
      "If the crawler depends on any external data or event to happen in a particular way, Don't assume it will happen like that. It won't MORE often than it does. For example: when fetching a URL, it could break because of timeouts, redirects, CAPTCHA challenges, IP blocks, etc.\n",
      "DON'T build custom code. Use a framework like scrapy.\n",
      "DON'T be too aggressive on a website. Check the response time of the website first. In fact, at crawltohell.com, our crawlers adjust their concurrency depending on the response time of the domain, so we don't burden their servers too much.\n",
      "DON'T write linear code. Don't write code that crawls, scrapes data, processes it, and stores it all in one linear process. If one breaks, so will the others, and also, you would be able to measure and optimize the performance of each process. Batch them instead.\n",
      "DON'T depends on your IP's. They will eventually get blocked. Always build in the ability to proxy your requests through a Rotating Proxy Service like Proxies API.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Python - Reproducing Requests\n",
      "URL: https://dev.to/albertulysses/web-scraping-with-python-reproducing-requests-2395\n",
      "Answer: Have you noticed a gap in knowledge between learning how to scrape and practicing with your first project?\n",
      "An introduction tutorial teaches us how to scrape using Beautiful Soup or Scrapy on simple HTML data. After completion, we think to ourselves, \"Finally, all the data in the World Wide Web is mine.\"\n",
      "Then BAM, the first page you want to scrape, is a dynamic webpage.\n",
      "From there, you will probably start reading about \"how you need Selenium,\" but first, you need to learn Docker. The problem then gets messier because you're using a Windows machine, so is the community version even supported? The result is an avalanche of more unanswered questions, and we abandon the project altogether.\n",
      "I'm here to tell you that there is a technique that lies between using HTML scraping tools and learning Selenium that you can try before jumping ship. Best of all, it's probably a technology you already know - Requests.\n",
      "By the end of this tutorial, you'll learn how to mimic a website's request to get more HTML pages or JSON data.\n",
      "Disclaimer, in some cases, the website will still need to use Selenium or other technologies for web scraping.\n",
      "The method is a three-step procedure.\n",
      "Postman\n",
      "A basic understanding of Web Scraping\n",
      "A basic understanding of Python's Requests library\n",
      "My browser is Firefox, but I'm sure Chrome has a similar feature. Under the Web developer option, select the network option.\n",
      "\n",
      "Generate dynamic data and examine the requests. For my example webpage, you can invoke this by selecting an option.\n",
      "\n",
      "Now, you might have noticed that many requests are happening. So how do you find the one you need? The trick here is to look at the type and the response itself. In my example, there are many js (Javascript) types and two HTML types. Since we are trying to avoid dealing with Javascript, it is a natural move for us first to inspect the two HTML options.\n",
      "Luckily, the first HTML type is the request we need. We can tell that it's the right choice because we can see the data we want in the response.\n",
      "\n",
      "Next, we will use Postman to mimic the request. We do this to isolate, inspect, and confirm that this is the data we need.\n",
      "With Firefox, we right-click to copy and select the URL option.Then paste into Postman with the right request type.\n",
      "\n",
      "Right-click to copy and select data form.\n",
      "\n",
      "In some cases, you might need to add something to the header, but Postman autocompletes a lot. So in this example, it's not necessary.\n",
      "Then run it.\n",
      "\n",
      "The data looks good. It's a dedicated HTML page that we can scrape.\n",
      "The last step is writing the python code. Because this isn't a Request or Python tutorial, I'm not going to get detailed in this step, but I will tell you what I did.\n",
      "I messed with the requests a bit and realized that I could get the same data without the county code, so my data form is numerical. Which means using a range is just fine.\n",
      "Here is my code for creating a request that gets all the HTML for all those pages.\n",
      "With this, we can use a combination of an HTML parser and requests to retrieve dynamic data.\n",
      "Here is a website that can turn cURL requests into Python code.\n",
      "https://curl.trillworks.com/\n",
      "Cool, right? You can use this if your request is simple, or you just are feeling lazy. Be warned, it doesn't work all the time, and some headers may need to be adjusted still.\n",
      "We went over how to use a web browser, Postman, and the Request library to continue your web scraping journey. An intermediate technique for those who are learning Web Scraping. Good Luck!\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping in Node.js - Puppeteer, Cheerio, Fetch\n",
      "URL: https://dev.to/leighhalliday/web-scraping-in-node-js-puppeteer-cheerio-fetch-28dh\n",
      "Answer: \n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping Lunch and Learn\n",
      "URL: https://dev.to/bdmorin/web-scraping-lunch-and-learn-184j\n",
      "Answer: Forward: Where I work, we have these things called Lunch and Learn where people in the company talk about something to everyone else. Sometimes it's a client overview, other times it's about scuba diving, sometimes it's just to introduce new people. I gave a talk about web scraping and how it could help your day to day business, personal, or other work. This is the presentation I gave, it might not make a ton of sense stand alone, but I wanted to share. Link to original presentation.\n",
      "The purpose of web scraping, or data mining, is to transform web data into structured data you can work with in different formats. There is a huge industry around data mining, web automation, and web scraping. I‚Äôve put together an example method for how to do a simple scrape if you run into data you need to structure yourself.\n",
      "These are the tools I used during the presentation\n",
      "https://data-miner.io/ (chrome extension)\n",
      "https://data-miner.io/quick-guides/menu\n",
      "https://sheets.google.com importxml() and importhtml() functions\n",
      "In order to scrape websites using dataminer, you would save yourself a lot of time by watching the tutorial videos. It shows you how to go about using the tool effectively in basic situations. As you need more advanced features, you may need to learn CSS selectors, jquery selectors, or xpath selectors. Additionally for more complex scraping tasks you may need a commercial account from data-miner.io, or move to an open source framework like scrapy/portia.\n",
      "One of the biggest challenges in web scraping is dealing with Javascript. Sites that use Angular, Vue, React will not render well to a typical request based web scraper. Data Miner already handles this well for basic use cases, as it‚Äôs using your browsers post-rendered HTML to scrape. A scraping library needs to deal with the javascript first either via a headless browser, or other option. There are commercial options for proxy loading HTML that will pre-render sites before your parser analyzes the HTML, and there are projects like Puppeteer that enable you to have a headless chrome browser running natively (not the same as phantomjs/capserjs).\n",
      "The scrapy ecosystem has a great project called Splash that is a dockerized headless web browser that‚Äôs api driven. Your spider simply makes requests to the api and it handles rendering. Splash has been very useful in many cases where an automated scraper needs to deal with a login page where javascript is required.\n",
      "Scrapy and Portia are an opensource endeavor with commercial services if you need. Scrapy is a python framework (based in Django) for deploy web scrapers, spiders and crawlers. Scrapy is easy to use and start out with, and scales to very advanced if the need arises. Portia is a opensource application that creates a visual method for developing scraping recipes. Portia can be self-hosted or hosted as a service. I run a local Portia instance via docker, and while it‚Äôs neat, it‚Äôs problematic and crashes frequently. This would be frustrating for new users.\n",
      "https://github.com/scrapinghub/portia\n",
      "https://github.com/scrapy/scrapy\n",
      "https://github.com/scrapinghub/learn.scrapinghub.com\n",
      "https://github.com/scrapy-plugins/scrapy-splash\n",
      "https://django-dynamic-scraper.readthedocs.io/en/latest/\n",
      "If you would like to write a scraping bot from scratch, and no framework overhead, BeautifulSoup4, and Requests is a great way to go. You can develop multistage scrapers in about 20 lines of code, but you need to understand the libraries and methods ahead of time. BS4 has excellent documentation, as does Requests and nearly any beginner pythonista could get started with them. There is also a very handy python library that pulls core content automatically from pages (like newspaper article content) called Newpaper3k, if you are looking to pull a large corpus of content for tasks like AI or ML, this is a great module to help you focus on NOT scraping, but what to do with the content you are scraping.\n",
      "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
      "http://docs.python-requests.org/en/master/\n",
      "https://newspaper.readthedocs.io/en/latest/\n",
      "I haven‚Äôt done much research scraping with Node, but I‚Äôve read a lot of articles about it. The biggest barrier to entry for me was that any requests library that didn‚Äôt use promises was too easily hung up. I tried some but I really enjoy developing in Python/Jupyter. Here are some resources for starting webscraping in Node.\n",
      "Framework: https://expressjs.com/\n",
      "Request library: https://github.com/mikeal/request or https://github.com/axios/axios\n",
      "HTML Parser: https://github.com/MatthewMueller/cheerio or https://github.com/jsdom/jsdom\n",
      "Sometimes, you just want to grab data directly from the command line. There are 2 tools that will make this remarkably simple: pup and jq.\n",
      "Example:\n",
      "This example uses the vigilante.pw website we looked at earlier. On command line you use curl as the requestor, pup extracts just the table‚Äôs rows and transforms them into json, then jq processes the json into a workable dataset you could use in any other web application. jq could further remove commas from numbers, and normalize other text if needed.\n",
      "Put this in a google sheet cell.\n",
      "You can import nearly any xpath you like into google sheets, enabling you to create custom dashboards of web content.\n",
      "Photo by Maik Jonietz\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web Scraping with Ruby Part Two\n",
      "URL: https://dev.to/amsmo/web-scraping-with-ruby-part-two-6d7\n",
      "Answer: Last time we got some of the simpler categories, you can see part one at https://dev.to/amsmo/web-scrape-episode-information-walk-through-part-one-1423\n",
      "Here we are going more in depth. We will start off simple though to get back into it. There is a plot on the top that involves going a link deeper but when scrolling down you see the full synopsis without having to go to another page. When scraping many episodes this will save us time so let's use that one.\n",
      "Highlighting the area we see that it is a class called canwrap. Let's try that. Again to look at a class you use \".\" before the name of it.\n",
      "\n",
      "narf.get_text(parsed_episode.css('.canwrap'))\n",
      "Hmm the results it give are the following\n",
      "A bit too much, let's try to be more specific, let's take one step further in. You can do that by using the \">\" css selector and then putting in the child you are looking at. Let's try .canwap > p Well, that still gives us the Written by. One more selector .canwap > p > span That worked to give us an array of 1 item. We can then try something different to expand our practice of nokogiri. When you know exactly where you want to be you can use .at instead of .css. So here parsed_episode.at('.canwrap > p > span')).text. It returns the string with an indentation \" The Pinky and The Brain who. Let's throw a .strip at the end and you'll get what we want. We won't use out get_text function because that is used for arrays, where we are unsure if we want more info. So writing out that function will look something like this:\n",
      "Let's start tackling some harder things now. Let's look at the cast. All the data is in a table, let's see what we get from that.\n",
      "\n",
      "Looking at the css td > a gives the result \" Maurice LaMarche\\nThe BrainQueen Roach's Aid Rob Paulsen\\nPinky Tress MacNeille\\n Frank Welker\\n\"... We lose non-recurring characters since they don't have a link. So that's not helpful, also how can we figure out where an actor ends and a character begins? Let's try something more specific, maybe have something with actors and then characters...\n",
      "Let's have fun with this one. Let's guide ourselves off the photos, and then also gather the characters to the right as separate arrays and join them letter. So the photos have the title of the actors name. Let's navigate to the photos with parsed_episode.css('.primary_photo > a > img')\n",
      "That will give us an array so let's look at the first item giving us the following information:\n",
      "There's more but we've got the actors name in it! great. With nokogiri since this is an element instance, the top two items (name, attributes) can be obtained with a method call.\n",
      "So if we go into .attributes we step into a hash that looks like.\n",
      "No we just need to get into title, and look there... another Attribute!!! so let's get the whole function now parsed_episode.css('.primary_photo > a > img')[0].attributes[\"title\"].value gives us just Maurice, let's write a function that gets all of them.\n",
      "That gives an array of actors, great. That's what we want so we can orderly join them with characters.\n",
      "So let's try td.character, that seems reasonable. The first in the array results in \"\\n The Brain / \\n Queen Roach's Aid \\n \\n \\n (voice)\\n \\n \\n \" Wow, that's a mess.. Let's try stripping...\n",
      "\"The Brain / \\n Queen Roach's Aid \\n \\n \\n (voice)\" Still not great, and we want to keep this as a single part of the array for now so we can join both the characters with the actors. Right... Ruby has regex built in. Let's change all those line breaks and spaces into a single space. You can do that with .gsub(/[[:space:]]+/, \" \" ).strip. So putting everything together you get:\n",
      "This results in\n",
      "It will all line up!\n",
      "Let's line them in a hash, that seems the best. Actor as key and characters as an array value. Remember the characters are separated with a slash so here I think it's a simple function to draw up.\n",
      "Remember, we are doing this in a class, so actors and characters are called functions. So we are just iterating through the array that those functions return. Let's see what we get.\n",
      "PERFECT!!!! Alright, next time we will tackle the crew. Writers and Directors, because that gets a little more complicated. And we will also write to json.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Webscraping: Beginner's Thoughts\n",
      "URL: https://dev.to/jengfad/webscraping-beginner-s-thoughts-15cn\n",
      "Answer: Decided to learn webscraping this month. The first thing I did was to watch courses in Pluralsight:\n",
      "Scraping Dynamic Web Pages with Python and Selenium\n",
      "Scraping Your First Web Page with Python\n",
      "Exploring Web Scraping with Python\n",
      "Webscraping can be done by using Python libraries like BeautifulSoup and Requests. This assumes that you have all urls predetermined and will just scrape the page source.\n",
      "But if you will scrape a dynamic page (ex: a div is rendered only if a specific button was clicked) then you will need a library like Selenium to emulate user interactions.\n",
      "When I was confident with the basics, I took a step further and learned the Scrapy framework. This requires a steeper learning curve than native Python libraries because you have to know the flow of how objects are passed in the framework. The main advantage is you won't have to write boilerplate codes (writing data to files, handling url requests, data modelling) redundantly because those are already integrated with its pipeline.\n",
      "Here are some Scrapy Pluralsight courses that helped me:\n",
      "Crawling the Web with Python and Scrapy\n",
      "Extracting Structured Data from the Web Using Scrapy\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping with Ruby Walk-through Part One\n",
      "URL: https://dev.to/amsmo/web-scrape-episode-information-walk-through-part-one-1423\n",
      "Answer: Last post was explaining why web scraping is a good project to educate yourself about many aspects of web development. Let's go over somethings before we start.\n",
      "If you work with Rails you probably have noticed there is this thing called Nokogiri... it takes fooooooooorever to load. Fun fact nokogiri is the Japanese word for woodsaw... Not sure why they named it that (maybe because saws scrape things...) but didn't research it so I could settle on the joke that it's named after the snoring I did while napping during installation. Nokogiri is an html parser, that you can use CSS selectors to find the information.\n",
      "Now we have to get that parser some information. that's where the aptly named HTTParty comes in handy.\n",
      "\n",
      "It will handle your get requests to retrieve the raw HTML. Another tool that isn't necessary but something I love about Ruby is Pry. Just drop a binding.pry in and manipulate variables, it's awesome. Especially if you are doing OOP instead of procedural. It really is useful to put it in a function to make sure your scope is right.\n",
      "Those are the tools we will start with. In part 2 or 3 we will add selenium which is a webdriver.\n",
      "It's best to break down a project into parts... Or Shakespeare was trying to communicate that there are probably only so many bad jokes people will sit through in one play... or post.\n",
      "So here is what you should have at the top of your ruby scraper file:\n",
      "In your Gemfile you should have:\n",
      "Don't forget to bundle install!\n",
      "To start we are not trying to make this interactive, just a quick scraper to make sure we can build on it.\n",
      "We're going to start with just an episode and try to get the information we want from that. If you didn't come from the first post, it talked about how when finding the information you are going to want to think about what datatypes you will use to store, we will go through this based upon what we want to scrape.\n",
      "What's useful to know about an episode... title, characters/actors, airdate, plot, rating, writers and directors. Oh boy that's a lot.\n",
      "Let's think about it though.\n",
      "To be able to further expand later, why don't we make the episode the key to a hash? And then have a hash inside to further question the scraped information.\n",
      "Wait a minute... there can be many writers, directors and then characters with actors. The best way to tackle multiples of this would be an array you can further go into. To save you the suspense, I chose to do an array of hashes for actors (I started with the Simpsons and sometimes someone voices 8 different characters). The rest of the hash would look like this:\n",
      "For this walk through let's pick a new target... How about Pinky and the Brain, because who doesn't want to take over the world?\n",
      "From previous wandering around IMDB I noticed the separate actors who played multiple characters with a \" / \" so finding an episode with someone who has several parts I landed on:\n",
      "https://www.imdb.com/title/tt0954793/?ref_=ttep_ep10\n",
      "Brain of the Future.\n",
      "Ok, we have where we want to start let's head back to Ruby to see if we can get the page.\n",
      "Again because ruby works best as an OOP let's make it a class, that will take in an argument of the URL when initializing that class so we can do it again with a different episode.\n",
      "\n",
      "Let's just go all in on the initialize and retrieve the page on initialization. Then let's create a function that parses the page and sets that to an instance variable as well. I am doing this because I want to make everything modular.\n",
      "Why are we having air_date, title and other information as separate functions? If IMDB changes, only a small thing breaks and you will have an easier time picking over the data for what you want.\n",
      "So here's what we have right now:\n",
      "With what we have no if you call:\n",
      "You will get a return of the parsed page, it should look something like:\n",
      "That goes on for a while, that's the information nokogiri is reading from the get request.\n",
      "I set my variable to narf = the new instance, this was done since I'm in pry and have some digging to do. Now nokogiri has a lot of neat functions but I am partial to #css, it really teaches you how to fine tune your selectors when you have to do styling. Other finders are #at_css (shows the first result of that css selector), and #x_path(let's you put in the xml path instead of css.\n",
      "Anyway, we have the page and a slight idea of how to select how are we going to proceed?\n",
      "Let's start with title. Hop back onto that website (I recommend using google chrome because their dev tools make the most sense to me, but most browsers will have an inspect function)\n",
      "If you inspect element and click on the title you will see\n",
      "Think... the class is empty how am I going to pick this out, wait... Search engine optimization works best you want to only have a singular H1 tag to pick out for a website (not a rule but a guideline most websites stick to). Let's try it out.\n",
      "Whoops... we need to get at that css, which we put in the parsed page. narf is only our created instance. Let's make it easy and set the css level to another variable so you don't have to chain so many methods\n",
      "Try it again\n",
      "SUCCESS!!! But what is that exactly, it looks like an array of information. This will be useful in other tasks but we see the title right there! Again, the important thing here is it's an array (nokogiri has some neat functions to read it too) but since it's an array, we only got one result here but later on we may get a few. We should map the nokogiri method of #text on to it and see what happens.\n",
      "Closer still, now ruby comes with the handy .strip or .chomp method that gets rid of that pesky whitespace, so throwing a .strip at the end of that ele.text will give you\n",
      "Perfect we have our title.\n",
      "I'd like that to be a function in itself, again you never know. So let's add the code:\n",
      "Awesome we have a title function. In case you were wondering why the [0] is there, it's because we are getting an array of things, here we only expect one result but you will see what to do with more items in part 2 of this walkthrough.\n",
      "Awesome. We're on a roll let's look at airdate\n",
      "Hmm, that's not very descriptive but check another episode page to see what's unique... I don't see any other tags with title=\"See more release dates\". Let's see if we can css select that. Ok time to throw some regex in there since we're lazy. css_level.css('[title^=\"See more\"]']) What does that even mean?!?!?!\n",
      "\n",
      "So many websites have their own selectors in there other than class and id a way around that is the bracket notation. Be careful about your ' and \" though, you don't want to escape too soon.\n",
      "\n",
      "And what's with that ^? That means it starts with, did you really want to type that whole thing out? Another regex character they use for selectors is $ which means ends with.\n",
      "OOO success... now lets get the text.. oh wait we already have done that once before... Let's just make that a function shall we?\n",
      "So let's fix our title and add airdate\n",
      "Now that you hopefully understand this a little we will quickly do one more and leave the more complicated data for next time.\n",
      "Let's try rating next. <span itemprop=\"ratingValue\">7.9</span> Oh great, ratingValue seems specific and we just learned the trick for the custom classifications so you guessed it\n",
      "The rest involves a little more sorting (even logic with .next_element, real fancy stuff) and you have sat through enough of my bad jokes already. I'll save that for the next blog.\n",
      "\n",
      "If you want to check out the full code for this walk-through you can head on over to:\n",
      "https://github.com/AmSmo/webscraper_narf\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping in Ruby, great practice for aspiring Web Developers\n",
      "URL: https://dev.to/amsmo/web-scraping-in-ruby-great-practice-for-aspiring-web-developers-2k5a\n",
      "Answer: When looking into learning to code I was trying to figure out if there was a project I could do that would touch on multiple areas of coding. Really the ideal was to find something to give a basic knowledge of a broad variety of topics and see what really interested me and what I wanted research further. I came across web scraping and thought it was the perfect start. \n",
      "Here are a few things you have to research:\n",
      "The real things I wanted to scrape were recipes, because I am often baking and my hands are covered in dough... the pages refresh or an ad pops up, wash my hands and then it happens all over again. It really is a scene befitting Scooby and Shaggy.\n",
      "\n",
      "You can scrape any website for information. A walkthrough of scraping IMDB for episode information will be up next.\n",
      "Let's talk about why I think Ruby is great for this. It has several tools at its disposal. Nokogiri, httparty and what I find separates Ruby from my scraping experience in Python is the Pry library.\n",
      "Ruby forces you to make decisions that really are foundational in Object Oriented Programming. In Python I found myself able to use procedural programming as a crutch when starting out. Ruby because a variable has to be passed into a function otherwise it is out of scope, it really makes you want to turn everything into a class and use instances. So you really start to think about how to not work on a variable but work with a variable. Why is that important? You start to think in modularity and reusability of code. Future you will thank you for that.\n",
      "So how does pry play into that? Well, make a run file that declares the instance with a binding.pry at the end enables you to explore that variable in webs craping, your basic information is the entirety of the HTML document. So when you are in pry, it's loaded into the memory and through trial and error you can find what you want. If you strike gold, you add that into your class as a method and continue on.\n",
      "Ruby also has a regex built in so if you need to parse out specific parts or search for keywords that is functionality built in. It can be paired with trying to find css-selectors to add some extra layer of adaptability to your program.\n",
      "Through learning to web scrape there are several other libraries I came across that are also useful. Selenium being one of them. This library allows you to run a browser from the command line. In my project I ended up using it to just double check that I was indeed navigating the scraper to the correct location. It also has the ability to send information to the browser and automate clicks and fill in forms for you. Real handy for automation.\n",
      "So webscraping also makes you think about data types. How do you want to store this information you are finding. Many APIs return responses in JSON format, so that is a natural choice in order to learn. Making decisions of how to layer a hash with arrays or strings or integers really will get you into thinking about how to manipulate this data and engineer ways to retrieve the information. Ruby has a great JSON gem that will let you write to a JSON and later read it if you want to go back into your scraped information in a persisting form (possible future post of manipulating a JSON into a database and how to think that through, still one step at a time though). More on this for when we go through an example.\n",
      "Enough about why it's great for a programming language, Ruby especially being my favorite but what do you learn about HTML and CSS while scraping?\n",
      "Well first thing I learned (again I started with trying to do recipes), was that http://www.allrecipes.com has a unicorn hidden on most pages. Sadly, they have not responded to my email asking why. Maybe the lesson is never question a unicorn?\n",
      "Anyway. Scraping makes you look at a lot of examples of HTML and occasionally see why some decisions were made. It is an extremely forgiving language to program in but someone who is looking at the HTML you write probably will be less so. Ugly HTML is awful to sort through, so it makes you aware of why some sites are easier to automate information gathering. So what you may ask? This is actually a really for screen readers and making the web accessible for all. The more explicit the HTML the more easily it is navigated. This also plays into css with adding titles and everything.\n",
      "Web scraping will have the effect of you having to narrow down css selectors if you are trying to get a specific answer. Not everything will accessible through an html tag. There be many href tags, but you will then see the choices of class and id use. What if there's a class of container that has information after a div tag? You learn how to use .class > div or .class + div and begin to differentiate through those selectors.\n",
      "CSS specificity is important when designing your own website in order to make it stand out among others that provide similar information.\n",
      "All in all you can learn the basics of all the tools you will be needing in the future in one fun project. What we will do tomorrow night:\n",
      "\n",
      "Or whenever the post is, we will get some episode information for Pinky and the Brain.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Web scraping project ideas in python\n",
      "URL: https://dev.to/codefatale/web-scraping-project-ideas-in-python-4ad3\n",
      "Answer: What to collect data from the internet? Web scraping is a great way to gather data. Here are a few projects:\n",
      "Writing a song can be tricky. To make your own hit song, scrape song lyrics from music websites and add data into a text file.\n",
      "Data from business and consumer trends shows data from various industries. Understanding what sells and what doesn't can help with marketing.\n",
      "Thinking about becoming a great author one day, use a text generator to create a poem or short story.\n",
      "Big cinema lover? Web scrape movie reviews and analysis them to build the perfect movie marathon.\n",
      "Use web scraping on job boards to collect data on occupations and job requirements for hiring.\n",
      "Know any more ideas or suggestions?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_dev_articles(tag, limit=1000):\n",
    "    articles = []\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(f'https://dev.to/search?q={tag}')\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.crayons-story__title a')))\n",
    "        \n",
    "        story_cards = driver.find_elements(By.CSS_SELECTOR, '.crayons-story__title a')[:limit]\n",
    "        \n",
    "        for card in story_cards:\n",
    "            title = card.text.strip()\n",
    "            link = card.get_attribute('href')\n",
    "            articles.append({'query': title, 'url': link})\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def get_article_content(url):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    content = \"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, '.crayons-article__body')))\n",
    "        \n",
    "        paragraphs = driver.find_elements(By.CSS_SELECTOR, '.crayons-article__body p')\n",
    "        content = \"\\n\".join([para.text for para in paragraphs])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return content\n",
    "tag = 'web scraping'\n",
    "articles = get_dev_articles(tag)\n",
    "output_data = []\n",
    "for article in articles:\n",
    "    print(f\"Query: {article['query']}\\nURL: {article['url']}\")\n",
    "    content = get_article_content(article['url'])\n",
    "    output_data.append({'query': article['query'], 'answer': content})\n",
    "    print(f\"Answer: {content}\\n{'-'*80}\")\n",
    "df = pd.DataFrame(output_data)\n",
    "df.to_csv('dev_articles.csv', index=False)\n",
    "with open('dev_articles.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
