{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting asyncpraw\n",
      "  Downloading asyncpraw-7.7.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting aiofiles<1 (from asyncpraw)\n",
      "  Downloading aiofiles-0.8.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting aiohttp<4 (from asyncpraw)\n",
      "  Downloading aiohttp-3.9.5-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting aiosqlite<=0.17.0 (from asyncpraw)\n",
      "  Downloading aiosqlite-0.17.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting asyncprawcore<3,>=2.1 (from asyncpraw)\n",
      "  Downloading asyncprawcore-2.4.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting update-checker>=0.18 (from asyncpraw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4->asyncpraw)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4->asyncpraw)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4->asyncpraw)\n",
      "  Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4->asyncpraw)\n",
      "  Downloading multidict-6.0.5-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4->asyncpraw)\n",
      "  Downloading yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.7.2 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiosqlite<=0.17.0->asyncpraw) (4.11.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from update-checker>=0.18->asyncpraw) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\natha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2024.2.2)\n",
      "Downloading asyncpraw-7.7.1-py3-none-any.whl (196 kB)\n",
      "   ---------------------------------------- 0.0/196.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 196.7/196.7 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
      "Downloading aiohttp-3.9.5-cp312-cp312-win_amd64.whl (369 kB)\n",
      "   ---------------------------------------- 0.0/369.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 369.0/369.0 kB 11.2 MB/s eta 0:00:00\n",
      "Downloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
      "Downloading asyncprawcore-2.4.0-py3-none-any.whl (19 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp312-cp312-win_amd64.whl (27 kB)\n",
      "Downloading yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: multidict, frozenlist, attrs, aiosqlite, aiofiles, yarl, update-checker, aiosignal, aiohttp, asyncprawcore, asyncpraw\n",
      "Successfully installed aiofiles-0.8.0 aiohttp-3.9.5 aiosignal-1.3.1 aiosqlite-0.17.0 asyncpraw-7.7.1 asyncprawcore-2.4.0 attrs-23.2.0 frozenlist-1.4.1 multidict-6.0.5 update-checker-0.18.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install asyncpraw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Monthly Self-Promotion Thread - June 2024\n",
      "Text: Hello and howdy, digital miners of /r/webscraping!\n",
      "\n",
      "The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n",
      "\n",
      "* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n",
      "* Maybe you've got a ground-breaking product in need of some intrepid testers?\n",
      "* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n",
      "* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n",
      "\n",
      "Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n",
      "\n",
      "Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d5khhl/monthly_selfpromotion_thread_june_2024/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I retrieve bearer token without using chromedriver?\n",
      "Text: I scrape a bunch of websites for cinema showtimes. When possible, I want to use their API.\n",
      "\n",
      "One of the cinemas is this one (chain):\n",
      "\n",
      "[https://www.cinesa.es/cines/diagonal-mar/](https://www.cinesa.es/cines/diagonal-mar/)\n",
      "\n",
      "Their API requires a bearer token, but you are issued a bearer token when you use the website. So it is not a secret bearer token - but it expires fairly quickly.\n",
      "\n",
      "They also use cloudflare and seem to block ips from outside Spain, which forces me to use a proxy.\n",
      "\n",
      "The way I have handled it so far is to run a chromedriver with a proxy server and capture all network requests in the browser until I find a bearer token. Then I can use that to access the api with a simple REST client.\n",
      "\n",
      "This works, but I would really like to NOT have to use chromedriver and proxy, as it is quite expensive - and it has a fail-rate that is way higher than anything else I scrape.\n",
      "\n",
      "Can you think of another way, where I can get the bearer token without running a chromedriver with a proxy?\n",
      "\n",
      "Ps. To be clear: I am not asking for advice on which proxy to use.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1de2bvv/how_can_i_retrieve_bearer_token_without_using/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Not sure if this is web scraping, but need to find out when a local government will have an organization on its agenda/minutes.\n",
      "Text: Basically, we are tracking when a local company (polluter) is being mentioned in the agenda/minutes of some of our local governments here. Want to automate something so I can get email alerts so we can be there and speak out against. Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ddxau8/not_sure_if_this_is_web_scraping_but_need_to_find/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: how to scrape a website with live data?\n",
      "Text: Im using python requests and having trouble scraping websites with javascript and changing elements. \n",
      "\n",
      "i would preferably like to keep everything using just requests instead of having to use selenium, and send least amount of requests as possible for live data, but not sure if thats possible.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ddup1k/how_to_scrape_a_website_with_live_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: \"Download as CSV\" keeps redirecting me to login page.\n",
      "Text: I'm trying to use python requests and sessions to download a csv file with my credentials but I keep getting redirected back to login. I'm only able to get this to work if I take a session cookie from my logged in browser and use that, which isn't a solution for me. Any help would be appreciated\n",
      "\n",
      "\n",
      "Save to CSV link: `https://oxlive.dorseywright.com/screener/simple/csv/title/stockscreener06112024/id_query/13957`\n",
      "\n",
      "Login Page link: `https://oxlive.dorseywright.com/login`\n",
      "\n",
      "Login Authentication redirect: `https://signin.nasdaq.com/api/v1/authn`\n",
      "\n",
      "What I have so far:\n",
      "\n",
      "    import requests\n",
      "    \n",
      "    s = requests.Session()\n",
      "    \n",
      "    headers = {...}\n",
      "    response = s.get(\n",
      "        'https://oxlive.dorseywright.com/screener/simple/csv/title/stockscreener06112024/id_query/13957',\n",
      "        headers=headers,\n",
      "    )\n",
      "    \n",
      "    headers = {...}\n",
      "    json_data = {\n",
      "        'password': 'pass',\n",
      "        'username': 'user,\n",
      "    }\n",
      "    response = s.post('https://signin.nasdaq.com/api/v1/authn', headers=headers, json=json_data)\n",
      "    \n",
      "    headers = {...}\n",
      "    response = s.get(\n",
      "        'https://oxlive.dorseywright.com/screener/simple/csv/title/stockscreener06112024/id_query/13957',\n",
      "        headers=headers,\n",
      "    )\n",
      "\n",
      "    print(response.content)\n",
      "\n",
      "*Note, Dorsey Wright hasn't gotten back to me on if they have an API for my account subscription level - I'm just looking to download this regularly without having to navigate the site.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ddyrln/download_as_csv_keeps_redirecting_me_to_login_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping BET365 using Python\n",
      "Text: I've found the following code online that worked about 5 months ago still for datascraping BET365, however when trying it today it no longer works, I've tried using it with both Selenium==3.4 and the most recent update of selenium but both give errors all over the place. Anyone knows how to make this code function or with what version of selenium it works? I've already updated the useragentarray and the chrome path ofcourse. \n",
      "\n",
      "    from selenium import webdriver\n",
      "    from selenium.webdriver.chrome.options import Options\n",
      "    import time\n",
      "    \n",
      "    def driver_code():\n",
      "        options = Options()\n",
      "        useragentarray = [\n",
      "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
      "        ]\n",
      "    \n",
      "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
      "        options.add_argument(\"--no-sandbox\")\n",
      "        options.add_argument(\"--disable-dev-shm-usage\")\n",
      "        options.add_argument(\"disable-infobars\")\n",
      "        options.add_argument(\"disable-blink-features=AutomationControlled\")\n",
      "    \n",
      "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
      "        options.add_experimental_option(\"useAutomationExtension\", False)\n",
      "        options.set_capability(\"pageLoadStrategy\", \"normal\")\n",
      "    \n",
      "        driver = webdriver.Chrome(\n",
      "            executable_path='C:\\\\Users\\\\name\\\\Downloads\\\\chromedriver-win32\\\\chromedriver-win32\\\\chromedriver.exe',\n",
      "            options=options\n",
      "        )\n",
      "        driver.execute_script(\n",
      "            \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n",
      "        )\n",
      "    \n",
      "        driver.execute_cdp_cmd(\n",
      "            \"Network.setUserAgentOverride\", {\"userAgent\": useragentarray[0]}\n",
      "        )\n",
      "    \n",
      "        options.add_argument(\"--disable-popup-blocking\")\n",
      "        driver.get(\"https://www.bet365.com/#/AC/B1/C1/D1002/E91422157/G40/H^1/\")\n",
      "        driver.set_window_size(390, 844)\n",
      "        time.sleep(1)\n",
      "        return driver\n",
      "    \n",
      "    def open_tab(driver, link):\n",
      "        driver.execute_script(f\"\"\"window.open('{link}', \"_blank\");\"\"\")\n",
      "        time.sleep(2)\n",
      "        driver.switch_to.window(driver.window_handles[-1])\n",
      "    \n",
      "    new_driver = driver_code()\n",
      "    open_tab(new_driver, 'https://www.bet365.nl/#/AS/B1/I^16/J^1/')\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ddd1ky/webscraping_bet365_using_python/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Extracting the title of YouTube video - relatively simple but I can't figure it out?\n",
      "Text: I'm pretty sure I've correctly identified the element that the title is in, but it won't extract for whatever reason. I've tried countless things, and it's running in Selenium, so I don't think it's YouTube 403ing me.\n",
      "\n",
      "It's identifying the **video\\_link**, so obviously that part of the element works. I just don't understand why it won't get the **video\\_title** from the same element.\n",
      "\n",
      "    from selenium import webdriver\n",
      "    from selenium.webdriver.chrome.service import Service\n",
      "    from selenium.webdriver.common.by import By\n",
      "    from selenium.webdriver.chrome.options import Options\n",
      "    from selenium.webdriver.support.ui import WebDriverWait\n",
      "    from selenium.webdriver.support import expected_conditions as EC\n",
      "    from webdriver_manager.chrome import ChromeDriverManager\n",
      "    \n",
      "    # Set up Selenium WebDriver\n",
      "    options = Options()\n",
      "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
      "    \n",
      "    # URL to scrape\n",
      "    url = \"https://www.youtube.com/@Meowmeow13/videos\"\n",
      "    \n",
      "    # Load the page\n",
      "    driver.get(url)\n",
      "    \n",
      "    # Wait for the page to load necessary elements\n",
      "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
      "    \n",
      "    # Find the first link containing 'watch?v='\n",
      "    first_link = None\n",
      "    links = driver.find_elements(By.TAG_NAME, \"a\")\n",
      "    for link in links:\n",
      "        href = link.get_attribute('href')\n",
      "        if href and 'watch?v=' in href:\n",
      "            first_link = link\n",
      "            break\n",
      "    \n",
      "    if first_link:\n",
      "        # Get the link URL\n",
      "        video_link = first_link.get_attribute('href')\n",
      "        \n",
      "        # Get the title of the video\n",
      "        video_title = first_link.get_attribute('title').strip()\n",
      "    \n",
      "        print(video_link)\n",
      "        print(video_title)\n",
      "    \n",
      "    # Close the driver\n",
      "    driver.quit()\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ddq4c8/extracting_the_title_of_youtube_video_relatively/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: school grading app help\n",
      "Text: I am making a school app that essentially takes information from our database and posts it. I don't know if this is the right place to ask but the website our school uses Skyward, restricts their API. I have recently heard about web scraping and was wondering if it's possible. As far as I can understand web scraping is a way to automate taking data from a website. But for a school website such as Skyward, everyone has their own username and password. The only way we sign in to our accounts is through \"sign in with Google\". Is there a way to \"web scrape\" and retrieve data in a way that anyone can use my application as long as they have a Skyward account? I know it probably doesn't work and I'm stupid because it would be illegal to get people's usernames and passwords. But is there something I don't know? Please let me know! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ddpezf/school_grading_app_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I can scrape any public page I want and have many scrapers I wrote but I am a \"beginner\", what would make me a \"pro\"? What skills do I need?\n",
      "Text: Hi all,\n",
      "\n",
      "I want to scrape harder pages like LinkedIn, etc. How do I accomplish this? What makes you \"advanced\"?\n",
      "\n",
      "To start, I don't use proxies so I know that's one thing at least. What else is there in your toolbelt that helps you scrape \"anything\"? \n",
      "\n",
      "I have experience (1-2 years at a hedgefund) setting up scrapers that have been running daily, navigating pages, and even entering one time passwords to authenticate and crawl 60+ tabs at once. What am I missing? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dds1zf/i_can_scrape_any_public_page_i_want_and_have_many/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: FreshRSS & Xpath\n",
      "Text: I got FreshRSS up and running and am trying to add RSS feeds. The Associated Press (AP) has an RSS Feed in the code but it returns articles on a 1 month delay.\n",
      "\n",
      "FreshRSS has a function where you can use HTML + XPath to create your own RSS Feeds. This is where I've really been struggling to get XPath nailed down.\n",
      "\n",
      "I've gotten to the point where it is bring in separate articles (see image). It isn't, however, bringing in the headline, description, or image. When I open a post and then click on the \"urn\" link, it does take me to a single article on AP.\n",
      "\n",
      "Can someone point me in the right direction on items need for FreshRSS. The right side of the image shows but I currently have listed. This is the AP site I'm trying to pull from: [https://apnews.com/us-news](https://apnews.com/us-news)\n",
      "\n",
      "https://preview.redd.it/e6k1yofstz5d1.jpg?width=1236&format=pjpg&auto=webp&s=52c82ee27a6669010c4e8bdbb03d9af6ab861013\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ddmor0/freshrss_xpath/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ticketmaster Automation\n",
      "Text: Hi, was successfuly using puppeteer extra stealth to log into ticketmaster accounts, but now it seems they're detecting it. Anyone know which automation library currently works for TM logins? Thanks in advance\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ddiy6q/ticketmaster_automation/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to make 100000+ requests?\n",
      "Text: Hi , Scraper's \n",
      "\n",
      "I have been learning webscraping for a quite some time and worked on quite a bit project's(personal for fun and learn). \n",
      "\n",
      "Never did a massive project where I have to make thousand of requests. \n",
      "\n",
      "I like to know that\n",
      "HOW TO MAKE THAT MANY REQUESTS WITHOUT HARMING THE WEBSITE OR GETTING BLOCKED?(I know Proxies are needed) \n",
      "\n",
      "What methods I came up with.\n",
      "\n",
      "1.httpx(Async)+Proxies\n",
      "\n",
      "   I thought I will use asyinco.gather with Httpx(async) client to make all the requests in one go.\n",
      "\n",
      "    But you can only use one proxy with one client  and If I make multiple client to make requests with different proxies then I think its better If I use non-async httpx(makes thing much easier). \n",
      "\n",
      "2.(httpx/requests)+(concurrent/threading)+Proxies\n",
      "\n",
      "  This Approach is simpler I would use normal requests with threading that way I can make different requests with different workers.\n",
      "\n",
      "   But this Approch is dependent on no. of workers that is dependent upon your cpu.\n",
      "\n",
      "\n",
      "So My Question is how to this properly where I can make thousands of requests(fast) without harming the website. \n",
      "\n",
      "Scraping As Fast As Possible. \n",
      "\n",
      "Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dd7c22/how_to_make_100000_requests/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do i get started? \n",
      "Text: Hi, very new into coding!\n",
      "\n",
      "\n",
      "\n",
      "Wish to create a simple program that can check a website every 10 minutes to see if specific text has been added and if true = message me with current info from the website so i can book it. \n",
      "\n",
      "  \n",
      "Additionally if possible id like to be able to add multiple or remove words that it searches for. How do i get started?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dde2mb/how_do_i_get_started/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: About Twitter Web scrapping\n",
      "Text: Could someone please let me know if Twitter is using dynamic CSS classes and how frequently these classes change? Thanks in advance.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ddcnp9/about_twitter_web_scrapping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: 403 Response\n",
      "Text: Hello All,\n",
      "\n",
      "I'm fairly new to scraping, but love the info you can find and collect while doing it. Recently, a website I've been scraping for a while is now producing a 403 error when i try to scrape it, but I can access it via my regular browser. I've also used fake user agents when attempting to scrape, but that's still producing a 403 error.\n",
      "\n",
      "  \n",
      "Any advice on where to turn next?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dd2z8f/403_response/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Seeking Guidance on Scraping LinkedIn Without Getting Blocked\n",
      "Text: Hi everyone,\n",
      "\n",
      "I'm working on a project where I need to scrape data from LinkedIn, and I'm trying to find a way to do this without getting blocked. Here is my current approach, and I'm hoping to get some guidance on whether this is feasible and any improvements I can make.\n",
      "\n",
      "# My Approach\n",
      "\n",
      "1. **Using the Same Chrome with User's Google Account:**\n",
      "   * I'm using the user's existing Chrome browser where they are already logged in with their Google account. This way, I can leverage the existing LinkedIn cookies and avoid the need for additional logins, which could trigger unusual activity detection.\n",
      "2. **Running the Script Without UI:**\n",
      "   * The script runs in the background without displaying any UI. This ensures that the user experience is not disrupted while the script is running.\n",
      "3. **Using the Same IP Address and Chrome Tab:**\n",
      "   * The script operates using the same IP address and Chrome tab that the user is already using. This minimizes the chances of LinkedIn detecting the scraping activity as coming from a different location or session.\n",
      "4. **Human Behavior Simulation:**\n",
      "   * The script simulates human behavior by mimicking mouse movements, clicks, and scrolling patterns. This helps in avoiding detection by LinkedIn's bot protection mechanisms.\n",
      "5. **Scraping Data:**\n",
      "   * The data scraping happens in the background. However, the main challenge is ensuring that the user's laptop remains open and connected to the internet during this process.\n",
      "\n",
      "# Key Challenges\n",
      "\n",
      "* **User's Laptop Cannot Be Closed:**\n",
      "   * The script requires the user's laptop to stay open and connected to the internet. If the laptop is closed or goes to sleep, the scraping process will be interrupted.\n",
      "\n",
      "# Questions\n",
      "\n",
      "1. **Feasibility:**\n",
      "   * Is this approach viable for scraping LinkedIn data without getting blocked? Are there any adjustments or improvements you would recommend?\n",
      "2. **Headless Mode Concerns:**\n",
      "   * Running in headless mode might use a different Chrome instance, requiring login credentials again. Is there a way to use headless mode while maintaining the same session and cookies?\n",
      "3. **Minimizing Detection:**\n",
      "   * Are there any additional techniques or best practices to further minimize the risk of detection by LinkedIn?\n",
      "\n",
      "I appreciate any insights or suggestions you can provide. Thank you for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dd7x5c/seeking_guidance_on_scraping_linkedin_without/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Removing articles referencing the same event\n",
      "Text: Hello all, I'm currently working on a webscraping project that scrapes and processes articles with some AI tools. An issue I'm running into is the prevalence of duplicate references. As one may expect of a news feed, several articles will often reference the same event, while what I'm interested in is mainly counting the number of distinct events. I appreciate there's no silver bullet for something like this, but has anyone dealt with anything similar? Is there an API or approach I could use that would make it easier to remove redundancies? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dcu0r9/removing_articles_referencing_the_same_event/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any best free scrapping tools to get leads for a software company \n",
      "Text: Hi,\n",
      "\n",
      "Can anyone suggest best free scrapping tools to get high ticket leads for software company \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dcw2ns/any_best_free_scrapping_tools_to_get_leads_for_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do I capture requests when I scroll down in Python selenium chromedriver?\n",
      "Text: The hidden API is insane, so I need to make chromedriver scroll down and get the API calls I need based off of some parsing. How do I do this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dcyfra/how_do_i_capture_requests_when_i_scroll_down_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I made script that collect proxies and store them\n",
      "Text: Hello guys i made a python script that collect proxies from many sources then check if they're working then store them in json file while organize them into countries and proxy type\n",
      "\n",
      "so far i only added one source , but i plan to add many more   \n",
      "i hope someone find it userfull\n",
      "\n",
      "[https://github.com/dragonscraper/ProxyHarvest](https://github.com/dragonscraper/ProxyHarvest)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dc77za/i_made_script_that_collect_proxies_and_store_them/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for a service to help scrape facebook groups \n",
      "Text: Hey all,\n",
      "\n",
      "  \n",
      "I have an application wherein I'm attempting to scrape two things:\n",
      "\n",
      "1) Group search results (such as https://www.facebook.com/groups/search/groups\\_home/?q=business),for which a login is needed\n",
      "\n",
      "2) The main data on the front page of particular groups (such as https://www.facebook.com/groups/2815042615255352) - for which a log in is not needed (just for member count, group description, etc. - not individual posts or comments).\n",
      "\n",
      "I'm hoping to find a simple service where I can do some kind of PHP cURL connection to request a certain URL like one or the other above above and receive back the source code in real time.   \n",
      "  \n",
      "(Bonus points if I could craft rules on their end to isolate and only return the tidbits I want, but either way would work.)\n",
      "\n",
      "Happy to pay for the service, of course (and I'm not sure #1 is even possible, with the login, but I'd take #2 at least).\n",
      "\n",
      "(I would use the Groups API ... if FB hadn't taken that down a few months ago 🫤)\n",
      "\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dcbmle/looking_for_a_service_to_help_scrape_facebook/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to download all philosophy works in project gutenberg?\n",
      "Text: I'm into philosophy and I want to make a RAG Generative AI model that pretends to be famous philosophers who you can discuss philosophical topics with.\n",
      "\n",
      "I'm looking to source my books from project gutenberg (which has a [library of philosophy](https://www.gutenberg.org/ebooks/bookshelf/57)), but their documented scraping methods only allow you to download the ENTIRE library.\n",
      "\n",
      "Anyone know how to get an individual topic (in my case, philosophy)?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dc6k43/how_to_download_all_philosophy_works_in_project/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Has anyone had success with Resident Advisor ra.co ?\n",
      "Text: I'm trying to create a simple web-scraping tool to use on the [Resident Advisor](https://ra.co/) website - I just want to either extract text or take a screenshot of certain pages.\n",
      "\n",
      "I think they use Cloudflare protection amongst other things possibly - I am not very technically knowledgable about web scraping and code stuff yet.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dbnk5f/has_anyone_had_success_with_resident_advisor_raco/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What is a reasonable amount of time to wait between one request and another?\n",
      "Text: Currently I'm not in a hurry and I calculate a random amount of time between 1000 and 3000 milliseconds, but I don't want to be a fool either, and if I can set it faster without causing problems, the better.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dbgkvf/what_is_a_reasonable_amount_of_time_to_wait/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Having trouble scraping this\n",
      "Text: I am trying to scrape the stock price on [https://www.investing.com/equities/nike](https://www.investing.com/equities/nike) but I get: AttributeError: 'NoneType' object has no attribute 'get\\_text'\n",
      "\n",
      "Is there anything wrong with my code that could be causing this? I am confused:\n",
      "\n",
      "https://preview.redd.it/zc581jo1qf5d1.png?width=1789&format=png&auto=webp&s=440f047daaddd7ee1fdafa21b3f0db17eeaa10d1\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    \n",
      "    url = \"https://www.investing.com/equities/nike\"\n",
      "    \n",
      "    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
      "    \n",
      "    page = requests.get(url, headers=header)\n",
      "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
      "    \n",
      "    price = soup.find(\"span\", attrs={'class':'instrument-price_instrument-price__2w9MW flex flex-wrap items-end font-bold'}).get_text()\n",
      "    \n",
      "    print(price)\n",
      "    \n",
      "    \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dbg6hd/having_trouble_scraping_this/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Getting a variable from a script (Scrapy)\n",
      "Text: Hey guys,\n",
      "\n",
      "  \n",
      "I am looking to scrape a website: [https://www.idealista.com/inmueble/102296378/](https://www.idealista.com/inmueble/102296378/)\n",
      "\n",
      "As you can see, there is around 36 images, but 33 are hidden. Although I found out that these are stored in the script.\n",
      "\n",
      "I am using Scrapy right now (no extras) and would like to get this variable:  \n",
      "`var adMultimediasInfo = {`\n",
      "\n",
      "`fullScreenGalleryPics: [...]`\n",
      "\n",
      "`};`\n",
      "\n",
      "https://preview.redd.it/2iayh4h26f5d1.png?width=3010&format=png&auto=webp&s=cc581fa86e8004d0e0c534804feb5d7cee042fab\n",
      "\n",
      "What is the easiest way to do this?\n",
      "\n",
      "Any help is appreciated!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dbdtmt/getting_a_variable_from_a_script_scrapy/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Parsing response for web scrapping\n",
      "Text: Hey. I came across very weird issue. I am working on a project to scrap different websites. I completed few of them and now I am scrapping WayFair. The URL for reference is\n",
      "\n",
      "URL: [https://www.wayfair.com/furniture/pdp/17-stories-daiah-ergonomic-curved-height-adjustable-standing-desk-w006256730.html?piid=41585585](https://www.wayfair.com/furniture/pdp/17-stories-daiah-ergonomic-curved-height-adjustable-standing-desk-w006256730.html?piid=41585585)\n",
      "\n",
      "The issue is that I was not able get tags while using BeautifulsSoup so after debugging the whole thing I came across my finding that the response which I am getting is not readable. By readable I mean that if I save my response as response.html file and try to open it, the IDE will show following warning\n",
      "\n",
      "The file is not displayed in the text editor because it is either binary or uses an unsupported text encoding.\n",
      "\n",
      "I dont know what to do now. I am stuck. Although if I ping using Postman as Get REQUEST I get the proper response. Below is my code\n",
      "\n",
      "      headers = {\n",
      "                    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
      "                    \"Accept\": \"*/*\",\n",
      "                    \"Connection\": \"keep-alive\"\n",
      "                }\n",
      "            try:\n",
      "                response = requests.get(url=final_url, headers=headers, timeout=8)\n",
      "                print(response.status_code)\n",
      "            except requests.RequestException as e:\n",
      "                return f\"Request failed: {e}\"\n",
      "    \n",
      "            # Check if the request was successful\n",
      "            if response.status_code == 200:\n",
      "                try:\n",
      "                    with open('response.html', 'w', encoding='utf-8') as file:\n",
      "                        file.write(response.content)\n",
      "                        print(\"Response saved to response.html\")\n",
      "                \n",
      "                except  Exception as e:\n",
      "                      print(\"File wrote fail\")\n",
      "                print(\"Response is okay\")\n",
      "                soup = BeautifulSoup(response.content, 'html.parser',from_encoding=\"iso-8859-1\")\n",
      "                with open('parsed_soup.html', 'w', encoding='utf-8') as file:\n",
      "                    file.write(soup.prettify())\n",
      "\n",
      "  \n",
      "Just for your reference, the content encoding of the request is br\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dbbwyn/parsing_response_for_web_scrapping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Bypass CF under headless machines\n",
      "Text: Hello,\n",
      "\n",
      "I am trying to bypass a cloudflare site from Ubuntu server machine.\n",
      "\n",
      "Till now I uses seleniumbase but since yesterday it can't bypass the CF (semes that it is not working on headless machines, again)\n",
      "\n",
      "Now I am using selenium-driverless on windows as backup solution, but I want to know if there are any backup solution in order to scrape a website from headless machines (like Ubuntu server)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1db0php/bypass_cf_under_headless_machines/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to web scrape tables which can be changed by selecting a date?\n",
      "Text: I'm trying to scrape data off of a webpage, and I've managed to make a small script that scrapes everything that is currently shown on the website. Problem is you have a date picker where you can choose a date and see tables relevant to that date. How can I add them to the scraper so it scrapers every table on the website and not just the table available on the landing page?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dbampm/how_to_web_scrape_tables_which_can_be_changed_by/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: URGENT Help! I need to scrape booking.com hotel pricing & room plans .\n",
      "Text: Hello I am a student i need to scrape pricing & room plans from individual hotel page. I tried using multiple methods but i failed\n",
      "\n",
      "Earlier i scrapped Guest reviews & total hotel list page for a particluar area using Scrapy & analyzing Network json request.\n",
      "\n",
      "But i find the pricing & room plans request so complex.\n",
      "\n",
      "PLEASE TELL ME HOW TO SCRAPE IT. ITS REALLY URGENT \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1db6iaa/urgent_help_i_need_to_scrape_bookingcom_hotel/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping a site that uses \"Wordfence\" (Wordpress plugin), does anyone has experiences with that.\n",
      "Text: I'm going to scrape content from a WordPress site that uses a plugin called \"Wordfence\", i got a error message after manually opening several links in quick succession (new tabs), but it worked when slowing down.\n",
      "\n",
      "I was wondering if anyone has some experiences about this plugin?\n",
      "\n",
      "I hope it's enough to fake my user agent, maybe add some legit cookies and header entries from my browser and rate limit myself by waiting a few seconds between each request.\n",
      "\n",
      "The site also has some paid section that i might want to scrape too, but i worry that the site will ban the account/payment method or link my scraping activity to my real name and payment info.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1db66iy/scraping_a_site_that_uses_wordfence_wordpress/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do I scrap the web for domain names with obfuscated letters?\n",
      "Text: Hello everyone.\n",
      "\n",
      "I am looking for any ideas on where to start with domain name searches. For example there is google.com.\n",
      "\n",
      "I would like to search for domains that are [1google.com](http://1google.com) or [googlle.com](http://googlle.com) or [goog1e.com](http://goog1e.com) or when letters are replaced with something from extended alphabet. \n",
      "\n",
      "Basically search for domains phishers use. My goal is to be able to catch those domains as soon as possible after registration. I know that there are companies like Zerofox that do this, however I wonder how and where I could start.\n",
      "\n",
      "Thanks all.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1db47nw/how_do_i_scrap_the_web_for_domain_names_with/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping images \n",
      "Text: Hey. I want to fetch images from my scrapper. The issue is that I am not able to get products images tag. The URL which I am using is \n",
      "\n",
      "Product URL:https://www.wayfair.com/furniture/pdp/17-stories-daiah-ergonomic-curved-height-adjustable-standing-desk-w006256730.html?piid=41585584\n",
      "\n",
      "My code is \n",
      "\n",
      "    kwargs = {'name': 'div', 'attrs': {'class': 'k81pbk4_6112'}}\n",
      "    soup = BeautifulSoup(b''.join(content), 'html.parser',from_encoding=\"iso-8859-1\")\n",
      "            found_element = getattr(soup, method_name)(**kwargs)\n",
      "\n",
      "https://preview.redd.it/wd8js50fxb5d1.png?width=1060&format=png&auto=webp&s=f293dc72cefd237b0829808973edb8c53489c747\n",
      "\n",
      "The images are inside class:k81pbk4\\_6112. Can anyone guide me that how to fetch these images. I have attached the complete hierarchy of where the images are  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1db0ct6/scraping_images/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Beachpatrol: Automate your daily browser from the CLI\n",
      "Text: \n",
      "URL: https://github.com/sebastiancarlos/beachpatrol\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Inconsistent Behavior with HTTP Requests - httpx, requests, and reqwest\n",
      "Text: Hey web scrapers,\n",
      "\n",
      "I've encountered some really odd behaviour when making HTTP requests to a specific URL, I am wondering if anyone has some insights into this weird behaviour. It is also not specific to one url, the behaviour also shows it self for other websites protected with cloudflare.\n",
      "\n",
      "I'm trying to fetch the content from this URL: https://www.novelupdates.com/ . Which is protected with cloudflare. The 403 error contains a javascript check .Here are the results using different HTTP clients:\n",
      "\n",
      "### Python (httpx)\n",
      "```python\n",
      "import httpx\n",
      "\n",
      "headers = {\n",
      "    'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 17_0_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0.1 Mobile/15E148 Safari/604.1',\n",
      "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
      "    'Accept-Language': 'en,en-US;q=0.5',\n",
      "    'Accept-Encoding': 'gzip, br'\n",
      "}\n",
      "\n",
      "url = \"https://www.novelupdates.com/\"\n",
      "response = httpx.get(url, headers=headers, follow_redirects=True)\n",
      "print(response.status_code)  # prints 200\n",
      "```\n",
      "\n",
      "### Python (requests)\n",
      "```python\n",
      "import requests\n",
      "\n",
      "url = \"https://www.novelupdates.com/\"\n",
      "response = requests.get(url, headers=headers, allow_redirects=True)\n",
      "print(response.status_code)  # prints 403\n",
      "```\n",
      "\n",
      "### Rust (reqwest)\n",
      "```rust\n",
      "use reqwest;\n",
      "\n",
      "fn main() -> Result<(), reqwest::Error> {\n",
      "    let client = reqwest::blocking::Client::new();\n",
      "    let url = \"https://www.novelupdates.com/\";\n",
      "\n",
      "    let response = client.get(url)\n",
      "        .header(\"User-Agent\", \"Mozilla/5.0 (iPad; CPU OS 17_0_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0.1 Mobile/15E148 Safari/604.1\")\n",
      "        .header(\"Accept\", \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\")\n",
      "        .header(\"Accept-Language\", \"en,en-US;q=0.5\")\n",
      "        .header(\"Accept-Encoding\", \"gzip, br\")\n",
      "        .send()?;\n",
      "\n",
      "    println!(\"Status 1: {}\", response.status());  // prints: Status 1: 403 Forbidden\n",
      "    Ok(())\n",
      "}\n",
      "```\n",
      "`Cargo.toml`:\n",
      "```toml\n",
      "reqwest = { version = \"0.12.4\", features = [\"blocking\", \"http2\", \"charset\", \"gzip\", \"brotli\", \"rustls-tls\"] }\n",
      "```\n",
      "\n",
      "**Observations:**\n",
      "\n",
      "- With `httpx` in Python, the request returns a status code of 200.\n",
      "- Using `requests` in Python, the same request returns a status code of 403.\n",
      "- In Rust, using `reqwest`, the request also returns a status code of 403.\n",
      "\n",
      "I've ensured that the headers and configurations are consistent across all three clients. The discrepancy in the response codes is puzzling, especially since the `httpx` client manages to get through without issues.\n",
      "\n",
      "Has anyone else faced similar inconsistencies with HTTP clients? Any idea why `httpx` would behave differently from `requests` and `reqwest`?\n",
      "\n",
      "Is there some fingerprinting going on here?\n",
      "\n",
      "Appreciate any insights or suggestions!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1daap0w/inconsistent_behavior_with_http_requests_httpx/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trouble getting json data from IMF website\n",
      "Text: My goal is to get json data about IMF working papers, so I went to [https://www.imf.org/en/publications/search?series=IMF+Working+Papers](https://www.imf.org/en/publications/search?series=IMF+Working+Papers) and with the help of the Chrome DevTools I found that [https://www.imf.org/coveo/rest/search/v2?sitecoreItemUri](https://www.imf.org/coveo/rest/search/v2?sitecoreItemUri) is the json file I need and is requested with the POST method.\n",
      "\n",
      "If I visit the json url directly, though, I only get 10 results, but I need, say, data about the most recent 100 working papers.\n",
      "\n",
      "I created an HTTP session with httpx.Client(), GET requested the first url, then POST requested the json url, but I still get only those 10 useless results.\n",
      "\n",
      "The session is supposed to have taken care of the cookie persistence, but I guess something else is missing...\n",
      "\n",
      "Thanks a lot in advance.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dahdbz/trouble_getting_json_data_from_imf_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: how can i bypassing cloudflare waiting room?\n",
      "Text: i have to purchase show tickets, but it's admin use cloudflare waiting room as a security system, it takes me 7-9 hours long to enter the website, what should i do? i already use some program on github, but it usually used for cloudflare captcha, not waiting room, thank you.\n",
      "\n",
      "PS : i have 0 knowledge at python\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1dadmzd/how_can_i_bypassing_cloudflare_waiting_room/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to bypass cloudflare\n",
      "Text: Hi, I am scraping a website which uses cloudflare to protect itself from bots. Previously I could bypass that by using a python library such as curl\\_cffi which impersonates chrome's tls/ja3/http2 fingerprints and that worked. However recently they enabled some other form of protection which basically works by first the websites returns a 403 response with rayId in the headers and then some other requests are made to the cloudflare servers with that rayId to obtain the cf\\_clearence cookie which at the end is used in a post request to the base url which includes some hashed parameters. I'm sure there are libraries / solutions out there which automate this whole process which I am not aware of so I was wondering if any of you can recommend some?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d9j6kh/how_to_bypass_cloudflare/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: does this mean i can’t scrape the site\n",
      "Text: hello i wanna scrape cargurus for this car i want i wanna scrape the listing and prices and area i been doing research and what i read on said to check the robots.txt file to see if they allow scraping  and they have sm stuff in that file i don’t understand example they have:\n",
      "\n",
      "user-agent: trivatbot \n",
      "Disallow /\n",
      "\n",
      "\n",
      "Disallow:  /forum\n",
      "\n",
      "\n",
      "user-agent: google bot \n",
      "Disallow /\n",
      "\n",
      "Disallow:  /more random things \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "does this mean i can’t use those specific bots or what is that exactly \n",
      "\n",
      "\n",
      "here’s the site so you can help me w more info in case i explained it dumb \n",
      "\n",
      "www.cargurus.com/robots.txt\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d9q8bu/does_this_mean_i_cant_scrape_the_site/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is using recursion to traverse nested json file? the best/universal way to get the specific value you want like \"size\" \"colour\" and etc.\n",
      "Text: I am trying to get better at web scraping so most of the time, I find html structure and use regex to get the json file, and I convert them to JSON and extract data. using for loop to get data I want.\n",
      "\n",
      "  \n",
      "and basically like the title says\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d9m8zq/is_using_recursion_to_traverse_nested_json_file/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Not able to scrape a website using GraphQL\n",
      "Text: I previously posted on this channel and think that I didn't provide the complete information. Here is the detailed description. So I am working on a task where I need to scrap Wayfair.com. The user will give me the product URL and in response I will return following attributes  \n",
      "- Price  \n",
      "- Original Price  \n",
      "-  Free Shipping available(boolean)  \n",
      "- Images of the product  \n",
      "- And product related information\n",
      "\n",
      "This is my first time where I will be using GraphQL to scrap a website. When I inspect the page, I see numerous graphql request has been made to server. But when I compare this with other websites like [justwatch.com](http://justwatch.com) there is only one graphql request to server.  So I am now confused that what should be my payload and how should I ping it. Just for your reference below is my code\n",
      "\n",
      "    headers = {\n",
      "            \"authority\": \"www.wayfair.com\",\n",
      "            \"method\": \"POST\",\n",
      "            \"path\": \"/federation/graphql\",\n",
      "            \"scheme\": \"https\",\n",
      "            \"Accept\": \"/\",\n",
      "            \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
      "            \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
      "            \"Apollographql-Client-Name\": \"@wayfair/sf-ui-layout\",\n",
      "            \"Apollographql-Client-Version\": \"bfc771bd03e6ef6861e311fbde485cc7b898d642\",\n",
      "            \"Content-Length\": \"470\",\n",
      "            \"Content-Type\": \"application/json\",\n",
      "            \"Origin\": \"https://www.wayfair.com\",\n",
      "            \"Priority\": \"u=1, i\",\n",
      "            #\"Referer\": \"https://www.wayfair.com/outdoor/pdp/kenmore-4-burner-propane-gas-grill-with-searing-side-burner-in-black-with-copper-accents-knmr1063.html\",\n",
      "            \"Sec-Ch-Ua\": \"\\\"Google Chrome\\\";v=\\\"119\\\", \\\"Chromium\\\";v=\\\"119\\\", \\\"Not?A_Brand\\\";v=\\\"24\\\"\",\n",
      "            \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
      "            \"Sec-Ch-Ua-Platform\": \"\\\"macOS\\\"\",\n",
      "            \"Sec-Fetch-Dest\": \"empty\",\n",
      "            \"Sec-Fetch-Mode\": \"cors\",\n",
      "            \"Sec-Fetch-Site\": \"same-origin\",\n",
      "            \"User-Agent\": self.ua.random,\n",
      "            \"Wf-Locale\": \"en-US\",\n",
      "            \"Wf-Pageview-Id\": \"I+F9OmZhbpDAOKP04+HUAg==\",\n",
      "            \"X-Parent-Txid\": \"I+F9OmZhb+R0uaRo6EUqAg==\",\n",
      "            \"X-Wayfair-Locale\": \"en-US\",\n",
      "            \"X-Wf-Way\": \"true\"\n",
      "            }\n",
      "            print(\"Here\")\n",
      "    \n",
      "            # GraphQL payload\n",
      "            payload ={\n",
      "                        \" operationName\": \"MinicartFeatureTogglesQuery\",\n",
      "                        \"variables\": {\n",
      "                            \"names\": [\n",
      "                                \"core-funnel-minicart-enable-protection-marketplace-frontend\",\n",
      "                                \"core-funnel-minicart-enable-service-marketplace-frontend\",\n",
      "                                \"core-funnel-minicart-enable-redirect-to-cart-from-hot-deals\",\n",
      "                                \"core-funnel-minicart-enable\",\n",
      "                                \"core-funnel-minicart-enable-federated-cart-contents-query\"\n",
      "                            ]\n",
      "                        },\n",
      "                        \"extensions\": {\n",
      "                            \"persistedQuery\": {\n",
      "                                \"version\": 1,\n",
      "                                \"sha256Hash\": \"3e2d3f070c0e34a870b3aebe41ee24b1f2a4f5c0f217070af408c0eb2b90b182\"\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "            try:\n",
      "                        # Introduce a delay before making the request\n",
      "                        time.sleep(2)\n",
      "    \n",
      "                        response = requests.post('https://www.wayfair.com/federation/graphql', headers=headers, json=payload)\n",
      "                        \n",
      "                        # Raise an exception if the request was unsuccessful\n",
      "                        response.raise_for_status()\n",
      "                        print(response)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d9e2ov/not_able_to_scrape_a_website_using_graphql/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How would I scrape this? https://dss.virginia.gov/facility/search/cc2.cgi\n",
      "Text: I'm trying to web scrape the following site but it seems to be loaded by AJAX. I could manually scrape it using selenium but I was wondering if I could intercept/retrieve the data from the HTTP calls.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d93gsw/how_would_i_scrape_this/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webpages to clean markdown PDFs. Check out my python packages and let me know if there are any issues, requests and ideas.\n",
      "Text: The package takes the sitemap url, index the website, parse the website content in markdown format (clean text content) and create a pdf out of it. \n",
      "\n",
      "Each url will have it's own PDF\n",
      "\n",
      "I'm planning to add the functionality to initiate the module with just one URL (as an example, a specific News Artile) and get a PDF file of the HTML text.\n",
      "\n",
      "```\n",
      "pip install MDPDF-scraper\n",
      "```\n",
      "\n",
      "\n",
      "```\n",
      "https://pypi.org/project/MDPDF-scraper/\n",
      "```\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d908wr/webpages_to_clean_markdown_pdfs_check_out_my/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Geo blocked websites\n",
      "Text: Hi,\n",
      "\n",
      "I'm trying to scrape a website and noticed that I can only access it from a specific country. This probably means it is using some kind of geo-blocking. I was wondering if there is an easy way to quickly determine from where I can access the site?\n",
      "\n",
      "10x :)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d8zrzc/geo_blocked_websites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web scraping outputting 3 out of 36 listings \n",
      "Text: Hi,\n",
      "\n",
      "Im trying to scrape prices of all listings on the  page: [https://www.otodom.pl/pl/wyniki/wynajem/kawalerka/cala-polska?](https://www.otodom.pl/pl/wyniki/wynajem/kawalerka/cala-polska?ownerTypeSingleSelect=ALL&viewType=listing) but Im getting only 3 out of 36. All listings (and their prices) are in the same element. \n",
      "\n",
      "Is website blocking too many requests or did I screw up somewhere in code?\n",
      "\n",
      "    import requests\n",
      "    \n",
      "    headers = {\n",
      "     \"User-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\"\n",
      "    }\n",
      "    \n",
      "    req = requests.get(\"https://www.otodom.pl/pl/wyniki/wynajem/kawalerka/cala-polska?ownerTypeSingleSelect=ALL&viewType=listing\", headers=headers)\n",
      "    req = req.content\n",
      "    \n",
      "    soup = BeautifulSoup(req, 'html.parser')\n",
      "    \n",
      "    rent_prices = []\n",
      "    ul = soup.find('ul', class_='css-rqwdxd e127mklk0')\n",
      "    lis = ul.find_all('li')\n",
      "    \n",
      "    for li in lis:\n",
      "        price = li.find_all('span', class_='css-1uwck7i evk7nst0')\n",
      "        rent_prices.append([price])\n",
      "    \n",
      "\n",
      "And rent\\_prices outcomes:\n",
      "\n",
      "    [[[<span class=\"css-1uwck7i evk7nst0\" direction=\"horizontal\">2499 zł<style data-emotion=\"css v14eu1\">.css-v14eu1{color:#495260;font-size:14px;font-weight:400;}</style><span class=\"css-v14eu1 evk7nst1\">+ <!-- -->czynsz: 680 zł/miesiąc</span></span>]],\n",
      "     [[<span class=\"css-1uwck7i evk7nst0\" direction=\"horizontal\">2300 zł</span>]],\n",
      "     [[<span class=\"css-1uwck7i evk7nst0\" direction=\"horizontal\">5098 zł</span>]]]\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d8s0sv/web_scraping_outputting_3_out_of_36_listings/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How would you scrape the LinkedIn profiles of people who've liked and commented my company LinkedIn post? (For a non-spam case study) \n",
      "Text: Hey!\n",
      "\n",
      "I'm trying to scrape the profile from people who are liking and commenting on my LinkedIn posts. \n",
      "\n",
      "I'm creating valuable guides and give them for free to everyone who like and comment my LinkedIn company posts, but it's getting too much to do it manually. \n",
      "\n",
      "The goal is to automatically scrape these profiles on a weekly basis, so I can get their LinkedIn profiles, job titles, etc on a spreadsheet, and create an email/LinkedIn automation on top of that, \n",
      "\n",
      "At the moment, I'm using the RowX extension to manually scrape the likes of each post on a weekly basis.\n",
      "\n",
      "I've looked into Phantombusters but I think I could get banned.\n",
      "\n",
      "Any suggestions on how I could do this would be welcome :)\n",
      "\n",
      "Thanks a lot!\n",
      "\n",
      "  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d8lc31/how_would_you_scrape_the_linkedin_profiles_of/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is This legal ?\n",
      "Text: I am scraping financial trade reports of US politicians from the US gov website . It's a simple bot that copy politician trades but Will i get ban hammered by US or is it okay?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d7tqtv/is_this_legal/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: scraping site in different countries \n",
      "Text: hey I am trying to scrape a website that has different content depending on which country you connect from. When I collected data by hand I just used Nord VPN but I heard they are pretty strict about webscraping. I have experience using puppeteer for js and selenium in python. Is there a way for me to connect to different countries to scrape all the content? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d7thyd/scraping_site_in_different_countries/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: scraping vision & mission from ~500 websites\n",
      "Text: i need help or ideas for this task. i need to extract vision&mission from app. 500 websites of medium/small companies. i need the program to identify if the information is in the main page (the home page of the website) or if it needs to click on the 'about us' page. is there a way i can do this? it's not that much info and the outputs are relatively short but i'm trying with beautiful soup and it doesn't identify when to click on the 'about us' page :(\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d7g0jw/scraping_vision_mission_from_500_websites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Requesting help/advise in scraping Shopee \n",
      "Text: I need help in scraping Shopee, specificaly the task is given a certain Shopee URL, I would need to go to the page and screenshot the page.\n",
      "\n",
      "However, I am having difficulty in accessing the website through automation. After opening the link, I am immediately redirected to a login page and required to complete a captcha or being denied access. \n",
      "\n",
      "If you don't feel comfortable discussing in public and wanted to help you can dm me.   \n",
      "Thank you in advance\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d7n5lw/requesting_helpadvise_in_scraping_shopee/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping For Potential Clients & Outreach\n",
      "Text: As the title suggests, I am looking to scrape data specifically emails and website urls for potential clients. I've got a plan but it is a bit weird in terms of accomplishing the task.\n",
      "\n",
      "Basically I'd like to find contact info and websites of those potential clients so that I can reach out to them. My initial idea was to use search terms as on many occasions whilst reaching out to textile industries and potential export clients for a company I'd used specific search terms. I feel that is not the most optimal way because right now I am searching for retailers and other entities or companies that may require some sort of CSR service which my company would provide.\n",
      "\n",
      "I have looked into LinkedIn web scraping and have found it to be against ToS so I am trying to move away from that and as a result have ended up here on this sub-reddit to find a solution to my problem.\n",
      "\n",
      "From you, I'd like to ask if there is any database or directory of sort that contains data of newly made companies or of newly registered domains where I can search for retail related companies. Specifically I'd like to ask if there is any other place where data of specific companies is listed to be scraped specifically those who might need CSR services, which I know extend from Retail companies as well but that is the one niche or category I would like to start from.\n",
      "\n",
      "As always all help is greatly appreciated\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d7ecnr/webscraping_for_potential_clients_outreach/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrapping Facebook to look for housing\n",
      "Text: I'm moving to  study in a big city in a couple months and I'm currently looking for housing in many platforms. I've heard the best deal are in Facebook and there are many different groups with multiple messages every day, so keeping track of all of them is a pain\n",
      "\n",
      "I'd like to automate the process of reading new posts and trying to extract information like price, apartment type, distance to university, and when it's available. However since the text is not in a proper format, this task doesn't look very straightforward (maybe AI or NLP can help?)\n",
      "\n",
      "Does anybody know of any project like this or can give tips and pointers on how to start?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d7ct16/scrapping_facebook_to_look_for_housing/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping to find golf tee times\n",
      "Text: I love golf but the tee times where I live are VERY competitive. The second someone cancels online, it is picked up by someone else. Is it possible to build a web scraper that can constantly check the website for available/recently canceled tee times? If so, is that easy to do myself with little to no experience or would you recommend I pay someone on a freelance website?\n",
      "\n",
      "  \n",
      "Thanks in advance!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d76u8l/webscraping_to_find_golf_tee_times/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: TikTok Search autocomplete API\n",
      "Text: Does anyone have an alternative to this api that would allow scraping the autosuggest options from TikTok?\n",
      "\n",
      "[https://apify.com/mscraper/tiktok-search-autocomplete](https://apify.com/mscraper/tiktok-search-autocomplete)  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d7bzcs/tiktok_search_autocomplete_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ajax pagination\n",
      "Text: Hi, I'm scraping a website with AJAX pagination and I'm having trouble bypassing it. Can you recommend any videos or books that could help me with this? Unfortunately, I can't use Selenium in this project. Thank you.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d7agah/ajax_pagination/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Verify the average price per stay in a city? Like airbnb almost?\n",
      "Text: Howdy folks, \n",
      "\n",
      "Looking to build a site that could verify the average price of stay in a city, and could tell someone what the price would be typically per their search qualifications?\n",
      "\n",
      "I dont think airbnb will allow access to their API but does anyone know if there might be some other way to pull this information possibly?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d6o1i1/verify_the_average_price_per_stay_in_a_city_like/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I'm having trouble accessing the description of a movie on Rotten Tomatoes, and I am only using Beautiful Soup.\n",
      "Text: https://preview.redd.it/wn2y7kyjq84d1.png?width=1106&format=png&auto=webp&s=839d8ef3a46168c641d34c1587b6396dd673f75c\n",
      "\n",
      "I am trying to acess the text under the shadowroot thing. There is no class associated with it so I am very stuck.\n",
      "\n",
      "https://preview.redd.it/r0hzuaopq84d1.png?width=969&format=png&auto=webp&s=1b85d3b65a51ca9c4eda24f9ac1dc326433def9b\n",
      "\n",
      "This is my code right now. Please let me know what I am doing wrong, or how I could go about getting the text!!!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d6q7e5/im_having_trouble_accessing_the_description_of_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help needed\n",
      "Text: I want to make a program that would help to collect specific info from many car listings on facebook marketplace. Any help or a guide to where to find the help would be appreciated. \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d6puzs/help_needed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Docker image providing api for Cloudflare Turnstile and WAF\n",
      "Text: With the new update, you can now also get Turnstile tokens. Cloudflare can give us a lot of problems in web scraping. For this, I created a native api that you can use with docker.\n",
      "\n",
      "It will solve it within 5 seconds on average. If you are dealing with WAF, if you send a request using the headers object in the response, you will bypass WAF including the corporate plan. If you are interested in Turnstile captcha, it will return you a token within 5 seconds on average.\n",
      "\n",
      "  \n",
      "[https://github.com/zfcsoftware/cf-clearance-scraper](https://github.com/zfcsoftware/cf-clearance-scraper)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d69kh2/docker_image_providing_api_for_cloudflare/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Im looking to automatize a brief report of hot topics on animal welfare. Where to start?\n",
      "Text: Long story short, I recently started a new position related to animal welfare policy.\n",
      "\n",
      "It'd be extremely helpful if I could get a weekly summary of the hottest topics in the field from different sources (X, Linkedin, News outlets, etc).\n",
      "\n",
      "I understand that webscrapping is the way to go if I'm to do this and I was thinking of using knime to do it (since its low code to no code I could easily build it and teach my much older colleagues how to use it for their specific sub-topics in the world of animal welfare).\n",
      "\n",
      "Now, Im completely lost as to where to start in practical terms:\n",
      "\n",
      "- Is it dumb of me to want to use Knime? Should I look into other toold first?\n",
      "\n",
      "- Is webscrapping not the best approach for what Im trying to do?\n",
      "\n",
      "- Is it too ambitious to want a weekly summary from multiple sources?\n",
      "\n",
      "- I dont know how to use the APIs, I have found some tutorials on the Knime hub for the use of newsapi.org, but Im not sure what I should be looking for in terms of technical limitations?\n",
      "\n",
      "- Lastly, when not using an API, what are the things I should be looking out for drom a legal pov? Is it something that can get me in trouble?\n",
      "\n",
      "\n",
      "Thanks a mill in advance, if anyone could help even for just one of these questions that would already mean a lot!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d6kw9l/im_looking_to_automatize_a_brief_report_of_hot/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Take a look at Open Proxy Project (Proxy Lists)\n",
      "Text: [https://opp.redsoc.in/](https://opp.redsoc.in/)\n",
      "\n",
      "  \n",
      "The collection of alive and maintained proxies available to public. Open Proxy Project is free and open source. \n",
      "\n",
      "Let me know of your thoughts.\n",
      "\n",
      "  \n",
      "Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d6hvru/take_a_look_at_open_proxy_project_proxy_lists/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: My puppeteer powered web scraper is being blocked by some websites like Etsy and angellist.\n",
      "Text: I'm building a web scraping tool powered by Puppeteer and I am hitting this error consistently on some websites like Etsy and Angellist.\n",
      "\n",
      "The error is \"Please enable JS and disable any ad blocker\" after which I get redirected to another page with the error \"Why is this verification required? Something about your browser’s behavior has caught our attention.\n",
      "\n",
      "There are various possible explanations for this:\n",
      "\n",
      "*  You are browsing and clicking at a speed much faster than expected of a human being.\n",
      "*  Something is preventing JavaScript from working on your computer.\n",
      "*  There is a robot on the same network (IP xx.xx.xx.xx) as you.\" \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d6ew8y/my_puppeteer_powered_web_scraper_is_being_blocked/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to test a scraper\n",
      "Text: Writing my scraper, I've been building overall logic on mock websites that are designed for scraping, but eventually I'll want to scrape the actual target website. How can I test my scraper then?\n",
      "\n",
      "  \n",
      "The solution I came up with would be to create a mock website with strictly the behavior I am expecting, meaning I only create the buttons I would need to click, the tables I'd have to scrape the data from, etc.. However this seems quite time consuming and prone for errors (the website might have honey traps, or behave slightly differently under the hood.\n",
      "\n",
      "  \n",
      "I should note that testing on the actual website is out of the question. There's a strong anti bot protection in place. It's a grocery store (something like Walmart), so they're very protective of their data to (I assume) stump competition.\n",
      "\n",
      "  \n",
      "I'm using Playwright in Golang. Not the most common choice, but I wanted to learn the language, as well as web scraping, so I thought it might kill two birds with one stone.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d6edwl/how_to_test_a_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Cloudfare Protection\n",
      "Text: Hello everyone, \n",
      "I am trying to access a website via Selenium Python so as to automate some daily actions I take at this domain almost daily. I have used Selenium before so I quicly tested against the website's home page and Selenium failed the second that it load the page, redirecting me to a special screen said that Cloudfare blocked me.\n",
      "I have heard things that Cloudafare is really hard to bypass but I give it another try. This time I have added/disabled certain known flags that make Selenium detectable and added the known thing about Selenium returning true if the domain execute JS on my browser to see if webdriver is set to true rather than undefined etc. again it failed same failure behaviour. Then I tried to load one of my chrome profiles to make it look more natural, run always non headless, maximazed window size etc, same results again. Configured chromedriver as a mobile one, again the same. \n",
      "Then I tried selenium stealth package and add this add on to my webdriver again failure. Havent tried to rotate my user agents since the failure happens at first request, judt used one two different ones just in case. All these attempts failed. \n",
      "Googled a little bit, found out about proxies. \n",
      "Signed Up for Zen Rows, got the free trial then used this service to send request to the website. All the attempts returned 422 status code. Enabled premium residential proxies origamited from my country as they claim, enbaled JS rendering option again nothing. Integrating Zen Rows with my selenium driver again nothing. Same with plain requests both from the dashboard and using the pip installed package they have and runned it through python locally.\n",
      "Tried another similar service, apiscrape same results and here I am lol \n",
      "The question is obvious, is there any way to do the job or cloudfare puts an end?\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d5zm10/cloudfare_protection/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help me make a program that webscrapes morningstar \n",
      "Text: Help me make a program that webscrapes morningstar for its one year trailing returns for stocks. The link for BHP is [https://www.morningstar.com/stocks/xasx/bhp/trailing-returns](https://www.morningstar.com/stocks/xasx/bhp/trailing-returns) and the current one year trailing return is 11.51. Ideally, I want to have a bunch of these stocks which the program collects and spits out. I dont know much (anything) about coding.\n",
      "\n",
      "Luckily someone has already pretty much done it:\n",
      "\n",
      "    import requests\n",
      "    \n",
      "    link = 'https://api-global.morningstar.com/sal-service/v1/fund/portfolio/holding/v2/FOUSA06WRH/data'\n",
      "    \n",
      "    headers = {\n",
      "        'apikey': 'lstzFDEOhfFNMLikKa0am9mgEKLBl49T',\n",
      "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'\n",
      "    }\n",
      "    \n",
      "    payload = {\n",
      "        'premiumNum': '100',\n",
      "        'freeNum': '25',\n",
      "        'languageId': 'en',\n",
      "        'locale': 'en',\n",
      "        'clientId': 'MDC',\n",
      "        'benchmarkId': 'mstarorcat',\n",
      "        'component': 'sal-components-mip-holdings',\n",
      "        'version': '3.59.1'\n",
      "    }\n",
      "    \n",
      "    with requests.Session() as s:\n",
      "        s.headers.update(headers)\n",
      "        resp = s.get(link,params=payload)\n",
      "        container = resp.json()\n",
      "        portfolio_date = container['holdingSummary']['portfolioDate']\n",
      "        equity_holding = container['numberOfEquityHolding']\n",
      "        active_share = container['holdingActiveShare']['activeShareValue']\n",
      "        reported_turnover = container['holdingSummary']['lastTurnover']\n",
      "        other_holding = container['holdingSummary']['numberOfOtherHolding']\n",
      "        top_holding = container['holdingSummary']['topHoldingWeighting']\n",
      "        print(portfolio_date,equity_holding,active_share,reported_turnover,other_holding,top_holding)\n",
      "\n",
      "The original link they used was: [https://www.morningstar.com/funds/xnas/aepfx/portfolio](https://www.morningstar.com/funds/xnas/aepfx/portfolio)\n",
      "\n",
      "It works, but this program gets different statistics for what I want, and the programmer found the api key and api link, which is also what I need help doing.\n",
      "\n",
      "I know any decent programmer will be able to do this in like 120 seconds. Cheers.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d652oj/help_me_make_a_program_that_webscrapes_morningstar/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: webscraping chatgpt website?\n",
      "Text: hello, I want to see if someone have tried webscrapping openai website before. basically instead of using the offical api to access the gpts, I want to instead find a way to access the gpts through the chats section so i can access things like custom gpts and gpt-4o\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d5jghq/webscraping_chatgpt_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Facebook Posts in Bulk\n",
      "Text: So as the title says, I want to bulk scrape posts from a facebook group. I can already scrape using scrolling and it is exactly what I'm doing. And it works but it's not as good as it should be. So is there a way to scrape posts from a facebook in bulk? If you know any method, please mention. 🙏\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d5kgye/scraping_facebook_posts_in_bulk/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: 12 yrs worth of posts getting deleted...\n",
      "Text: Hi there, I was directed here from another reddit. I am not massively tech literate nor sure of where to go or who to ask about this, but I have this fanfic website I've been using for 12 years called Quotev, recently, they took out the social feed & forums where you could see other people's posts & plan on disabling the messages feature & groups, now it's hard to find your friends on there and such from years passed. It says they are deleting all messages on July 1st, and I was hoping if anybody knows an easy method to archive all my posts on the website? the activity page is the main part i'd want to save; it's [https://www.quotev.com/(username)/activity](https://www.quotev.com/(username)/activity) (but with my username there instead haha). I felt it sad to know that so many posts from so many users may be lost\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d57zqm/12_yrs_worth_of_posts_getting_deleted/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Legal Web Scraping\n",
      "Text: I have a client that is asking for facebook and instagram web scraping. Although there are ways to do this and to handle ip bans with VPNs, proxyes, etc, the client is asking also for private profiles/groups information. I'm pretty sure that unless the client wants at most the numbers of follower and following of an Instagram private profile, anything beyond that would be trying to force or hacking the Intagram profile (not planning to that at all). Which would be the line I should not cross on this request?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d5ar54/legal_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to decode compressed JSON stream while scraping with Node\n",
      "Text: I am trying to scrape some financial data from a chart.  I can get most of the site but I have a problem when trying to decode the compressed JSON that represents values on the chart - I can see it's the right data by looking at the raw text, but trying to decompress I get the error 'incorrect header check\".' so it's not in the correct GZIP format, though the server indicates the below:\n",
      "\n",
      "  \n",
      "**content-encoding = \"gzip\"**\n",
      "\n",
      "**content-type = \"application/json\"**\n",
      "\n",
      "Any hints for this one - been banging my head against a wall for a day trying different things.\n",
      "\n",
      "Here is some example code:\n",
      "\n",
      "    import { connect } from 'puppeteer-real-browser';\n",
      "    import pako from \"pako\";\n",
      "    \n",
      "    connect({\n",
      "        headless: 'auto',\n",
      "        args: [],\n",
      "        customConfig: {},\n",
      "        skipTarget: [],\n",
      "        fingerprint: false,\n",
      "        turnstile: true,\n",
      "        connectOption: {},\n",
      "        fpconfig: {},\n",
      "    \n",
      "        // proxy:{\n",
      "        //     host:'<proxy-host>',\n",
      "        //     port:'<proxy-port>',\n",
      "        //     username:'<proxy-username>',\n",
      "        //     password:'<proxy-password>'\n",
      "        // }\n",
      "    })\n",
      "        .then(async response => {\n",
      "            const {browser, page} = response\n",
      "    \n",
      "            const urlToMonitor = 'https://io.dexscreener.com/dex/chart/amm/v3/solamm/bars/solana';\n",
      "    \n",
      "    \n",
      "            page.on('response', async (response) => {\n",
      "                if (response.url().startsWith(urlToMonitor)) {\n",
      "                    const encoding = response.headers()['content-encoding'];\n",
      "    \n",
      "                    if (encoding && encoding.toLowerCase() === 'gzip') {\n",
      "                        \n",
      "                        const buffer = await response.buffer();\n",
      "                        const decompressed = pako.inflate(buffer, { to: 'string'});\n",
      "    \n",
      "                        const json = JSON.parse(decompressed.toString());\n",
      "                        console.log(json);\n",
      "                    } else {\n",
      "                        const text = await response.text();\n",
      "                        console.log(text);\n",
      "                    }\n",
      "                }\n",
      "            });\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "            await page.goto('https://dexscreener.com/solana/5o9kgvozarynwfbytzd1wdrkpkkdr6ldpqbuuqm57nfj')\n",
      "            \n",
      "            await page.waitForTimeout(1000)\n",
      "            await page.screenshot({ path: 'page_puppeter.png' });\n",
      "            await page.waitForSelector('#tv-chart-container');\n",
      "            const innerHTML = await page.$eval('#tv-chart-container', element => element.innerHTML);\n",
      "    \n",
      "            console.log('Chart innerHTML:', innerHTML);\n",
      "    \n",
      "            console.log('Chart loaded')\n",
      "    \n",
      "    \n",
      "        })\n",
      "        .catch(error=>{\n",
      "            console.log(error.message)\n",
      "        })\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d5a0f9/how_to_decode_compressed_json_stream_while/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is this book about webscraping worth buying?\n",
      "Text: https://preview.redd.it/byoc475r5u3d1.png?width=459&format=png&auto=webp&s=e2d5ab51828504ca3677b964774f4027b3f290da\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d57nv2/is_this_book_about_webscraping_worth_buying/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do you monitor / analyze bandwidth usage when using scrapy\n",
      "Text: I am relatively new to scrapy. I built a crawler that mostly grabs text from text heavy pages. Most of these pages have no images. If there are, it's 1 small image like a logo.\n",
      "\n",
      "I ran it on one set of urls and my proxy server reported 2GB bandwidth for about 100k pages.\n",
      "\n",
      "That seemed to go well so I increased the concurrency on a second set of urls and then the proxy server reported 20GB for about 50k pages.\n",
      "\n",
      "Unfortunately the proxy server granularity is only by day. Is there some way to figure out what is causing and also reducing the bandwidth spike?\n",
      "\n",
      "Edit:   \n",
      "Solution: I set DOWNLOAD\\_WARNSIZE (to log warnings over a certain size) and DOWNLOAD\\_MAXSIZE (to block over a certain size)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d514te/how_do_you_monitor_analyze_bandwidth_usage_when/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Memory spike when scraping Facebook\n",
      "Text: So, I'm scraping Facebook by continously scrolling and grabbing the posts links. And it works great except that the memory usage keeps increasing and increasing. Even though, I delete old posts and there are never more than 10 or so posts at a time, the ram usage still doesn't decrease and infact it keeps increasing. Any help would be greatly appreciated 🙏.\n",
      "\n",
      "Here's the code:\n",
      "\n",
      "    from selenium.webdriver.common.by import By\n",
      "    from selenium.webdriver.chrome.webdriver import WebDriver\n",
      "    from selenium.webdriver.support.ui import WebDriverWait\n",
      "    from selenium.webdriver.support import expected_conditions as EC\n",
      "    from selenium.webdriver import ActionChains\n",
      "    from time import sleep\n",
      "    from post import scrape_post\n",
      "    from typing import List\n",
      "    \n",
      "    FEED_XPATH = \"//div[@role='feed']\"\n",
      "    TIME_PARENT_XPATH = \".//div[@role='article']/div/div/div/div[1]/div/div[13]/div/div/div[2]/div/div[2]//div[2]/span/span\"\n",
      "    TIME_TOOLTIP_XPATH = \"//div[@role='tooltip']//span\"\n",
      "    SHARE_BTN_XPATH = \".//div[13]/div/div/div[4]/div/div/div/div/div[2]/div/div[3]/div\"\n",
      "    COPY_LINK_BTN_XPATH = \"//div[@role='dialog']//span[text()='Copy link']\"\n",
      "    \n",
      "    def scrape_n_posts(browser: WebDriver, feed: str, n: int, batch_size: int):\n",
      "        browser.get(feed)\n",
      "    \n",
      "        feed_el = browser.find_element(By.XPATH, FEED_XPATH)\n",
      "    \n",
      "        post_class = feed_el.find_elements(By.XPATH, \"*\")[1].get_attribute(\"class\").strip()\n",
      "    \n",
      "        links_count = 0\n",
      "        posts_count = 0\n",
      "        links: List[str] = []\n",
      "    \n",
      "        while links_count < n:\n",
      "            all_posts = feed_el.find_elements(By.XPATH, f\"*[@class='{post_class}']\")\n",
      "            \n",
      "            if posts_count < len(all_posts):\n",
      "                post = all_posts[posts_count]\n",
      "                print(f\"Interacting with post {links_count + 1}...\")\n",
      "                \n",
      "                try:\n",
      "                    time_parent = post.find_element(By.XPATH, TIME_PARENT_XPATH)\n",
      "    \n",
      "                    time_hover = time_parent.find_element(By.XPATH, './/a[@role=\"link\"]')\n",
      "    \n",
      "                    actions = ActionChains(driver=browser)\n",
      "                    actions.click_and_hold(time_hover).perform()\n",
      "                    links.append(time_hover.get_attribute(\"href\").split(\"?\")[0])\n",
      "                    links_count += 1\n",
      "                except Exception as e:\n",
      "                    print(f\"Error interacting with post {posts_count}: {e}\")\n",
      "    \n",
      "                finally:\n",
      "                    browser.execute_script(\"arguments[0].remove();\", post)\n",
      "                    posts_count += 1\n",
      "            else:\n",
      "                print(\"No more posts to interact with. Waiting for more posts to load...\")\n",
      "                browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
      "                sleep(3)\n",
      "                all_posts = feed_el.find_elements(By.XPATH, f\"*[@class='{post_class}']\")\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d55f31/memory_spike_when_scraping_facebook/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Google map data extraction\n",
      "Text: Effortlessly extract valuable data from Google Maps with precision and speed using our advanced A.I. integrated Google Map Scraper tool.\n",
      "\n",
      "Features of this extension:\n",
      "- Information like Name, Place ID, Amenities, Mobile, and many more in few clicks.\n",
      "- Custom API Integration.\n",
      "- Filter or remove duplicate fields.\n",
      "- Automatic scraping without any human intervention.\n",
      "- Supports Hunter API integration for email scraping.\n",
      "- Export in JSON or CSV format.\n",
      "- Easy to use, [explore now](https://addons.mozilla.org/en-US/firefox/addon/geoharvest/)\n",
      "\n",
      "#googlemaps #leadgeneration #googlemapscraper #googlemapscraperfree\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d4v6k6/google_map_data_extraction/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: which sites does it need to scrape to make a scraper like d7 lead finder \n",
      "Text: i need to make a web site scraper/lead finder like d7 lead finder, which gives information like,name,phone, email,web site, linked in, facebook, instargram, twitter account details,reviews etc..so what sites should i scrape in order to get these details.. there are some google map scrapers in github.. but they don't give social media account details..\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d4qg9e/which_sites_does_it_need_to_scrape_to_make_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What is the best AI powered tool that you have used so far ?  Looking for Recommendations!\n",
      "Text: I've been digging into web scraping recently and have been experimenting with some AI-powered tools that leverage LLMs, Langchain, and other advanced methods. So far, I've tried a few different ones, but I'm curious to hear about your experiences and recommendations.\n",
      "\n",
      "Here's a quick sumup of what I've tested:\n",
      "\n",
      "1. **Langchain**: This tool has been pretty impressive in terms of integrating language models for scraping tasks. It feels like it can understand the context better than traditional methods, but I'm still figuring out the best ways to optimize it, learning curve is huge\n",
      "2. **BeautifulSoup + GPT-4/4o**: I combined BeautifulSoup with GPT-4o to see if the language model could help in parsing and understanding the data better. The results were mixed; sometimes it worked like a charm, other times it just failed terribly\n",
      "3. **Scrapy with AI enhancements**: I found a few plugins and scripts that integrate AI models with Scrapy. This setup seems powerful, but I'm still in the early stages of testing it out.\n",
      "\n",
      "Has anyone else tried these or other AI-powered tools for web scraping? What have your experiences been like? Any tips or recommendations for getting the most out of these tools?\n",
      "\n",
      "p/s: i'm not related of any of the tool, just want to look for recommendation\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d4r80o/what_is_the_best_ai_powered_tool_that_you_have/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web scraping competition sites\n",
      "Text: Are there any paid competition sites for web scraping? Seems like a fun way to improve my skills. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d4cx02/web_scraping_competition_sites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping images from Nike\n",
      "Text: Hi all,\n",
      "\n",
      "I'm trying to scrape Nike's site for images only. I don't need metadata at all, so I was hoping I could be lazy and get it done with Httrack or Cyotek WebCopy. Obviously that is not working.\n",
      "\n",
      "The image paths look fairly straightforward, but they aren't being picked up by the scraper. Does this mean that the site is being rendered server side on demand?\n",
      "\n",
      "I can put together a custom scraper in Python, but I would love some tips so that I don't have to start from scratch.\n",
      "\n",
      "Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d4983i/scraping_images_from_nike/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Tricking Google (not just reCaptcha)\n",
      "Text: I'm working on a project that scrapes Google and clicks on a certain result based off a certain query. I'm currently using selenium and using a range of mobile devices & headers, smooth scrolling, etc. It seems to work fine, but I have a feeling Google knows it's a bot. I want to do everything in my power to hide that it's a bot, and do everything I can to make it look like a real mobile device, and not a windows computer using a fake safari user agent.\n",
      "\n",
      "Very keen to hear what people think.\n",
      "\n",
      "I'm interested in playing around a bit more with selenium with stealth add on, [nodriver](https://github.com/ultrafunkamsterdam/nodriver), [pupeteer real browser](https://github.com/zfcsoftware/puppeteer-real-browser), [selenium driverless](https://github.com/kaliiiiiiiiii/Selenium-Driverless), [hrequests](https://github.com/daijro/hrequests) but not convinced any of them will suit my use case.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d41xwi/tricking_google_not_just_recaptcha/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Indeed Remote Job Scraper\n",
      "Text: Hey everyone, i'm trying to figure out how to scrape indeed for remote job posts in my industry, I saw there are some available from other's but cannot verify whether they're legit or potentially have malicious code embedded. They don't have many stars so I don't think the code has been audited by anybody.\n",
      "\n",
      "Can someone help me out with getting a working scraper?\n",
      "\n",
      "These are the ones I found: [https://github.com/israel-dryer/Indeed-Job-Scraper](https://github.com/israel-dryer/Indeed-Job-Scraper) [https://github.com/Eben001/IndeedJobScraper](https://github.com/Eben001/IndeedJobScraper)  \n",
      "[https://github.com/linooohon/creative-coding-jobs-update](https://github.com/linooohon/creative-coding-jobs-update)  \n",
      "[https://github.com/Bunsly/JobSpy](https://github.com/Bunsly/JobSpy)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d3zq43/indeed_remote_job_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Reversing snapchat's GRPC web proto\n",
      "Text: I'm trying to send messages and snaps using snapchat but am having no luck trying to decode the payload being sent, attached below is what it looks like. And in the code block is the payload when i copy it from the HAR file of the webpage.\n",
      "\n",
      "https://preview.redd.it/1kyh05eucd3d1.png?width=512&format=png&auto=webp&s=5f01e72e6f732e416e2f855ab00799e5c6097305\n",
      "\n",
      "    \\u0000\\u0000\\u0000\\u0001\\n\\u0012\\n\\u0010\\n\\u0016�ñ\\u001b/Gt¿úZÃ\\u0003w¾d\\u0010ö¦�ÊË\\u001c\\u001aÓ\\u0001\\n\\u0016\\n\\u0012\\n\\u0010\\u0007ª®�Xï©(~ó6n\\u0010\\b\\u0006·\\u0001*´\\u0001\\n+\\n\\u0005i¨\\u0010\\n\\u001a àZ\\u0012uC'\\u001b\\u0012Îx´ýE@æ¦ýl¹ñã¥8@y@UD\\n+\\n\\u0005Ùà~\\u001d\\u0010\\n\\u001a  +)Îhø\\u0019\\\"¿\\u0002jÄbØ¿jÜXt?\\u0013åyÏ¨\\u0013\\u001c\\\\\\u000b\\n+\\n\\u0005\\u0017tÿÐ\\u0010\\n\\u001a \\u0014ZÁvfM¥³g±Z¤f4noÆÄB\\u0001¿k.ìT\\u0015ß¬â\\u0000#\\n+\\n\\u0005~­æ^\\u0010\\n\\u001a ô8Ë/ß,;ç}\\u001d)NU\\u0015%CÈpð*{þ{e3ë¢UôÏ\\\"x\\u0010\\u0001\\u001aZ*X\\n\\f½¾¯FMk\\\\u¥�\\u0012\\u0010 =æ\\u0016é¨>>\\u0005,±!¿\\u001a!\\u0003ø\\u0004f´ÓQ°óHBÈæ\\u000f³*,Ó¡{\\u0005\\u0007éïâý\\u000e\\u0014§ \\n\\u0001\\u0010ßßK[[\\\"º\\u0017\\u001f¶H`Á\\u0017 \\\"\\u0016ª\\u0012ñæx÷P¤\\u001d¦\\u001aT\\u0016Ï\\u001a\\tÐ~8\\u0002B\\u0014\\n\\u0012\\n\\u0010(\\u0007¢Íø\\rAú§Z¢r\\rf&\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d3cj0g/reversing_snapchats_grpc_web_proto/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help needed\n",
      "Text: Hi all! Im new to scraping and also python, but i want to build a program to help me buy tickets to the olympic games. The site works like this: people can post their tickets for sale, if there is a ticket avalable there is a arrow icon, otherwise its hidden. I want the program to monitor the site then sent a notification to the console when there is a ticket. The problem is i get access denied after the 2. refresh on the website. [LINK](https://ticket-resale.paris2024.org/tickets/all/1/1/18185784)\n",
      "\n",
      "`from selenium import webdriver`\n",
      "\n",
      "`from selenium.webdriver.common.by import By`\n",
      "\n",
      "`from selenium.webdriver.support.ui import WebDriverWait`\n",
      "\n",
      "`from selenium.webdriver.support import expected_conditions as EC`\n",
      "\n",
      "`from time import sleep`\n",
      "\n",
      "`import random`\n",
      "\n",
      "`import time`\n",
      "\n",
      "\n",
      "\n",
      "`options = webdriver.ChromeOptions()`\n",
      "\n",
      "`options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')`\n",
      "\n",
      "`driver = webdriver.Chrome()`\n",
      "\n",
      "\n",
      "\n",
      "`url = 'https://ticket-resale.paris2024.org/tickets/all/1/1/18185784'`\n",
      "\n",
      "`driver.get(url)`\n",
      "\n",
      "\n",
      "\n",
      "`def check_for_icon():`\n",
      "\n",
      "`try:`\n",
      "\n",
      "`icon = driver.find_element(By.XPATH, '//*[@id=\"detailBShowOfferButton-7822432\"]/div/span[2]')`\n",
      "\n",
      "`return icon is not None`\n",
      "\n",
      "`except Exception as e:`\n",
      "\n",
      "`return False`\n",
      "\n",
      "\n",
      "\n",
      "`try:`\n",
      "\n",
      "`while True:`\n",
      "\n",
      "`# Oldal frissítése`\n",
      "\n",
      "`driver.refresh()`\n",
      "\n",
      "\n",
      "\n",
      "`if check_for_icon():`\n",
      "\n",
      "`print(\"Az ikon megjelent az oldalon!\")`\n",
      "\n",
      "`break`\n",
      "\n",
      "\n",
      "\n",
      "`sleep_time = random.randint(60, 120)`\n",
      "\n",
      "`time.sleep(sleep_time)  # Vár egy percet mielőtt újra frissíti az oldalt`\n",
      "\n",
      "`finally:`\n",
      "\n",
      "`pass`  \n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d3gc4r/help_needed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: 403 on request, what am I missing?\n",
      "Text: Been doing webscraping for a while now and I like to think I have a pretty good grasp on it and how to use most of the common libaries. Currently, I have been working on a project where I need to use requests (can't use automated browsing due to the nature of it, needs to be more lightweight) to get a lot of data quickly from a webpage. I have been doing it sucessfully for a while now until recently it seems like some security updates are making it harder. \n",
      "\n",
      "Based on the testing I have done now, I cannot get a single request through for some reason (the contents of the page typically returns the Just a moment... so it seems like its a cloudflare issue/its hitting the cloudflare challenge page). When I access the page via chrome that I regularly use for browsing, I rarely ever get the cloudflare challenge page.\n",
      "\n",
      "What I have tried is to go on my browser, go to the network tab, copy the cURL command from the headers section that is being made to the resource on the page that I want, and integrating all/the most important headers (cookies, referer, user agent, etc...) into my python script that is making the request. Still getting 403s every time!\n",
      "\n",
      "I guess my question is, why, if the headers are identical to my browser, and its coming from a trustworthy ip, do all my requests get hit with a 403? I asked a freelancer and he said it could be because my \"signatures aren't matching\" but I dont really understand what that means exactly in this context or how I would go about fixing that. What other aspects aside from the headers and the information in the nextwork tab that is sent within the request do services like cloudflare look for when verifying a request? I want to get a fundamental understanding of this as opposed to just looking for a libary that band-aids the problem until cloudflare inevitably comes up with a patch...\n",
      "\n",
      "If anyone can help me understand this Ill buy them a coffee!\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d30fd5/403_on_request_what_am_i_missing/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Easy ways to build a complete website around a Python webscraper?\n",
      "Text: I don't have a web developing background so would really appreciate to have some pointers here! I wrote a simple web scraping script using Selenium and would like to learn how to build a fully functioning website around it (allows user accounts, saves users' search history, can run ads, can process payments for subscriptions etc). How and where should I start?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d2t6im/easy_ways_to_build_a_complete_website_around_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Both proxy/no proxy work locally but nothing works on cloud server (Python)\n",
      "Text: EDIT:\n",
      "\n",
      "I solved this by putting a NAT Gateway in front of my server so it goes out of its static IP instead of a dynamic public IP of the cloud.\n",
      "\n",
      "\\_\\_\\_\\_  \n",
      "  \n",
      "Hey, a web scraping noob here.\n",
      "\n",
      "I have a scraper for an e-commerce website. As the title says, I don't know what is it about my request that the website recognizes.\n",
      "\n",
      "Locally, every single proxy and non-proxy request I make to that site works. They don't even restrict my local IP. However on my cloud machine, I tried multiple proxies from countless sources, both free and not free, residential, mobile, different regions, etc. No matter what, the cloud server gets 403 when using those. If I use them on my local machine, it works as usual.\n",
      "\n",
      "I know there must be something trivial about the fact that my request is from a local machine as opposed to a cloud machine, but I don't know how to fix it. It seems like a common problem. Does anybody know?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d2z8qo/both_proxyno_proxy_work_locally_but_nothing_works/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can Web Scraping with Selenium and WebDriver Be Done on Google Colab?\n",
      "Text: I'm having a doubt about doing web scraping using Selenium and WebDriver on Google Colab. Is Colab no longer allowing web scraping? If so, can anyone provide me with a code example? I've been trying to find a solution for my project for days, but I haven't been able to get any results\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d2xnjx/can_web_scraping_with_selenium_and_webdriver_be/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best way to find all labs in the US who test food items?\n",
      "Text: Hello, I'm looking for ideas on how to identify laboratories in the US who test food items? I know some of them can be found on the FDA website but I'm sure there are many more. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d2urif/best_way_to_find_all_labs_in_the_us_who_test_food/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape paginated pages\n",
      "Text: I am working on a crm scraper the issue is crm provider server is quite slow to load new pages around 10-20s sometimes on 500 pages. The link in the browser doesn’t change when i go to next page . Is there any efficient and fast way to do this paginated scraping\n",
      "\n",
      "Im using selenium and pandas\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d2cmee/scrape_paginated_pages/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Cloudflare (and similar solutions) blocking concerns vs. building a SEO + Search solution.\n",
      "Text: I'm working on a solution that is essentially providing backlink stats / SEO + Search, the former being most important. There are other smaller use cases / tools but these two are the primary.\n",
      "\n",
      "Side note we aim at the budget zone in case you wonder. Not building the next Ahrefs. But still, it's a large bot traffic volume monthly.\n",
      "\n",
      "The issue we have is obviously Cloudflare (and similar) = not having access. \n",
      "\n",
      "I know we can submit request to get access for our bots. We do obey robots.txt properly etc and planning to stay always compliant on the good side of things like a professional would do.\n",
      "\n",
      "The problem is still the control CF has over this aspect and the unilateral decisions on which you have no control. \n",
      "\n",
      "One day you might get banned (for whatever reason) and voila' - no longer having access. Which means you're toast. Your business can be crippled or erased an you have no control over it. ( Been in a somewhat similar spot in the past - got sites penalized by Google, guess many of us know what that means... anyway)\n",
      "\n",
      "The bot volume overall is quite high, as you can imagine while the usage of the data is pretty basic - as described. We extract links and index textual content for search.\n",
      "\n",
      "What would you recommend in this case? How to handle the CF \"locked gate\" issue? We are not planning to do a permanent battle to circumvent the protection, that doesn't make sense for us from several different reasons.\n",
      "\n",
      "*Mitigation: For now the only approach we have is combining our own bots with data from commoncrawl for example. \n",
      "\n",
      "Issue being, depending on release date it can be up to 2 months stale for certain websites (those protected by CF). We can however show fresh links to those sites, but the stale part is the outbound links and content from those sites.*\n",
      "\n",
      "So - what do you recommend? Is there another way to go by that I'm unaware of?\n",
      "\n",
      "TIA for any advice!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d1sfxu/cloudflare_and_similar_solutions_blocking/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping dynamic content with dynamic unique IDs\n",
      "Text: Hello!  \n",
      "I want to start by saying that I am not good at coding and I am beginner with webscraping. \n",
      "\n",
      "I am trying to webscrape e-shop and I have run into a problem that some of the info is hidden behind a dropdown. I use Selenium to find this dropdown element and click on it and select a value. After that, all the necessary info can be accessed with BeautifulSoup. \n",
      "\n",
      "My problem is that when there are multiple dropdowns, Selenium only clicks a value on the first dropdown. I use a line to identify all dropdowns:  \n",
      "\n",
      "\n",
      "            elements = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"select2-selection__rendered\")))\n",
      "            first_element = elements[0]\n",
      "            second_element = elements[1]\n",
      "    \n",
      "\n",
      "For each element then I perform the following ActionChains:\n",
      "\n",
      "            button = wait.until(EC.element_to_be_clickable((first_element)))\n",
      "            button.click()\n",
      "            time.sleep(0.1)\n",
      "            action = webdriver.common.action_chains.ActionChains(driver)\n",
      "            action.move_to_element_with_offset(first_element,0,70)\n",
      "            action.click()\n",
      "            action.perform()\n",
      "    \n",
      "\n",
      "However, when I substitute first\\_element for second\\_element, no value is selected.   \n",
      "There might now be question why I use ''move\\_to\\_element\\_with\\_offset''. Once Selenium clicks on dropdown, there appears options that have dynamic class (it changes from selectable to highlighted depending on mouse position). And each option has unique ID, so I do not see an option to find an element, instead I just move mouse 70px down to select the first option.\n",
      "\n",
      "I have also tried send\\_keys(Keys.ARROW\\_DOWN) followed by send\\_keys(Keys.ENTER) but that didn't select anything either.\n",
      "\n",
      "An example of the HTML that contains the value I am trying to select:\n",
      "\n",
      "<li class=\"select2-results\\_\\_option select2-results\\_\\_option--selectable select2-results\\_\\_option--selected\" role=\"option\" data-select2-id=\"select2-data-40-h95h\" aria-selected=\"false\"> --- Please Select --- </li>  \n",
      "<li class=\"select2-results\\_\\_option select2-results\\_\\_option--selectable select2-results\\_\\_option--highlighted\" id=\"select2-input-option8576807-result-fpp5-29554313\" role=\"option\" data-select2-id=\"select2-data-select2-input-option8576807-result-fpp5-29554313\" aria-selected=\"true\">5,75</li>\n",
      "\n",
      "So I would like to click on the second element that has '5,75' value. In this case it is approximately 70px down from the dropdown.\n",
      "\n",
      "Are there any errors in my code? Why can't I get the value of second\\_element, but it always succeed with first\\_element? Are there easier or more elegant ways to get to this value?\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d1rw68/scraping_dynamic_content_with_dynamic_unique_ids/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Easy way of scraping a react based website \n",
      "Text: Hi folks I am having trouble scraping the data from react based websites bs4 and other scrapping tools do not work as the data that is coming is not compiled . I tried using chromium drivers but take so much time on one request and face a lot of trouble running the script on server is their any library or tool you guys can recommend that can easily scrap the Client side rendered websites \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d1br7z/easy_way_of_scraping_a_react_based_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrap page that does not finish loading\n",
      "Text: Hi all. I've been scrapping for a while a website that worked perfectly fine.  \n",
      "They recently did some changes and I've been struggling because the `webdriver.get()` of selenium gets \"*stuck*\".\n",
      "\n",
      "It's not a Selenium problem; the page does not finish loading on a normal Chrome browser either. Chrome just shows a spinner on the favicon.\n",
      "\n",
      "Do you know any way of setting a timeout (e.g. 10 secs) and try to read elements from the page even if it did not finish loading?\n",
      "\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d1c4or/scrap_page_that_does_not_finish_loading/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How would I scrape articles from a website like CNN news network that changes daily\n",
      "Text: Hi, I have worked on a few simple scraping projects but all of them have been relatively simple and have scraped them from a static website. I am working on a small project that involves scraping these news articles but since the site updates so many times I am not sure what approach should i take to this. Any help would be much appreciated. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1d04gt2/how_would_i_scrape_articles_from_a_website_like/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Whats the hardest thing about web scraping?\n",
      "Text: Title. Curious what the biggest challenges everyone encounters while scraping\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1czrxas/whats_the_hardest_thing_about_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How Web Scraping Solves Your Business Problems?\n",
      "Text: For business owners and individuals with no coding skills. What problems did you face or maybe facing? And you resort to web scraping for the solution. Could you share your experience how web scraping benefits your business?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1czmzba/how_web_scraping_solves_your_business_problems/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Insta DM bot\n",
      "Text: Hello guys im doing a test for an insta dm bot for school but ive had some problems with my code as im not advanced in python? Any helpers willing to comment?\n",
      "\n",
      "(Editing post for code)\n",
      "\n",
      "from selenium import webdriver\n",
      "from webdriver_manager.chrome import ChromeDriverManager as CM\n",
      "from selenium.webdriver.common.by import By\n",
      "import time\n",
      "\n",
      "driver = webdriver.Chrome(\n",
      "    executable_path=CM().install())\n",
      "driver.set_window_position(0, 0)\n",
      "driver.set_window_size(414, 936)\n",
      "driver.get('https://www.instagram.com')\n",
      "\n",
      "time.sleep(5)\n",
      "\n",
      "driver.find_element_by_name('username').send_keys('')\n",
      "driver.find_element_by_name('password').send_keys('')\n",
      "driver.find_element_by_xpath('/html/body/div[2]/div/div/div[2]/div/div/div[1]/section/main/article/div[2]/div[1]/div[2]/form/div/div[3]').click()\n",
      "\n",
      "time.sleep(10)\n",
      "\n",
      "driver.find_element_by_xpath('/html/body/div[2]/div/div/div[2]/div/div/div[1]/div[1]/div[2]/section/main/div/div/div/div/div').click()\n",
      "\n",
      "time.sleep(6)\n",
      "\n",
      "driver.find_element_by_xpath('/html/body/div[7]/div[1]/div/div[2]/div/div/div/div/div[2]/div/div/div[3]/button[2]').click()\n",
      "\n",
      "time.sleep(6)\n",
      "\n",
      "accounts = [\"\", \"\", \"\"]\n",
      "\n",
      "for account in accounts:\n",
      "    driver.find_element(By.CSS_SELECTOR, \"a[href='/direct/inbox/']\").click()\n",
      "\n",
      "    time.sleep(4)\n",
      "    driver.find_element_by_xpath('/html/body/div[2]/div/div/div[2]/div/div/div[1]/div[1]/div[2]/section/div/div/div/div[1]/div/div[2]/div/div/div/div[4]/div').click()\n",
      "\n",
      "    time.sleep(4)\n",
      "    driver.find_element_by_xpath('/html/body/div[7]/div[1]/div/div[2]/div/div/div/div/div/div/div[1]/div/div[2]/div/div[2]/input').send_keys(account)\n",
      "    time.sleep(4)\n",
      "    driver.find_element_by_xpath('/html/body/div[7]/div[1]/div/div[2]/div/div/div/div/div/div/div[1]/div/div[3]/div/div/div/div[1]/div/div/div[2]/div/div').click()\n",
      "    time.sleep(4)\n",
      "    driver.find_element_by_xpath('/html/body/div[7]/div[1]/div/div[2]/div/div/div/div/div/div/div[1]/div/div[4]/div').click()\n",
      "    time.sleep(4)\n",
      "    message_input_field = driver.find_elements(By.XPATH, \"//textarea[@placeholder='Message...']\")\n",
      "    if message_input_field:\n",
      "        message_input_field[0].send_keys('Hello ')\n",
      "        time.sleep(4)\n",
      "        driver.find_element_by_xpath('/html/body/div[2]/div/div/div[2]/div/div/div[1]/div[1]/div[2]/section/div/div/div/div[1]/div/div[2]/div/div/div/div/div/div/div[2]/div/div/div[2]/div/div/div[3]').click()\n",
      "    else:\n",
      "        print(f\"Message already sent to {account}. Moving to the next account.\")\n",
      "        time.sleep(5)\n",
      "    driver.get('https://www.instagram.com')\n",
      "    time.sleep(60)  # Wait for 1 minute before sending the next message\n",
      "\n",
      "time.sleep(4)\n",
      "driver.quit()\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1czv2qe/insta_dm_bot/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How scrape Instagram posts published in the last X period, e.g. 7 days?\n",
      "Text: Hey everyone!\n",
      "\n",
      "I'm trying to find a way to obtain Instagram posts linked to specific hashtags published in the last \"X\" period, e.g. 7 days.\n",
      "\n",
      "I tried 3rd party tools like Phantombuster or Apify but they lack a way to filter the data based on period. I don't mind giving it a go to develop something on my own. \n",
      "\n",
      "Do you have any suggestions on how to go about it? \n",
      "\n",
      "I'm also wondering if it's possible to scrape posts based on keywords within the caption or Instagram profile rather than hashtags. \n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1czlibz/how_scrape_instagram_posts_published_in_the_last/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping strategy\n",
      "Text: Hello guys! \n",
      "\n",
      "I'm scraping throttled marketplace internal api (read amazon). It has around ~10M items, split by categories and every request is throttled by 4 seconds (otherwise IP ban). \n",
      "\n",
      "The goal is to get price updates for each item daily. At this time I drill through categories with variable page length e.g. get 40 pages in sunglasses category which results in 800 item updates.\n",
      "\n",
      "But I get a lot of items outdated, as category positions are moving and one item moved from page 39 to 42 does not get updates anymore.\n",
      "\n",
      "How do you solve this problem? Getting 500 proxies and requesting each item separately?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1czdmkb/scraping_strategy/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: update the chatgptautomation lib\n",
      "Text: Hi\n",
      "\n",
      "My library had a problem due to the OpenAI update, the problems were solved and now you can use it for GPT4o\n",
      "\n",
      "[Github ChatGPTAutomation](https://github.com/iamseyedalipro/ChatGPTAutomation)\n",
      "\n",
      "[Pypi ChatGPTAutomation](https://pypi.org/project/ChatGPTAutomation/)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1czlcbe/update_the_chatgptautomation_lib/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking to decrypt a JSON used to render a chart\n",
      "Text: Hello everyone, first post here! I'm looking to download the data that is drawn on the chart on the right hand side of this page: [https://live.euronext.com/en/product/equities/NL0012969182-XAMS](https://live.euronext.com/en/product/equities/NL0012969182-XAMS)\n",
      "\n",
      "I was able to find the location of the data by using the developer tools. The URL of the data is: [https://live.euronext.com/en/intraday\\_chart/getChartData/NL0012969182-XAMS/intraday](https://live.euronext.com/en/intraday_chart/getChartData/NL0012969182-XAMS/intraday)\n",
      "\n",
      "However this returns a JSON with encrypted data. I explored further the developer tools so as to try and find how the browser decrypts the data when it renders the chart, but I'm a bit stuck at this stage...\n",
      "\n",
      "What should the next step be? Any help appreciated!\n",
      "\n",
      "  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cz4ehg/looking_to_decrypt_a_json_used_to_render_a_chart/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help me find this XPath?\n",
      "Text: Hey. I'm going crazy trying to find find the XPath of this 'Next' Button on LinkedIn. I had one that (I think) failed because it's being dynamically generated. I installed an extension called 'SelectosHub,' that seems to help find XPaths. But I think I'm still missing it. Feels like such a boneheaded problem. What would you use? Thanks in advance.\n",
      "\n",
      "https://preview.redd.it/1gqnenzxl82d1.png?width=1932&format=png&auto=webp&s=e1b7fa86ab6feff002b1fdab8124f48e0060e3a5\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cz31mu/help_me_find_this_xpath/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: A Public Challenge: Harvesting the X Hidden APIs\n",
      "Text: Hey folks,\n",
      "\n",
      "I've been having a bit of fun over the past day or so poking at hidden APIs.  Part of the cause of doing so was because I wanted to scrape comments from a Tobacco website and that was the seemingly quickest way to do so.  Worked like a charm.    \n",
      "  \n",
      "For the initial step of learning, [this external blog post was incredibly useful](https://aatt.io/newsletters/how-to-find-and-use-hidden-apis-to-automate-processes), although [Postman](https://www.postman.com/) was a far superior mid-step than what they recommended (LLMs still kinda blow for this sort of thing, I tried ChatGPT again and was still disappointed).    \n",
      "  \n",
      "Inspect Network for Fetch/XHR with Preserve Log and Disable cache on, reload page, find content, copy request, put into postman, export as request in language of preference, process response to get what you want.  Pretty straightforward, if partially manual, workflow.\n",
      "\n",
      "So that working out for my tobacco interest got me thinking about more general applications, so I tried poking around at X.  I've gotten some payloads out that I need to process, and the general technique works in the sense that I can get payloads of Tweets out, but a few questions occur to me at this hour (it's literally 4:20 AM):\n",
      "\n",
      "1. What is the extent to which Tweets may be harvested without login?  I.e., open an \"Incognito\" tab and go to an X page.  What can be collected using the hidden API method?  With a few minutes of work, I can get SOME for sure, but I'm still parsing the reply.\n",
      "\n",
      "2. What is the extent to which Tweets may be harvested with basic login?  \n",
      "  \n",
      "3. What is the extent to which Tweets may be harvested with Blue login?\n",
      "\n",
      "4. What is the best way to automate grabbing the hidden API details?\n",
      "\n",
      "I personally thing #4 is the most interesting - how can I reliably automate that workflow, for Tobacco reviews, Tweets, etc??? - but I think 1-3 are practically relevant given the questions asked here about Twitter/X and also of pure interest.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cynr40/a_public_challenge_harvesting_the_x_hidden_apis/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Anybody effectively scraping yelp?\n",
      "Text: I have a custom script, written in node, that works very well and used to be great. My issue lately is that yelp has been blocking all my proxies heavily. I use usa datacenter rotating proxies with BrightData. Any other suggestions to not get blocked? I’m scraping using just http requests \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cyyzob/anybody_effectively_scraping_yelp/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Crawler blocked by Cloudflare despite respecting robots.txt and sending one request per day\n",
      "Text: Hi,\n",
      "\n",
      "There is a website on which I want to scrape one page, but I don't want to play catch-up. Hence I first checked the robots.txt, and the page I'm interested in is allowed.\n",
      "\n",
      "Still, I see that my crawler is blocked by some Cloudflare protection after some days, despite sending one request \"per day\"!\n",
      "\n",
      "Is there something I am doing wrong, or it is just how it is? Will it help if I change the job schedule to one per week? :-|\n",
      "\n",
      "P.S. And I really don't want to appear on [Cloudflare naughty list](https://news.ycombinator.com/item?id=32912075)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cylo52/crawler_blocked_by_cloudflare_despite_respecting/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selenium script to capture console logs - help 🙏 \n",
      "Text: hi,  \n",
      "i'm currently working on a project involving capturing the console logs of a website as they happen in real time. For now, I'm using Youtube as an example and printing the logs to terminal. The starter code i have so far only seems to be printing warnings and errors regarding permission issues and third-party cookies. anyone know a quick fix for this, or if the task is even possible?\n",
      "\n",
      "    from selenium import webdriver\n",
      "    from selenium.webdriver.chrome.service import Service\n",
      "    from selenium.webdriver.chrome.options import Options\n",
      "    from webdriver_manager.chrome import ChromeDriverManager\n",
      "    import time\n",
      "    \n",
      "    options = Options()\n",
      "    \n",
      "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
      "    options.add_argument(f\"user-agent={user_agent}\")\n",
      "    \n",
      "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
      "    options.add_argument('--disable-infobars')\n",
      "    options.add_argument('--disable-popup-blocking')\n",
      "    \n",
      "    options.set_capability(\"goog:loggingPrefs\", {\"browser\": \"ALL\"})\n",
      "    \n",
      "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
      "    \n",
      "    driver.get('https://www.youtube.com/')\n",
      "    \n",
      "    time.sleep(10)\n",
      "    print(\"waiting done!\")\n",
      "    \n",
      "    try:\n",
      "        while True:\n",
      "            time.sleep(5)\n",
      "            logs = driver.get_log('browser')\n",
      "            for entry in logs:\n",
      "                print(entry)\n",
      "    except KeyboardInterrupt:\n",
      "        print(\"Exiting...\")\n",
      "    finally:\n",
      "        driver.quit()\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cylqle/selenium_script_to_capture_console_logs_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Bypassing bot recognition with GeeTest\n",
      "Text: Hey,\n",
      "any chance to bypass this screen or avoid it? I’m using BrightData residential proxies so the IPs SHOULD be clean. \n",
      "\n",
      "I’ll post my code in the comment, it works on the German version of the site. \n",
      "\n",
      "Thanks for any replies!\n",
      "\n",
      "\n",
      "URL: https://i.redd.it/tnez564cb22d1.jpeg\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Avoid being detected \n",
      "Text: Hi,\n",
      "I scrap data from website that is protected by a Datadome.\n",
      "Theoretical I'm successful I can download data from this site(using headers, Proxy and stealth version of Chromedriver) but the next time this IP is being banned. I'm losing a lot of IP's by that and scraping is expensive. I can't say exactly which IP are banned which not because I'm using rotating proxy. But the at the beginning 1/10 attempt was blocked now it's like 1/10 attempts are passing. I just starting this Script to run so I downloaded only 1 site at a time. So I don't think that I'm spamming to much.\n",
      "\n",
      "I tried to use catcha solver but I get the info back that the IP Is banned in Datadome. Are the only available way is to buy 50k residential proxies?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cy271j/avoid_being_detected/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape a page that returns \"Max retries exceeded with url\"?\n",
      "Text: I am trying to save [this page](https://edge.nhl.com/en/skater/20232024-regular-8480188).\n",
      "\n",
      "I tried a few different mechanisms to download, but I got the furthest with pywebcopy save\\_webpage in Python 3.7:\n",
      "\n",
      "    url = 'https://edge.nhl.com/en/skater/20232024-regular-8480188'\n",
      "    download_folder = '/Users/myname/Downloads/'\n",
      "    \n",
      "    kwargs = {'bypass_robots': True}\n",
      "    \n",
      "    save_webpage(url, download_folder, **kwargs) \n",
      "\n",
      "This page calls wss://edge.nhl.com/en/skater/20232024-regular-8480188, and it keeps pinging the server indefinitely.\n",
      "\n",
      "I get the following error:\n",
      "\n",
      "  \n",
      "requests.exceptions.SSLError: HTTPSConnectionPool(host='edge.nhl.com', port=443): Max retries exceeded with url: /en/skater/20232024-regular-8480188 (Caused by SSLError(SSLCertVerificationError(1, '\\[SSL: CERTIFICATE\\_VERIFY\\_FAILED\\] certificate verify failed: self signed certificate in certificate chain (\\_ssl.c:1091)')))\n",
      "\n",
      "Is this really a certificate issue? Or is it caused by the WSS traffic not completing?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cy2weu/how_to_scrape_a_page_that_returns_max_retries/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Email Acquiring Scraping \n",
      "Text: Hi,\n",
      "\n",
      "Just learned about web scraping and have been testing strings in Python with Jupyter notebook but hit a few pitfalls to mine the data. I am looking to pull emails from websites relating to food. I keep hitting robots.txt files that basically block any data to pull from the site I’m trying. Would anyone have any work around code or is robots.txt impenetrable?\n",
      "\n",
      "Would love any advice or sample code to learn more.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cy1gfk/email_acquiring_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ads scraping\n",
      "Text: Hey guys,\n",
      "\n",
      "I was wondering is there any tools that scrape whos running ads for certain search terms? E.G roofers in Miami\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cxuyex/ads_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Quizlet\n",
      "Text: I want to create a function that will take a Quizlet flashcard set URL and will respond with a list of key value pairs of the form `term : definition`. I want to deploy this as an API in the cloud.\n",
      "\n",
      "I've tried using a simple beautiful soup scraper and using selenium. I got it to work locally using selenium in non-headless mode, but it doesn't work in headless mode. It's a bit tricky to run it in non-headless mode inside a Docker container running on AWS Lambda. I am planning on trying to set that up, but I want to know if anybody has some advice on how to get this to work. I know that [knowt.com](http://knowt.com) does this, but I don't know how they do it.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cxpx4s/quizlet/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do you guys use scraping?\n",
      "Text: I’m applying for a position in a web scraping company and as I’m new in the field, I would like to better understand a typical user.\n",
      "If you can answer these 3 short questions, it would help me a big time.\n",
      "\n",
      "What is your current job position?\n",
      "What scraping tool are you using?\n",
      "How are you using the results of the scraping?\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cxs7sb/how_do_you_guys_use_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to use R to extract urls with the same pattern across multple sites simultaneously?\n",
      "Text: I am trying to download videos from a site, which requires:\n",
      "\n",
      "1. manually identifying pages of specific videos. I'm not trying to scrape the entire domain and get the download url for every video.\n",
      "2. using R to extract the download url (with \"get\\_file\" in the url) that resides on each video url. One issue is the download url (ie, with \"get\\_file\" in it) only becomes available if the account is signed in.\n",
      "\n",
      "Example: video url: https://www. example.com/video/\\[string1\\] download url (residing on the video page): https://www. example.com/get\\_file/\\[string2\\]\n",
      "\n",
      "In short, I want the R code to read a list of video urls I manually compile, then produce a list of download urls (e.g., with \"get\\_file\" in it).\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cxklnh/how_to_use_r_to_extract_urls_with_the_same/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help with web scraping (Webscraper)\n",
      "Text: Good evening everyone, I need some help please.  I'm trying to extract emails from a website (with Web Scraper), the problem is that when I launch a search I don't get a complete url to use to creating a Sitemap,, but only a #results-anchor-1 is added for example.  Specifically, I would need to extract all the emails that appear from this tool, for example from the city of Milan, on the 20145 postcode. How can I do this please?  I also tried other Chrome extensions for automatic email recovery from page, but they don't detect anything, it's as if there was protection in the backend\n",
      "\n",
      "Link: https://www.odcec.mi.it/cerca-iscritti\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cxevd1/help_with_web_scraping_webscraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is selling scraped content illegal?\n",
      "Text: Me and the website is from EU.\n",
      "\n",
      "Let's say I scrape all the data I can get from a huge website ( let's call it Y) to where people can list items up for sale. Then I save this data to my database. Then I sell this data to multiple other companies by offering a paid API. The website Y also offers their own official API but with a huge price. I plan to ask for much less.\n",
      "\n",
      "Is this legal? ChatGPT said no. Any way to make it legal? \n",
      "\n",
      "\n",
      "EDIT: Changed X to Y\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cwrvug/is_selling_scraped_content_illegal/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to become better at web scarping?\n",
      "Text: As the title says. I use python for web scraping, I use selenium, beautiful soup and requests.\n",
      "\n",
      "I am a freelancer on Upwork. All of the jobs I take are the easy ones that require only a one-time scrape and be done. I see a lot of other jobs that require a web scraper, but are more complex, like scraping a website with hundreds of pages daily or scrape websites that have good security.\n",
      "\n",
      "I have done a lot of scraping jobs but I still consider myself as an amateur, and I wanna be a pro.\n",
      "\n",
      "What do you guys suggest I do? Thanks!\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cwzmjx/how_to_become_better_at_web_scarping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: OneFC & Bellator detailed fight statistics\n",
      "Text: Hey guys, do you know a place where I can find more details of previous (hopefully recent as well) fights of onefc? I found one dataset online but it's with pretty old data (April 2021 as far as I remember) and I need more recent fight stats. OneFC keeps only minimal fight stats, was hoping something similar to [ufcstats.com](http://ufcstats.com) data. Tapology and Sherdog only show the bare minimum fight details as well, so I'm looking for any other great sources.  \n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cx89j8/onefc_bellator_detailed_fight_statistics/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Find company details (Industry, Size, etc.) having URLs\n",
      "Text: I have a list of companies' URLs. What's the best way to get a company description, primarily industry and employee count? Thanks in advance for any advice.   \n",
      "\n",
      "\n",
      "  \n",
      " \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cx6bu4/find_company_details_industry_size_etc_having_urls/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trouble with Web Scraping: Getting Blocked\n",
      "Text: I'm using Node.js with Puppeteer for web scraping on LinkedIn Navigator. However, I'm getting blocked after scraping around 30 to 40 pages. My goal is to scrape at least 80-100 pages. I've also built a front-end solution where I manipulate the DOM, take the URL, loop through the DOM elements, and handle pagination. To make it appear more human-like, I slowed down the DOM manipulation process. However, I still face the same blocking issue after around 35-40 pages. I've tried using proxies as well, but the issue persists in Node.js. Additionally, most web scraping API and SDK solutions, such as Zenscrape, ScraperAPI, and many others, don't support LinkedIn Navigator. Does anyone have any tips or strategies to prevent getting blocked? Any advice on how to achieve this goal would be greatly appreciated. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cx1iew/trouble_with_web_scraping_getting_blocked/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Advice on how to get mi first freelance job?\n",
      "Text: Hi! I work as a full time web scraper but Im not having success on getting my first Upwork client! I’m making a lot of bids but still no luck.\n",
      "Does anyone have any tips or advice using Upwork or another freelance platform on how to get my first job?\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cx05y9/advice_on_how_to_get_mi_first_freelance_job/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can i be alerted when a website changes, specifically when something is added not removed?\n",
      "Text: Hello, i'm a complete novice here and i cannot code but i'm learning! I have this website i want to scrape data. This website is kinda like a pinboard i'd say, very simple just boxes that are added or removed if it has been booked by a user and there are no more spaces available (so a box can have more than one space available).\n",
      "\n",
      "  \n",
      "I want to track when a box is added, not a space. Are there any free extensions i can use on chrome to do this, and if so how?\n",
      "\n",
      "  \n",
      "Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cwozly/how_can_i_be_alerted_when_a_website_changes/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping name of class\n",
      "Text: Hi,\n",
      "\n",
      "I am trying to scrape pollen data from weather website to use with home assistant.\n",
      "\n",
      "The level of pollen is displayed as graphic, but I noticed that it is actually an I class, where different names indicate levels of pollon, ie. <i class=\"dots dots--lvl-2\"></i>. Is it possible to scrape that value of i?\n",
      "\n",
      "  \n",
      "If I do select for the parent span class, and use apropriate value template, will I be able to pull the value of 2 from the i class?\n",
      "\n",
      "https://preview.redd.it/bw5zrvz2xk1d1.png?width=414&format=png&auto=webp&s=e67e11151b8027046a3a373a5da40865c31b9c3f\n",
      "\n",
      "  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cwey3q/scraping_name_of_class/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Proxy Usage Calculator\n",
      "Text: \n",
      "URL: https://www.gotdetected.com/proxy-usage-calculator/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping graph from companiesmarketcap.com\n",
      "Text: I'm trying to scrape the data from the graph on for example [https://companiesmarketcap.com/microsoft/marketcap/](https://companiesmarketcap.com/microsoft/marketcap/), but I can't figure out how. Anybody who can help figure it out?\n",
      "\n",
      "Want to have it into a sheet finally.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cwa4qp/scraping_graph_from_companiesmarketcapcom/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Map can't be inspected. The rest of the page can be inspected, but map disappeared.\n",
      "Text: [this](https://propertyinformationportal.nyc.gov/) is the page I'm trying to scrape.\n",
      "\n",
      "This is a tax map of NYC. I'm trying to gather a few information for these properties and analyze. Typically, I would just go to the property on the map and select it and I can see the information. Now that I'm trying to scrape it. I was trying to inspect the map to get all the properties, when I right click, nothing came up. However, if I click outside the map, I was allowed to select \"Inspect\". The problem is, when I selected \"Inspect\", the map disappeared.\n",
      "\n",
      "Could you please advice me on how I can scrape all in formation on this page?\n",
      "\n",
      "There are some other sites I worked with where the page get the data from a csv file. I'm not exactly sure if this is the same. If I could just get the data without writing a crawler would be perfect.\n",
      "\n",
      "  \n",
      "Thank you!!!!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cw8guj/map_cant_be_inspected_the_rest_of_the_page_can_be/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need working github repo for Facebook scraping\n",
      "Text: Am new to scraping and trying to scrape social media pages for post comments and likes. My focus now is on Facebook. Can anyone share free github repo, I can use. I would be most grateful\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cwb30h/need_working_github_repo_for_facebook_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Why is this scraping a different amount of info every time. Also, companyName and jobLocation are always shorter than jobTitles and links\n",
      "Text:     I'm not sure why this is happeneing. Even if there is a way to just add a blank entry when it can't pull the info would be great.\n",
      "    \n",
      "    \n",
      "    import hrequests\n",
      "    from bs4 import BeautifulSoup\n",
      "    import pandas as pd\n",
      "    import csv\n",
      "    \n",
      "    ####################\n",
      "    ## INDEED SCRAPER ##\n",
      "    ####################\n",
      "    \n",
      "    # Creates the CSV in Write Mode\n",
      "    with open('Indeed_Jobs.csv', 'w', newline='') as file:\n",
      "        writer = csv.writer(file)\n",
      "    \n",
      "    #Creating Lists for info\n",
      "    jobTitles = []\n",
      "    companyName = []\n",
      "    jobLocation = []\n",
      "    links = []\n",
      "    \n",
      "    #Creates list of URLs\n",
      "    URLs = [\"https://www.indeed.com/jobs?q=IT+Entry+Level&l=United+States&from=searchOnHP&vjk=55e3c7e5e7a919c9\", \n",
      "            \"https://www.indeed.com/jobs?q=development+entry+level&l=United+States&from=searchOnDesktopSerp&vjk=996b270bd119f225\",\n",
      "            \"https://www.indeed.com/jobs?q=user+experience+entry+level&l=United+States&from=searchOnDesktopSerp&vjk=4fc6a443fd7c11df\",\n",
      "            \"https://www.indeed.com/jobs?q=ux%2Fui+experience+entry+level&l=United+States&from=searchOnDesktopSerp&vjk=254dcd8c33926527\",\n",
      "            \"https://www.indeed.com/jobs?q=data+entry+level&l=United+States&from=searchOnDesktopSerp&vjk=bd4fd45f0feb91c2\"]\n",
      "    \n",
      "    for url in URLs:\n",
      "    \n",
      "        #Connecting to Indeed and reading HTML\n",
      "        target_url = url\n",
      "        head= {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\",\n",
      "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
      "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
      "            \"Connection\": \"keep-alive\",\n",
      "            \"Accept-Language\": \"en-US,en;q=0.9,lt;q=0.8,et;q=0.7,de;q=0.6\",\n",
      "        }\n",
      "        resp = hrequests.get(target_url, headers=head)\n",
      "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
      "    \n",
      "        #Finds all items in list\n",
      "        for div in soup.find_all('div', {'class': 'css-dekpa e37uo190'}):\n",
      "            #Pulls job title\n",
      "            for span in div.find_all('span'):\n",
      "                jobTitles.append(span.text.strip())\n",
      "            #Pulls link to job\n",
      "            for a in div.find_all('a'):\n",
      "                links.append(\"indeed.com\" + a.get('href'))\n",
      "            #Pulls company name\n",
      "        for div in soup.find_all('div', {'class': 'company_location css-17fky0v e37uo190'}):\n",
      "            for span in div.find_all('span',{'data-testid': 'company-name'}):\n",
      "                companyName.append(span.text.strip())\n",
      "            #Pulls job location\n",
      "            for divLocation in div.find_all('div', {'data-testid': 'text-location'}):\n",
      "                jobLocation.append(divLocation.text.strip())\n",
      "    \n",
      "    \n",
      "        # Changing the heading on the CSV depending on which URL was searched\n",
      "        if url == \"https://www.indeed.com/jobs?q=development+entry+level&l=United+States&from=searchOnDesktopSerp&vjk=996b270bd119f225\":\n",
      "            additional_row = [\"Development - Entry Level\"]\n",
      "        elif url ==\"https://www.indeed.com/jobs?q=it+entry+level&l=United+States&from=searchOnHP&vjk=bf9a1055e66b240c\":\n",
      "            additional_row = [\"IT - Entry Level\"]\n",
      "        elif url == \"https://www.indeed.com/jobs?q=user+experience+entry+level&l=United+States&from=searchOnDesktopSerp&vjk=4fc6a443fd7c11df\":\n",
      "            additional_row = [\"User Experience - Entry Level\"]\n",
      "        elif url == \"https://www.indeed.com/jobs?q=ux%2Fui+experience+entry+level&l=United+States&from=searchOnDesktopSerp&vjk=254dcd8c33926527\":\n",
      "            additional_row = [\"UX/UI - Entry Level\"]\n",
      "        elif url == \"https://www.indeed.com/jobs?q=data+entry+level&l=United+States&from=searchOnDesktopSerp&vjk=bd4fd45f0feb91c2\":\n",
      "            additional_row = [\"Data - Entry Level\"]\n",
      "        else:\n",
      "            additional_row = [\"Title Not Found\"]\n",
      "    \n",
      "        # Blank Row to make formatting nice\n",
      "        blank_row = ['']\n",
      "    \n",
      "    \n",
      "    \n",
      "    print(len(jobTitles))\n",
      "    print(len(companyName))\n",
      "    print(len(jobLocation))\n",
      "    print(len(links))\n",
      "    \n",
      "    #\n",
      "    #   # Open the CSV file in append mode\n",
      "    #    with open('Indeed_Jobs.csv', 'a', newline='') as file:\n",
      "    #        writer = csv.writer(file)\n",
      "    #        # Write the header and blank rows for formatting\n",
      "    #        writer.writerow(blank_row)\n",
      "    #        writer.writerow(additional_row)\n",
      "    #        writer.writerow(blank_row)\n",
      "    #\n",
      "    #        # Write jobs to the CSV\n",
      "    #        df = pd.DataFrame({'Job Title': jobTitles, 'Company Name': companyName, 'Location': jobLocation, 'Link': links})\n",
      "    #        df.to_csv(file, mode='a', header=True, index=False)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cwivtt/why_is_this_scraping_a_different_amount_of_info/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to extract data from `__APOLLO_STATE__` in `__NEXT_DATA__`\n",
      "Text: Hi Reddit, I need help with scraping,\n",
      "\n",
      "I am using Nodejs with fetch to scrape some websites.\n",
      "\n",
      "I want to extract(scrape) data from \\`window.\\_\\_NEXT\\_DATA\\_\\_.props.pageProps\\` in plain JSON.\n",
      "\n",
      "But site I am scraping has \\_\\_APOLLO\\_STATE\\_\\_\\` inside \\`window.\\_\\_NEXT\\_DATA\\_\\_.props.pageProps\\` which has lot of data and bit  difficult to parse due to nested structure and lot of extra data.\n",
      "\n",
      "Is there any easier way to achieve this?  \n",
      "Thanks in advance\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cw85v6/how_to_extract_data_from_apollo_state_in_next_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Twitter Scraper Chrome extension\n",
      "Text: Hi guys!\n",
      "\n",
      "I am using tweets as the data for my masters' thesis on a certain politician's rhetoric. I used the chrome extension Twitter Scraper, which worked okay-ish. I was quite happy at first, but later found out the scraper skipped/missed a lot of tweets on the first (and at that time only) scrape-session. When running the scraper again over smaller increments of the same timeframe, I found in one instance 234 'new' tweets. This meant I have to run the scraper over all my intended timeframes again.\n",
      "\n",
      "However, this extension stopped working mid-scrape. While it malfunctioned a few times during one of the many (50+) runs I did scattered over multiple days, it never completely stopped working like this before. I have reinstalled the extension, used a different X-account, a different Chrome-account, and even turned my PC on/off a couple of times, it still doesn't work.\n",
      "\n",
      "Does anyone here use this extension as well, and has run into the same problem? Or otherwise, is anyone able to help me with the final stretch of my Thesis by a workable solution for scraping (at the least/worst case scenario) 3 months worth of Tweets (date/content/type of tweet) from a singular X-account?\n",
      "\n",
      "You will have my eternal academic gratitute!  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cvxj67/twitter_scraper_chrome_extension/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I scrape pages with Cloudflare protection when encountering a 403 block?\n",
      "Text: Hello, how can I avoid Cloudflare protection while scraping? \n",
      "\n",
      "When I use the same proxy on Firefox with the FoxyProxy extension, I also get a 403 block. \n",
      "\n",
      "I am using an Amazon or Azure server and IP.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cvisv8/how_can_i_scrape_pages_with_cloudflare_protection/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there a free way to scrape tweets from X?\n",
      "Text: I'm working on my masters thesis and I want to conduct a sentiment analysis using tweets with certain keywords. I tried everything but nothing seems to work. \n",
      "Ps: I am using Python \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cv77cr/is_there_a_free_way_to_scrape_tweets_from_x/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do you get a list of companies that signed up for a certain government grant? (Canada)\n",
      "Text: Not sure if this is the most accurate place to post this but I guess it's somewhat related.\n",
      "\n",
      "One of my friend's company told me that they signed up for a certain government grant however I don't believe them.\n",
      "\n",
      "Is there some way of verifying if that company really did sign up for that grant?\n",
      "\n",
      "Is there some national database which lists various grants and lists all the organizations which signed up for it? Is accessing this database free? Or is it one of those pay-to-view databases?\n",
      "\n",
      "My main problem is that I don't know where to look..........so if you were building a web scraper, how would you program it to search the web to find this database?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cvgj5r/how_do_you_get_a_list_of_companies_that_signed_up/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Airtable\n",
      "Text: Hey there everyone, i was scraping airtable database which is publicly available and opens on my browser. Now i don't want to use selenium as it gets really cocky sometimes. I want an alternative way to do scrape the data. When i scroll the new rows are added to the panel html code and previous ones are deleted making it very difficult to scrape from easy methods.  \n",
      "Or i must use selenium for this purpose?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cvgazr/scraping_airtable/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: deduplicatuion strategies\n",
      "Text: writing a scraper that runs hourly. i use multithreading. per run it downloads, parses, and loads to a file. \n",
      "\n",
      "it fetches hourly data from yesterday to today.\n",
      "\n",
      "how can i make sure there s no duplicates? i thought of loading each concerned file if it exists and comparing it with scraped data, but i guess its inefficient? also there's concurrency issues with file reads and writes.\n",
      "\n",
      "im also thinking of creating a clean function that runs after all the data has been loaded. \n",
      "\n",
      "what do you guys think?\n",
      "\n",
      "im usung python, pandas, requests.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cv0hxz/deduplicatuion_strategies/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Newbie buying Apify actor\n",
      "Text: Hey everyone, I am communicating with the developer about creating an actor to scrape. I want the actor to be shared on my Apify account to avoid the risk of the developer deleting the scraper from his profile.\n",
      "\n",
      "I wanted to ask the community:\n",
      "\n",
      "1. What to watch out for when closing the deal\n",
      "2. Estimated Fair price? - yeah, it depends on the site security, etc.\n",
      "3. Other alternative suggestions?\n",
      "\n",
      "I also wanted to check for an opinion on whether I could create the scraper myself - on the [Apify](https://apify.com/) platform with the help of an AI model. I have no coding experience but actively engage in GPT and prompting.\n",
      "\n",
      "Thanks for the valuable answers!\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cuv4ed/newbie_buying_apify_actor/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape business data from google \n",
      "Text: Hello, I've developed a method to scrape all available data on businesses listed on Google, including their reviews and contact details, sorted by city. What are some potential uses for this information?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cv39md/scrape_business_data_from_google/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I am not able find a single good article/blog on using Scrapy to scrape Google SERP rank. Everywhere paid tools pushing their products?\n",
      "Text: I am just starting my scraping journey, though I am a developer proficient in backend and DevOps. Generally I am able to find tons of blogs and articles even on niche topic.\n",
      "\n",
      "However, I am little surprised that all the articles on how to use Scrapy for Google SERP are by paid tools. They present  convoluted steps, highlight why you shouldn't do this by your own and push their product. Even Github is not spared by them. I understand they are trying to convert users but even in this sub-reddit I see tons of posts by these paid tools.\n",
      "\n",
      "Pardon me if I am getting this wrong and would be very thankful if someone point to any good resources. Cheerios!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cuz6rj/i_am_not_able_find_a_single_good_articleblog_on/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: scraping stack/approach\n",
      "Text: So I just started with scrapping and since I know python, I was using python libraries (bs4, requests, scrapy, scrapy playwright) with some John Watson Rooney videos (super helpful) but I kept getting blocked by an http error 503 (with all the bells and whistles proxy rotation, headless browsing etc).\n",
      "Then I moved to crawlee and it has been such an amazing time. Not sure why, but no 503 errors. So in case you struggle with bot detection, maybe something to look at.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cupgvi/scraping_stackapproach/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Retail Sites Difficulty\n",
      "Text: I am a full time programmer that makes websites and apps for a living currently. I have a family member who asked me if I could make something that scrapes the prices off of some retail sites every so often given some urls. I know the crux of this whole thing would be getting past the sites scraping policies. So I have two main questions. \n",
      "\n",
      "1. How hard is this? If it's insanely difficult I'll tell them to just use one of these paid services that already do this. Will I have to constantly update the code to get past whatever sites latest anti-scraping measures as they come out?\n",
      "2. Anything to worry about legally? I can see they have policies on their sites but it's also public facing and they've already lost some similar lawsuits it seems like?\n",
      "\n",
      "Please guide me so I don't waste my time and/or get sued. :D\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cujv75/scraping_retail_sites_difficulty/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I design a queue so that I do not get errors from having too many connections open with one website?\n",
      "Text: I am trying to webscrape multiple urls and am currently using RabbitMQ as a queue from webscraping. The issue with that approach is that it is a FIFO queue and when I add new URLs from a website it scrapes them in order. That results in errors from having too many connections open with one website. What queue can I use that interleaves the elements from the queue so that all the websites from one URL are not scraped in order?\n",
      "\n",
      "Edit: I think the solution might be find URLs from multiple different sources, interleave them, and add them in FIFO order for the queue to scrape. Previously I was having CRON jobs for each of the websites to scrape. I will instead have CRON jobs to groups of websites to scrape.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cuap1y/how_can_i_design_a_queue_so_that_i_do_not_get/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there a guide on the legality of webscraping?\n",
      "Text: I want to scrape information from a company's website.  Their terms of service page on the site lists\n",
      "\n",
      "(iii) page or screen scrape, web harvest, or use any robot, spider, indexing agent or other automatic device, process or means to access the <COMPANY REDACTED> for any purpose, including extracting data from, monitoring or copying the Content\n",
      "\n",
      "Does this make it illegal?  Is there a guide about this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cu63j1/is_there_a_guide_on_the_legality_of_webscraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need Help , Not able to get response using requests.get()\n",
      "Text: Hello Guys,\n",
      "\n",
      "I want to extract some information from a web page but when im using requests.get() method its giving me timeout error. I tried to add headers too still its giving me error.\n",
      "\n",
      "Below is my code and Error:\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "\n",
      "headers = {\n",
      "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',\n",
      "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
      "    'Referer': 'https://parivahan.gov.in/',\n",
      "    'Cookie': 'JSESSIONID=F1848B1BEB2C7254960E092E0C803EF1'\n",
      "}\n",
      "time.sleep(0.01)\n",
      "url1= \"https://vahan.parivahan.gov.in/vahan4dashboard/\"\n",
      "r1 = requests.get(url1,  headers=headers)\n",
      "code= BeautifulSoup(r1.text, 'html')\n",
      "code\n",
      "\n",
      "\n",
      "Can anyone please help me to extract information?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cucoti/need_help_not_able_to_get_response_using/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Open-Source LinkedIn Scraper\n",
      "Text: I'm working on developing a LinkedIn scraper that can extract data from profiles, company pages, groups, searches (both sales navigator and regular), likes, comments, and more—all for free. I already have a substantial codebase built for this project. I'm curious if there would be interest in using an open-source LinkedIn scraper. Do you think this would be a good option?\n",
      "\n",
      "Edit: This will User's LinkedIn session cookies\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ctawfs/opensource_linkedin_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for a scraper to check if local businesses have websites - any recommendations?\n",
      "Text: I'm in need of a scraper tool to quickly determine whether local businesses have websites. Are there any free options available?\n",
      "\n",
      "On a side note, while manually checking through google maps isn't terribly difficult, it feels like automating this process should be fairly straightforward. Any insights or recommendations would be greatly appreciated. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ct8s6g/looking_for_a_scraper_to_check_if_local/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web scraping suggestions \n",
      "Text: Hey everyone, so i wanted to make a project on webscraping. Basically create a bot that will be given certain keywords and it will pull out data regarding those keywords from websites, \n",
      "On researching, I figured out I can use scrapy with playwright because I’m comfortable with Python. But recently I came across ScrapeGraphAI that i think can be very useful. \n",
      "Any suggestions how i can to about this project ? (It’s been only few days I’ve started learning these mentioned frameworks)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cthyfy/web_scraping_suggestions/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any advice for a newbie ?\n",
      "Text: I am a second year, Computer Engineering student, and i have experience with Java, C and basic python. I want to get my hands wet by doing a project to scrape data using Python, while I continue to learn Python. Can i message anyone for mentorship or advice ?. I have some ideas on what data, i'd like to get but still not entirely sure as just about everything is saturated.  Feel free to comment if I'm being too unrealistic for now. I would love to message someone with a business tho. I'd love to work with someone as well, as I'm into sports we can work on some projects where this is concerned. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ctbydv/any_advice_for_a_newbie/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping for *website* URLs from an image search- help!\n",
      "Text: For context, I am pretty new to scraping in general, but I cannot seem to find a tool to scrape an image search for the websites that the images are on rather than the image url itself.\n",
      "\n",
      "  \n",
      "Ex: I want to scrape a list of all websites that have the NBA logo on them.\n",
      "\n",
      "I've tried searching for this, but all that comes up are tools to scrape the images themselves. Any tips? TIA!\n",
      "\n",
      "p.s.- bonus points if it can also be done for the side bar that pops out when you use the \"related search\" feature on images\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ctgf5o/scraping_for_website_urls_from_an_image_search/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping YouTube trending videos in the last 5 years\n",
      "Text: I need data for YouTube trending videos, specifically US, in the last 5 years. I'm thinking maybe scraping from [archive.org](http://archive.org) and go from there? But I'm not really sure... would love insights on this matter\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ct7mx1/scraping_youtube_trending_videos_in_the_last_5/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Akamai cookie _abck\n",
      "Text: Does anyone have an idea how Akamai cookie is created, can anyone direct me to the application/tool which generates similar valid cookies what sorts of data does it contains. Thanks \n",
      "\n",
      "\\_abck=45E7562E8C598D848BE1DB2FB5DCC705\\~0\\~YAAQzCfIF6XssVyPAQAAcBt9gQtRN2fNam05v6PrXpCOgDE1slKwWIlbk3tGtkfqO1wG4CfAO8A26F6fJpKBmG9V/OtFqzMbprS0xLeCoJuVDWLHcyRWZYqgWRsAgBVY8xiYdkKkKO7u7SFkni4U3sgu1dnojOZcHkFU2fyJ3zr6mQyXl0wvDzzxiAs2BHfZj0lkXlV/VlUTue14+WvPTI7yTZ0Tss4xeBjQDHGFjD3Gc+mxLgTJc5TL2TlUlgmwzDt9G3s1+npZr3NFio7iPTPEvlrfueMrAWT1q1EEBVIg1yhuH7wCTSlPRrM6n5xaUIdZ5rvlc1vRSNIJAH8T+83iKr4Mp1Ob81WCDw1OUIM34Va0Y9lrtR2/hEc94FoTuy0DiVoOCEcw6VLeIH6OmQ+QKPMV6qXA\\~-1\\~-1\\~-1; [Domain=xyz.com](http://Domain=xyz.com); Path=/; Expires=Fri, 16 May 2025 13:00:10 GMT; Max-Age=31536000; Secure\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ctcvxz/akamai_cookie_abck/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Make.com and XQuery and Puppet\n",
      "Text: I'm trying to automate access to some of my own banking accounts and vendors that I make repeated purchases from. I saw some videos on [make.com](http://make.com) and was lead to believe they have the holy grail of web parsing. However, it seems like it'll fetch a page, and might provide crude parsing, but not much more. Or, it'll route things to an action (I think that's the right term), that'll employ some variant of AI to parse things. The pages don't change much more than 1-3/year structurally, but it's enough that cooking up some CSS, HTML->XML for XQuery, or puppet would be exhausting to maintain. What do people use? \n",
      "\n",
      "For example, let's say I want to get daily prices of a Canadian Maple Leaf Gold Coin in 1oz form from multiple vendors. How do people normally fetch the page and ultimately get something like SchiffGold $2400, ACME Gold $2800, etc. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ct597l/makecom_and_xquery_and_puppet/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to decouple chrome for personal use and python selenium chromedriver\n",
      "Text: Every month or so chrome updates automatically, and I forget to replace chromedriver with the latest version, causing my script to break down.  How do I keep a version of chrome and chromedriver for my script constant, while also updating chrome for personal use?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ct0sbz/how_to_decouple_chrome_for_personal_use_and/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Are there websites where I can sell leads\n",
      "Text: I have a long list of leads, for a specific niche, created by a combination of web scraping and manual research. Are there websites, where I could sell those leads? Is it even legal to do so?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1csvx5g/are_there_websites_where_i_can_sell_leads/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Difficulty Retrieving Table Data from Website: Need Assistance\n",
      "Text: Hi, I'm having trouble trying to retrieve table data from this website ([https://bluecollardfs.com/nba-optimizer](https://bluecollardfs.com/nba-optimizer)). The table data loads once after I select any option from the tags below:\n",
      "\n",
      "<div class=\"container-fluid py-2 bg-white shadow\">  \n",
      "  <div class=\"container-fluid d-flex justify-content-center justify-content-sm-start pb-4\">  \n",
      "<button class=\"dropdown-toggle slate-dropdown\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"#slatesCollapse\" aria-expanded=\"true\" aria-controls=\"slatesCollapse\">  \n",
      "SLATE: 7:00PM ET Main 2 Games  \n",
      "</button>  \n",
      "<div class=\"container d-flex justify-content-end smaller-font\">updated: 13:21:33 ET</div>  \n",
      "  </div>  \n",
      "  <div class=\"container-fluid collapse\" id=\"slatesCollapse\" style=\"\">  \n",
      "<div class=\"container\">  \n",
      "<div class=\"row gy-2\">  \n",
      "<button class=\"col-md-2 col-sm-3 col-xs-10 text-center slate-selector me-2\">  \n",
      "<div>7:00PM ET</div>  \n",
      "<div class=\"small-text\">(CLE vs BOS)</div>  \n",
      "</button>  \n",
      "<button class=\"col-md-2 col-sm-3 col-xs-10 text-center slate-selector me-2\">  \n",
      "<div>7:00PM ET</div>  \n",
      "<div class=\"small-text\">Main 2 Games</div>  \n",
      "<div class=\"small-text\">Main 2 Games</div>  \n",
      "</button>  \n",
      "<button class=\"col-md-2 col-sm-3 col-xs-10 text-center slate-selector me-2\">  \n",
      "<div>9:30PM ET</div>  \n",
      "<div class=\"small-text\">(DAL vs OKC)</div>  \n",
      "</button>  \n",
      "<button class=\"col-md-2 col-sm-3 col-xs-10 text-center slate-selector me-2\">  \n",
      "<div>10:00PM ET</div>  \n",
      "<div class=\"small-text\">(WNBA LAS vs ATL)</div>  \n",
      "</button>  \n",
      "</div>  \n",
      "</div>  \n",
      "  </div>  \n",
      "</div>  \n",
      "But no matter what I try, I can't get any of these buttons clicked so that I can retrieve the table data. Does anyone know what I'm doing wrong? Any help would be really useful. Thank you.                                                                 \n",
      "\n",
      "the code im using is right below                                                                                                                                            \n",
      "\n",
      "\n",
      "\n",
      "    from selenium import webdriver\n",
      "    from selenium.webdriver.common.by import By\n",
      "    from selenium.webdriver.firefox.options import Options\n",
      "    from selenium.common.exceptions import NoSuchElementException\n",
      "    \n",
      "    # Set Firefox options for headless browsing\n",
      "    firefox_options = Options()\n",
      "    firefox_options.headless = True\n",
      "    \n",
      "    # Initialize the driver\n",
      "    driver = webdriver.Firefox(options=firefox_options)\n",
      "    \n",
      "    try:\n",
      "        # Navigate to the website\n",
      "        driver.get(\"https://bluecollardfs.com/nba-optimizer\")\n",
      "    \n",
      "        # Find and click the DraftKings option\n",
      "        try:\n",
      "            draftkings_option = driver.find_element(By.XPATH, \"//div[contains(@class, 'site-selector') and text()='DraftKings']\")\n",
      "            draftkings_option.click()\n",
      "            print(\"DraftKings option clicked.\")\n",
      "        except NoSuchElementException:\n",
      "            print(\"DraftKings option not found or unable to click.\")\n",
      "    \n",
      "        # Check if DraftKings option is active\n",
      "        try:\n",
      "            draftkings_active = driver.find_element(By.XPATH, \"//div[contains(@class, 'site-selector') and contains(@class, 'active') and text()='DraftKings']\")\n",
      "            print(\"DraftKings option is active.\")\n",
      "        except NoSuchElementException:\n",
      "            print(\"DraftKings option is not active or not found after clicking.\")\n",
      "    \n",
      "        # Find and click the specified button using the specified XPath\n",
      "        try:\n",
      "            button_option = driver.find_element(By.XPATH, \"//*[@id='root']/div/div[3]/div[1]/button\")\n",
      "            button_option.click()\n",
      "            print(\"Button option clicked.\")\n",
      "        except NoSuchElementException:\n",
      "            print(\"Button option not found or unable to click.\")\n",
      "    \n",
      "        # Check if button option is active\n",
      "        try:\n",
      "            button_active = driver.find_element(By.XPATH, \"//*[@id='root']/div/div[3]/div[1]/button[@aria-expanded='true']\")\n",
      "            print(\"Button option is active.\")\n",
      "    \n",
      "            # Get the entire HTML content of the page\n",
      "            html_content = driver.page_source\n",
      "    \n",
      "            # Print the entire HTML content with UTF-8 encoding\n",
      "            print(html_content.encode('utf-8').decode('utf-8'))\n",
      "        except NoSuchElementException:\n",
      "            print(\"Button option is not active or not found after clicking.\")\n",
      "    \n",
      "    finally:\n",
      "        # Quit the driver\n",
      "        driver.quit()\n",
      "    \n",
      "    #only after and if  button option is active and clicked  is when Get the entire HTML content of the page beacuse after  XPATH, \"//*[@id='root']/div/div[3]/div[1]/button[@aria-expanded='true'] a tale with data is loaded into the html\n",
      "    \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cstbuj/difficulty_retrieving_table_data_from_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any solutions for an automated scraping of Instagram stories?\n",
      "Text: Hi! I would like to implement a social listening of Instagram stories. With it, the user of my app will be able to set up filters and notice the most important posts from their network (e.g. somebody needs help, calls for a meeting, etc.)\n",
      "\n",
      "I'm looking for the ways to set up the listening of the followings' Ig stories. Can you recommend any feasible solutions?\n",
      "\n",
      "Clarification:\n",
      "\n",
      "* we *can* ask the user to synchronize/log in to their Ig account, the process doesn't have to be anonymous;\n",
      "* we don't have to listen every profile by default: it's fine setting the listening for particular social handles;\n",
      "\n",
      "P.S. If you would like to use one, let know in comments too. Can get back if I find one.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1csd9tn/any_solutions_for_an_automated_scraping_of/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping amazon.com product pages\n",
      "Text: Hello there,\n",
      "\n",
      "Would someone be kind enough to help me crawl amazon.com at relative scale?\n",
      "I am using playwright in python and also tried my hand with apache http client in java.\n",
      "A few requests are fine where I am able to get the html content with changing headers and proxies.\n",
      "At relative scale, amazon starts throwing captchas.\n",
      "Note: with Apache HttpClient, I am also changing the TLS signature.\n",
      "\n",
      "It would be really helpful if someone will be able to share some solutions/suggestions to improve my success. Thank you in advance.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cs4ed6/scraping_amazoncom_product_pages/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: scraping a form\n",
      "Text: Hi guys,\n",
      "\n",
      "I am trying to scrape this form from this website https://www.excard.com.my/spec/Litho/Bill-Book.\n",
      "\n",
      "The form is dynamic, the first option is size, a dropdown menu and with each option the price is updated. This is the code I've manage to write so far but I am not able to achieve the final goal, that is, to select each option and then scrape the price for that option:\n",
      "\n",
      "    import scrapy\n",
      "    from scrapy_splash import SplashRequest\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    class ExcardSpider(scrapy.Spider):\n",
      "        name = \"excard\"\n",
      "        allowed_domains = [\"www.excard.com.my\"]\n",
      "        script_two = '''\n",
      "            function main(splash, args)\n",
      "    \n",
      "                url = args.url\n",
      "                assert(splash:go(url))\n",
      "                splash:wait(1)\n",
      "                input_username = assert(splash:select(\"#TemplatedContent1__product_txtusername\"))\n",
      "                input_username:focus()\n",
      "                input_username:send_text(\"you_username\")\n",
      "                assert(splash:wait(0.5))\n",
      "                input_password = assert(splash:select(\"#TemplatedContent1__product_txtpassword\"))\n",
      "                input_password:focus()\n",
      "                input_password:send_text(\"your_password\")\n",
      "                assert(splash:wait(0.5))\n",
      "                assert(splash:runjs('document.querySelector(\"#TemplatedContent1__product_excardLogin\").click()'))\n",
      "                \n",
      "                assert(splash:wait(5))\n",
      "    \n",
      "                splash:set_viewport_full()\n",
      "                assert(splash:wait(5))\n",
      "                local dropdown_selector = \"#mainContent_order_spec_controller1_order_spec_bizdoc1_ddlSize\"  -- Change to the appropriate selector\n",
      "                local option_selectors = splash:select_all(dropdown_selector .. \" option\")\n",
      "                \n",
      "                local dropdown_selector = \"#mainContent_order_spec_controller1_order_spec_bizdoc1_ddlSize\"  -- Change to the appropriate selector\n",
      "        \n",
      "                -- Select the first option\n",
      "                local script = string.format(\"document.querySelector('%s').selectedIndex = 5; var event = new Event('change'); document.querySelector('%s').dispatchEvent(event);\", dropdown_selector, dropdown_selector)\n",
      "                splash:runjs(script)\n",
      "                assert(splash:wait(5))\n",
      "    \n",
      "                return splash:html()\n",
      "            end\n",
      "        '''\n",
      "    \n",
      "        def start_requests(self):\n",
      "            yield SplashRequest(url='https://www.excard.com.my/spec/Litho/Bill-Book', callback=self.parse, endpoint=\"execute\", args={\n",
      "                    'lua_source': self.script_two\n",
      "                })\n",
      "    \n",
      "        def parse(self, response):\n",
      "            price = response.xpath(\"//td[@id='tdPriceb4Disc2']/text()\").get()\n",
      "            print(price)\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1crztsw/scraping_a_form/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping instagram, tiktok, facebook\n",
      "Text: Hi, i would like to gather from all of these platforms the amount of likes, comments and shares i got in the last 30 days, the “scraping” will be done once a month..What chance of getting banned do i have? I am using node.js (next.js to be exact) and currently implemented it only for ig using instagram-private-api.. So, what chance of getting punished do i have?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1crzcvu/webscraping_instagram_tiktok_facebook/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How much would this cost?\n",
      "Text: Hey guys I’m a non technical founder, so the question i am here with today is how much would it cost to get a freelance developer to develop a custom web scraper for my Project?\n",
      "\n",
      "The Functionality for the Scraper:\n",
      "\n",
      "• **Scrape Content from Sites like:** YouTube, Google Search, Reddit, Instagram and Mega NZ. In every site there should be a Search criteria Key word given by the User.\n",
      "\n",
      "• **Data that it should “Farm”:** The type of data that i nee from those sites are:”Images, Videos, Titel, Description, Content Link, Platform name”.\n",
      "\n",
      "• **Filter System:** the current user must have the ability to exclude/ignore some Results by giving it Keywords.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1crnbxy/how_much_would_this_cost/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help finding script on website\n",
      "Text: Looking for better ways of achieving this.\n",
      "\n",
      "Let’s say i am trying to confirm the presence of a script with a src url of website.com. I need to traverse the main website pages so I am using all homepage href links and checking each url for said script using python/beautiful soup. I don’t want to grab sitemap data as this could yield too many pages. The purpose is to find a booking tool which has a specific script tag.\n",
      "\n",
      "I have a list of around 200 websites to check and this process is very slow.\n",
      "\n",
      "I am wondering if anyone could suggest a better way. I am proficient in both python and node so open to using either language.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1crwfwe/help_finding_script_on_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: PDF direct dowload issue\n",
      "Text: The site [*vermittelerregsiter.info*](http://vermittelerregsiter.info) allows to download a PDF file by a regular GET request: eg. [https://www.vermittlerregister.info/recherche?a=pdf&registernummer=D-W-111-BHC1-55](https://www.vermittlerregister.info/recherche?a=pdf&registernummer=D-W-111-BHC1-55)\n",
      "\n",
      "We want to automate it \\[for mass loading\\] with JAVA but we've failed.\n",
      "\n",
      "# Failed attempts\n",
      "\n",
      "See some eg. of what we've tried:\n",
      "\n",
      "1. [https://medium.com/@pasanmanohara/download-a-pdf-file-from-a-url-in-the-spring-boot-java-30fa325d6ab9](https://medium.com/@pasanmanohara/download-a-pdf-file-from-a-url-in-the-spring-boot-java-30fa325d6ab9)\n",
      "2. [https://www.baeldung.com/java-download-file#using-java-io](https://www.baeldung.com/java-download-file#using-java-io) (point 2)\n",
      "3. Scrapeops own requests with browsers.\n",
      "\n",
      "All of them **return the web page rather then a PDF file**.\n",
      "\n",
      "# Supposed site/server operation\n",
      "\n",
      "I've checked and it turned out **the site first checks if bot or real user (browser) is requesting** and only afterward it returnes PDF:\n",
      "\n",
      "When I try to open a PDF link in a browser (Edge and also in Chrome), then  \n",
      "(1) the web page opens first \\[and there it checks the authenticity of the browser\\] -- my assumption.  \n",
      "(2) when I request the same link again (F5), the file gets indeed loaded. The subsequent requests download PDFs immediately.  \n",
      "Can we try \"double click\" or something similar ?\n",
      "\n",
      "# Check for antibots on site\n",
      "\n",
      "**Check for antibots (at** [**discord server**](https://discord.com/channels/1168271207997644940/1200004478074630290)**) has shown that the site is void of those**: \n",
      "\n",
      "https://preview.redd.it/rnrnas6mue0d1.png?width=727&format=png&auto=webp&s=84da52fa3b66084b84acc0cd1b22ed0020a6e1e3\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cruxxj/pdf_direct_dowload_issue/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I need some help with scrapping a site\n",
      "Text: Hello, I have been trying to scrape this site [https://satsuitequestionbank.collegeboard.org/digital/results](https://satsuitequestionbank.collegeboard.org/digital/results)  \n",
      "but until now I can't find a good way to do it. any ideas?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1crufvw/i_need_some_help_with_scrapping_a_site/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Saas Salestech Company seeking Data Engineer with advanced web scraping knowledge\n",
      "Text: If you are or know someone that might the below criteria and have advanced web scraping expertise, please DM me. \n",
      "\n",
      "**AutoLead** researches, nurtures, qualifies, and delivers high-intent B2B leads right in your calendar.\n",
      "\n",
      "Whether you’re a B2B company, agency, investor, recruiter, integrator, or else, AutoLead will reduce your meeting acquisition costs by x10, help you better understand your market, forecast your sales growth, and help your sales team focus on… selling.\n",
      "\n",
      "📈 F**orecast-** The company is expected to grow to a multi-million ($) valuation by 2025. Not only because the addressed market is consequent (all the B2B companies) and our approach faces almost no concurrence yet, but also because the solution will be able to obtain meetings with its own potential customers. Yes, very practical.\n",
      "\n",
      "**As an early core member, \\*\\*you would get shares\\*\\* of the company and so \\*\\*direct exposure\\*\\* to this growth.**\n",
      "\n",
      "\\*\\*📍Current Stage \\*\\*- We're currently 80% finished with the research phase and are looking for more developers to lead and accelerate the release of a v1 (scheduled for release in Q3 2024).\n",
      "\n",
      "\\*\\*🌱 Team culture \\*\\*- On a human-level, our foremost goal is to see our team members thrive and grow through their experience with us. We create a supportive environment where open communication of feelings and needs is encouraged. This ensures that everyone is always engaged in meaningful and enjoyable work. Additionally, we place great importance on fair value distribution, emotional caring, environmental respect, authentic relationships, and maintaining a positive, light-hearted culture.\n",
      "\n",
      "**💻 Technical requirements - For this Data Engineer role.**\n",
      "\n",
      "* Proficiency in at least 8 of the following:\n",
      "* Python, Node.js, Typescript, Bash, SQL\n",
      "* ETL pipelines design\n",
      "* Dagster, Apache Airflow\n",
      "* LLMs Prompt Engineering\n",
      "* Embeddings and ANN vector search\n",
      "* Column-oriented DBMS\n",
      "* Data Mining\n",
      "* Data Sourcing & Traceability\n",
      "* Query optimization (indexing, etc.)\n",
      "* Real-time Data Processing\n",
      "* Advanced Web Scraping\n",
      "* Experience in any of the following is a plus:\n",
      "* Machine learning & Forecast models\n",
      "* B2B big data\n",
      "* Data Science\n",
      "\n",
      "**Only those passionate about making a real impact should apply. Startup life can be challenging, but the rewards are profoundly fulfilling.**\n",
      "\n",
      "**Compensation:** Initially, this role is equity-based, with specific percentages tailored on an individual basis(as we grow, you grow with us).\n",
      "\n",
      "As we secure funding, the role offers a temporary monthly salary ranging from $3,500 to $5,000 USD. Post-funding, we will review and adjust salaries to align with individual experience and prevailing market rates.\n",
      "\n",
      "**Application Deadline:** We aim to fill this position by June 1st, 2024.\n",
      "\n",
      "Interested? Please direct message me and include a link to your LinkedIn profile. Looking forward to hearing from you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1crlleg/saas_salestech_company_seeking_data_engineer_with/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Bypassing Cloudflare for API Endpoint\n",
      "Text: It has been quite some time since I did some webscraping and never went too deep before.\n",
      "\n",
      "I am trying to fetch data from a Public API that feeds a UI game information. I found the request by inspecting the network traffic and am trying to make the same request from python:\n",
      "\n",
      "    import json, requests\n",
      "    \n",
      "    url = \"https://<domain>/api/fixtures\"\n",
      "    \n",
      "    cookies = {}\n",
      "    \n",
      "    sess = requests.Session()\n",
      "    sess.cookies.set(\"SERVERID\", \"public-03\", domain=\"<domain>\", path=\"/\")\n",
      "    \n",
      "    sess.get(url).text\n",
      "\n",
      "the response is cloudflare:\n",
      "\n",
      "    You are unable to access</h2>\\n      </div><!-- /.header -->\\n\\n      <div class=\"cf-section cf-highlight\">\\n        <div class=\"cf-wrapper\">\\n          <div class=\"cf-screenshot-container cf-screenshot-full\">\\n            \\n              <span class=\"cf-no-screenshot error\"></span>\\n            \\n          </div>\\n        </div>\\n      </div><!-- /.captcha-container -->\\n\\n      <div class=\"cf-section cf-wrapper\">\\n        <div class=\"cf-columns two\">\\n          <div class=\"cf-column\">\\n            <h2 data-translate=\"blocked_why_headline\">Why have I been blocked?</h2>\\n\\n            <p data-translate=\"blocked_why_detail\">This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.</p>\\n\n",
      "\n",
      "Any thoughts on how to approach this? When inspecting the website I have a couple of session cookies which I have tried adding and that hasn't helped. Would Selenium or another form of webscrapper work better?\n",
      "\n",
      "Thanks!\n",
      "\n",
      "EDIT: Tinkering with the headers and can get this message instead... not sure if better or worse xd\n",
      "\n",
      "    '<html>\\r\\n<head><title>403 Forbidden</title></head>\\r\\n<body>\\r\\n<center><h1>403 Forbidden</h1></center>\\r\\n<hr><center>cloudflare</center>\\r\\n</body>\\r\\n</html>\\r\\n<!-- a padding to disable MSIE and Chrome friendly error page -->\\r\\n<!-- a padding to disable MSIE and Chrome friendly error page -->\\r\\n<!-- a padding to disable MSIE and Chrome friendly error page -->\\r\\n<!-- a padding to disable MSIE and Chrome friendly error page -->\\r\\n<!-- a padding to disable MSIE and Chrome friendly error page -->\\r\\n<!-- a padding to disable MSIE and Chrome friendly error page -->\\r\\n'\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cretnt/bypassing_cloudflare_for_api_endpoint/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for chrome addon for scraping data from yahoo finance\n",
      "Text: I used to have a chrome addon that I could scrape the financials tables from yahoo but I don't remember what was it, and all the solutions I find online are to create a scraper using python which I have zero knowledge about it, and I don't have time to learn how to code a scraper on python. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1crhcd7/looking_for_chrome_addon_for_scraping_data_from/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Fight with Cloudflare\n",
      "Text: Hey there  [**webscraping**](https://www.reddit.com/r/webscraping/)  community, I'm in a fight with cloudflare . I tried everything, Selenium, Undetectable browser, seleniumbase and  [puppeteer](https://github.com/puppeteer/puppeteer) .  \n",
      "\n",
      "\n",
      "As I read somewhere Cloudflare protection has different modes and versions meaning some websites use more advanced Cloudflare security than others and are harder to reach  \n",
      "I'm guessing the website that i'm tryna reach has activated the most advanced version.  \n",
      "\n",
      "\n",
      "What should I do? any idea?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cqyr0q/fight_with_cloudflare/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Embedded ESRI map on archived website on the WayBackMachine\n",
      "Text: Hi, \n",
      "\n",
      "I have been unsuccessful in getting the data table used on an embedded map on an archived website by the WayBackMachine. I am trying to import all possible dates on which this website was archived. \n",
      "\n",
      "Here is the link: [https://web.archive.org/web/20201104110015/https://www.schools.nyc.gov/school-year-20-21/return-to-school-2020/health-and-safety/daily-covid-case-map](https://web.archive.org/web/20201104110015/https://www.schools.nyc.gov/school-year-20-21/return-to-school-2020/health-and-safety/daily-covid-case-map)\n",
      "\n",
      "Any suggestions? \n",
      "\n",
      "Thanks.   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cr6y77/embedded_esri_map_on_archived_website_on_the/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Headless browser performance issues\n",
      "Text: I am currently facing a performance bottleneck with my webscraper. I am using crawlee with playwright and I am doing around 3 requests per second + like 1-2 http requests per second. Suprisingly, this is enough to max out my ryzen 5 3600. Any suggestion? This performance seems pretty underwhelming to me\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cqwhyp/headless_browser_performance_issues/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Code showing no results in the Python idle\n",
      "Text: Hello everyone, i hope you are all doing well   \n",
      "this is the code i took from chatgpt to take out data from [https://www.blacklane.com/en/](https://www.blacklane.com/en/)  \n",
      "but it is showing nothing in the python idle   \n",
      "the code  \n",
      "import requests\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "\n",
      "# Define the list of postcodes (from locations) and airports (to locations)\n",
      "\n",
      "postcodes = \\[\n",
      "\n",
      "\"RH6 0NP\", \"Stansted CM24 1RW, UK\", \"Luton LU2 9NW, UK\", \"London EC1A 7BE, UK\"\n",
      "\n",
      "\\]\n",
      "\n",
      "\n",
      "\n",
      "airports = \\[\n",
      "\n",
      "\"London Heathrow Airport (LHR) - All terminals, Heathrow Airport, London, Greater London\",\n",
      "\n",
      "\"London Stansted airport (STN)\", \"London Gatwick Airport (LGW)\", \"Luton Airport London (LTN)\", \"London City Airport (LCY)\", \"London Southend Airport (SEN)\"\n",
      "\n",
      "\\]\n",
      "\n",
      "\n",
      "\n",
      "# Define the URL template for Blacklane\n",
      "\n",
      "url\\_template = \"https://www.blacklane.com/en/booking\"\n",
      "\n",
      "\n",
      "\n",
      "# Create an empty list to store the results\n",
      "\n",
      "results = \\[\\]\n",
      "\n",
      "\n",
      "\n",
      "# Iterate over postcodes and airports\n",
      "\n",
      "for from\\_location in postcodes:\n",
      "\n",
      "for to\\_location in airports:\n",
      "\n",
      "# Construct the payload for the request\n",
      "\n",
      "payload = {\n",
      "\n",
      "\"pickup\": from\\_location,\n",
      "\n",
      "\"dropoff\": to\\_location\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "# Send a GET request to Blacklane with the payload\n",
      "\n",
      "response = requests.get(url\\_template, params=payload)\n",
      "\n",
      "\n",
      "\n",
      "# Parse the HTML response\n",
      "\n",
      "soup = BeautifulSoup(response.content, \"html.parser\")\n",
      "\n",
      "\n",
      "\n",
      "# Extract the taxi fares for Business Class, Business SUV, and First Class\n",
      "\n",
      "business\\_class\\_fare = soup.find(\"div\", {\"aria-label\": \"Business Class\"}).find(\"p\", {\"class\": \"SuggestedCarClasses\\_price\\_\\_lFVVQ\"}).text\n",
      "\n",
      "business\\_van\\_suv\\_fare = soup.find(\"div\", {\"aria-label\": \"Business Van/SUV\"}).find(\"p\", {\"class\": \"SuggestedCarClasses\\_price\\_\\_lFVVQ\"}).text\n",
      "\n",
      "first\\_class\\_fare = soup.find(\"div\", {\"aria-label\": \"First Class\"}).find(\"p\", {\"class\": \"SuggestedCarClasses\\_price\\_\\_lFVVQ\"}).text\n",
      "\n",
      "\n",
      "\n",
      "# Append the results to the list\n",
      "\n",
      "results.append({\n",
      "\n",
      "\"From\": from\\_location,\n",
      "\n",
      "\"To\": to\\_location,\n",
      "\n",
      "\"Business Class Fare\": business\\_class\\_fare,\n",
      "\n",
      "\"Business SUV Fare\": business\\_van\\_suv\\_fare,\n",
      "\n",
      "\"First Class Fare\": first\\_class\\_fare\n",
      "\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "# Convert the results to a DataFrame\n",
      "\n",
      "df = pd.DataFrame(results)\n",
      "\n",
      "\n",
      "\n",
      "# Save the DataFrame to a CSV file\n",
      "\n",
      "df.to\\_csv(\"taxi\\_fares.csv\", index=False)\n",
      "\n",
      "  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cq5u9n/code_showing_no_results_in_the_python_idle/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Manipulating classnames\n",
      "Text: Hey everyone! I started using Cheerio lately and i love it, it's like unlocking a whole new world. I've trying to scrape the NASCAR schedule from this website ( [https://www.motorsport.com/nascar-cup/schedule/2024/](https://www.motorsport.com/nascar-cup/schedule/2024/) ) and while i got the session names and etc but i have a slight problem. I get the wrong times for each event. (I want the \"your time\" ones). I noticed that by default it selects \"local time\" instead of \"your time\". Would like to know whether there's a way in Cheerio to have \"your time\" selected and thus display the corresponding data to scrape. I tried manipulating the className itself but it didn't help.\n",
      "\n",
      "  \n",
      "Thanks in advance!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cq3ir5/manipulating_classnames/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best way to see how actual request is sent?\n",
      "Text: I have some code that executes using Python requests and successfully gets the html content of the page, however when using another library (Rust reqwest) with the same headers I get the cloudflare “You are not authorized to view this page”.\n",
      "\n",
      "I’m thinking there is something in how the user agent headers are coming across that is different in the library.  \n",
      "\n",
      "What would be the best way to see the raw http request from both libraries to compare and see what the difference is? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cpsvaa/best_way_to_see_how_actual_request_is_sent/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Crawling gopro and medline.com playwright \n",
      "Text: Can someone plase help me to crawl gopro and medline.com with playwright? I keep getting detected as a bot. Even with a single request using Playwright.\n",
      "I am using playwright in python.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cpvr9s/crawling_gopro_and_medlinecom_playwright/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to check whether captcha was solved or not using JavaScript?\n",
      "Text: Hello fellow webscrapers. Good day everyone. \n",
      "\n",
      "I'm facing an issue where I have to check whether captcha was solved or not. I've got a webscraper that scrapes certain web pages with Google's reCaptcha; I'm able to bypass captcha successfully, but I can't figure out a way to check whether it was solved or not. I used to do so a few months ago by checking grecaptcha.getResponse().length; if it is equal to 0 it wasn't solved; otherwise it is. Now that solution doesn't work and I'm pretty much stuck trying to figure out how to solve this issue.\n",
      "\n",
      "My scrapers are built with Selenium.. I tried almost everything I could think of. I sniffed incoming HTTP requests using Selenium, but I couldn't get the response where I would be able to check whether it's solved or not. I tried grecaptcha.enterprise.getResponse().length; and it always returns 0. I tried checking captcha box using Selenium, but it doesn't update its state. I inspected every possible value on grecaptcha object; all values stay the same before and after captcha is solved! Been stuck for almost 24hrs. I would appreciate any outside perspective on this, I'm desperate for a solution!\n",
      "\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cpobli/how_to_check_whether_captcha_was_solved_or_not/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: bypass CloudFlare\n",
      "Text:  does anyone know how to set TLS on openbullet to bypass cf?  \n",
      "   \n",
      "\n",
      "\n",
      "([https://github.com/bogdanfinn/tls-client](https://github.com/bogdanfinn/tls-client))\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cpimfq/bypass_cloudflare/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any good leecher that works with Ubiqfile or with Xubster?\n",
      "Text: Hoping this doesn't get deleted, Is there any good (May 2024) working leecher that works with Ubiqfile or Xubster? That it's mostly safe to use, and overall, that works? The cheaper the better. Or if you know any alternative method to get the wanted content, please tell me.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cpg1c1/any_good_leecher_that_works_with_ubiqfile_or_with/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I want to scrap just a list on a page of a website automatically every week to anywhere like (notion,google sheets)\n",
      "Text: Hello guys there is a website called eksisozluk, it is something like turkish reddit/hackernews type site\n",
      "\n",
      "[https://eksisozluk.com/istatistikler](https://eksisozluk.com/istatistikler)\n",
      "\n",
      "the left column of this page of the website updates every week on monday and it lists the comments that got the most upvotes that week.\n",
      "\n",
      "after one week it is replaced by new weeks best ıpvotes on monday (I think) and there is now way to see the previous week's most upvoted comments.\n",
      "\n",
      "What ı want to do is to just get the (20 item) list on the left column with links to every week.\n",
      "\n",
      "I will try it with a google script to a google sheet if I can, but if there is any other simple way can you help me about this. This is just a personal obsession nothing more nothing less. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cp96qa/i_want_to_scrap_just_a_list_on_a_page_of_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Browser fingerprinting, blocking me after scraping\n",
      "Text: Hello,\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I have setup an app to scrape a website periodically for information. I have randomized the refresh rate etc to not be too burdensome on their website and draw attention to myself.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "However, after a few days of on and off scraping. I will eventually be permanently blocked from logging into their system. If I swap to my computer downstairs with the same details it will allow me to login, eventually leading to being permanently blocked on that computer after some time too.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "We have employed fakeuseragent, tried proxy, tried VPN etc. After waiting a week then the block will disappear and reset all over again.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Does anyone have any advice on how to reduce this from happening? It is driving me mad.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Apologies if this has been posted before. I had a look but couldn't find anything specific to my case.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Thank you\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cp3v3l/browser_fingerprinting_blocking_me_after_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Why does scraping the google search results page yield some different HTML?\n",
      "Text: Disclosure: I'm neither well versed with web development concepts nor with web scraping. I'm sorry if I'm making any obvious mistakes. I wanted to make this project to learn more about web scraping, and build an ease-of-living tool for me alongside it.\n",
      "\n",
      "I am building a command line dictionary tool for myself, where I display the meaning of a word entered as the argument. After researching, I figured there were two ways to go about this:\n",
      "\n",
      "1. Web scraping then subsequently parsing the HTML that I would get\n",
      "2. Using the google search API.\n",
      "\n",
      "I decided to go for the first option, because I didn't want to use the google console. Even if they say it is free for first $300 or something, you have to provide credit card deets, which for me, as a student, is a big no-no. So I made the first prototype with web scraping, but I ran into an obstacle. I was able to extract and parse the html, but it wasn't exactly the HTML I was seeing in the \"Inspect\" view of the search results web page.\n",
      "\n",
      "* eg, `$ define travesty` sends an HTTP request to google with the following query: \"https://www.google.com/search?q=travesty+meaning\". But the html upon parsing that I got versus the html upon inspection in the browser were completely different.\n",
      "\n",
      "Also I read somewhere that scraping google's websites is against their policy, If I get caught my account could be banned. So I went with the other approach instead, because I just wanted to build this quickly. I found an API that gives you google search results in JSON format. But the catch is that I can only query it 100 times a month, which is not that serious of a limit, but still, I feel unsatisfied. I'd still prefer using web scraping as I wanna learn this tech, so regarding that my queries are:\n",
      "\n",
      "1. Why did the HTML differ?\n",
      "2. Can I scrape google without getting blocked from using my account forever?\n",
      "3. Is there any other, better approach, to building such a tool?\n",
      "\n",
      "BTW, I made this project using Rust with tokio, clap and serf-search-rust.\n",
      "\n",
      "[the tool's output as of now](https://preview.redd.it/w5ocnroi6qzc1.png?width=1460&format=png&auto=webp&s=02a86bd7661066123a28154c2956b41705b7eb7e)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cp95g9/why_does_scraping_the_google_search_results_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best practice for when speed doesn't matter but not getting blocked is critical? \n",
      "Text: I'm doing a daily scrape of a small amount of data (edit: 100-300ish calls) behind a login. I'm using selenium to host the session and using an API call that I got from the network calls to get the info. \n",
      "\n",
      "My current setup navigates to the page where the data is shown to the user, waiting 5-15 seconds between API calls, and quits after the first response that gives a status other than 200. \n",
      "\n",
      "Can I drop that delay to 1-3 seconds? Should I be doing anything else? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cow0sj/best_practice_for_when_speed_doesnt_matter_but/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Moving from Python to Golang to scrape data\n",
      "Text: I have been scraping sites using Python for a few years. I have used beautifulsoup for parsing HTML, aiohttp for async requests, and requests and celery for synchronous requests. I have also used playwright (and, for some stubborn websites, playwright-stealth) for browser based solutions, and pyexecjs to execute bits of JS wherever reverse engineering is required. However, for professional reasons, I now need to migrate to Golang. What are the go-to tools in Go for webscraping that I should get familiar with?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cof6sa/moving_from_python_to_golang_to_scrape_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best choice for SBC when scraping \n",
      "Text: Whats your best coice for a SBC when webscraping?\n",
      "I am curious and looking for tips which SBC to buy for my scraping tasks.\n",
      "\n",
      "I use python, selenium and a non headless browser. My data is stored in a JSON file. Nothing fancy. My script runs scheduled by cronjob every 5 minutes.\n",
      "\n",
      "If it were just requests, I wouldn't worry about it, but it has to start a Chrome instance all the time and I don't know how hardware-heavy that is. What are your purchase recommendations? Maybe a Raspberry 5 is enough?\n",
      "\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cosubj/best_choice_for_sbc_when_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Completely new to this, advice needed.\n",
      "Text: Hi, how easy would it be to scrape amazon to find the biggest price drops?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1coss90/completely_new_to_this_advice_needed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: how to make a web automation to this good product finding proccess with AI, webscraping and excel\n",
      "\n",
      "Text: **the input:** an aliexpress search page result that contains a lot of products, like this: [scissors – קנה scissors עם משלוח חינם ב- version AliExpress](https://he.aliexpress.com/w/wholesale-scissors.html?dp=cbd326ad93df700c706801785d0864ac&af=701906&cv=47843&afref=https%3A%2F%2Faliradar.com%2F&mall_affr=pr3&utm_source=admitad&utm_medium=cpa&utm_campaign=701906&utm_content=47843&dp=cbd326ad93df700c706801785d0864ac&af=701906&cv=47843&afref=https%3A%2F%2Faliradar.com%2F&mall_affr=pr3&utm_source=admitad&utm_medium=cpa&utm_campaign=701906&utm_content=47843&aff_fcid=b501a8888d1749e4a12e4b792eb55e30-1715324667541-00401-_ePNSNV&aff_fsk=_ePNSNV&aff_platform=portals-tool&sk=_ePNSNV&aff_trace_key=b501a8888d1749e4a12e4b792eb55e30-1715324667541-00401-_ePNSNV&terminal_id=57c47f7dc8994ba196a0357f22396ed2)\n",
      "\n",
      "**the output:** be an excel or google sheets file with list of links of products, their description according to the website and an alternative marketing description of AI\n",
      "\n",
      "the automation:\n",
      "\n",
      "i will give an aliexpress page with all the product pages, and the automation will go into each of these products pages, and detrmine if it stands in a standart that i defined(it has above this rating and costs less than x).\n",
      "\n",
      "if it doesnt just continue to the next product. if it does, copy the product link to [ScamAdviser.com | Check a website for risk | Check if fraudulent | Website trust reviews |Check website is fake or a scam](https://www.scamadviser.com/). if the trust score>70, add the link of the product and its description(from inside the link) in excel or google sheets,\n",
      "\n",
      "and add a cell next to it in which an ai generate alternative description.\n",
      "\n",
      "continue the process on all the products on the page, and do the same thing in the next 10 pages of the search result page(the input)\n",
      "\n",
      "https://preview.redd.it/qmio7dzlakzc1.png?width=555&format=png&auto=webp&s=72c207f63ab81ec63253169825e530de6437ff4c\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cokt0l/how_to_make_a_web_automation_to_this_good_product/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What is the most cost efficient tool to scrape a Datadome protected website ?\n",
      "Text: I am trying to scrape Leboncoin.fr which is protected by DataDome.\n",
      "\n",
      "I have tried Bright Data and Oxylabs web unlocker but they are pretty expensive and will not be a viable option for this project.\n",
      "\n",
      "Do you have any other alternative?\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1co6a3g/what_is_the_most_cost_efficient_tool_to_scrape_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I'm learning beautifulsoup4 and new to web scraping\n",
      "Text: I was wondering if I can get a little help with a simple script that I am working on. All it does is just scrapes and print out pokedex data. My issue is scraping the data from multiple tables, I have the first set which is the \"pokedex data \"section and it gives you a brief detail about the animal this is the first which shares the class 'vitals-table' with the other tables.\n",
      "\n",
      "For the second table that I want to scrape I am using the exact same method for the first table but I keep getting this error as if the data/tags do not exist when they do exist.\n",
      "\n",
      "`ev_yield = table.find('th', string=\"EV yield\").find_next_sibling('td').text.strip()`\n",
      "\n",
      "`^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^`\n",
      "\n",
      "`AttributeError: 'NoneType' object has no attribute 'find_next_sibling'`\n",
      "\n",
      "`Here's the` [link to the gist](https://gist.github.com/Pr3m0ThaD3v/81fe6f668373ad9f38a29e2b28d81a37) `and` [html source](https://pokemondb.net/pokedex/bulbasaur#dex-stats) `if you're interested in helping.`\n",
      "\n",
      "You don't have to hold my hand or anything I do have a feeling my issue has to do with parent and child relationship within the tree structure and that I need to access the second table differently than the first.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cnzfpf/im_learning_beautifulsoup4_and_new_to_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How To Deploy Puppeteer Script in AWS Lambda using AWS SAM CLI\n",
      "Text: \n",
      "URL: https://www.youtube.com/watch?v=INlCCRdOfj4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Thank you for making it easy 😂\n",
      "Text: \n",
      "URL: https://i.redd.it/frmlgssd5azc1.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Are LeetCode’s Problem Sets and Solutions copyrighted?\n",
      "Text: I’m building my own website similar to LeetCode with coding challenges and sample solutions. Is it legal to crawl data from some popular coding practice sites and use as my own data source?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cnyt8n/are_leetcodes_problem_sets_and_solutions/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Facebook User ID\n",
      "Text: Hello, I have developed a code for scraping user IDs from Facebook. It works well, but it only scrapes a small number of members. For example, in a group with 60k members, it only scrapes 1k users.\n",
      "\n",
      "I want to develop the code to scrape more users.\n",
      "\n",
      "    from selenium import webdriver\n",
      "    from selenium.webdriver.common.by import By\n",
      "    from selenium.webdriver.support.ui import WebDriverWait\n",
      "    from selenium.webdriver.support import expected_conditions as EC\n",
      "    from bs4 import BeautifulSoup\n",
      "    import time\n",
      "    \n",
      "    # Function to scroll down to the bottom of the page\n",
      "    def scroll_to_bottom(driver):\n",
      "        # Scroll to the bottom of the page\n",
      "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
      "        # Wait for some time to let the content load\n",
      "        time.sleep(2)\n",
      "    \n",
      "    # Initialize the WebDriver\n",
      "    driver = webdriver.Chrome()  # Assuming you have Chrome WebDriver installed\n",
      "    \n",
      "    # Navigate to the Facebook login page\n",
      "    driver.get('https://www.facebook.com')\n",
      "    \n",
      "    # Find the phone number and password fields and input your credentials\n",
      "    phone_field = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'email')))\n",
      "    phone_field.send_keys('your_phone_number')\n",
      "    \n",
      "    password_field = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'pass')))\n",
      "    password_field.send_keys('your_password')\n",
      "    \n",
      "    # Find and click the login button\n",
      "    login_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.NAME, 'login')))\n",
      "    login_button.click()\n",
      "    \n",
      "    # Wait for the page to load after login\n",
      "    time.sleep(5)\n",
      "    \n",
      "    # Navigate to the desired URL\n",
      "    group_members_url = 'https://m.facebook.com/groups/eBooks.Downloads/members/'\n",
      "    driver.get(group_members_url)\n",
      "    \n",
      "    # Set the time limit for script execution (e.g., 1 hour)\n",
      "    start_time = time.time()\n",
      "    duration = 3600  # 1 hour in seconds\n",
      "    \n",
      "    # Define the maximum wait time between scrolls\n",
      "    max_wait_time = 10  # seconds\n",
      "    \n",
      "    # Keep scrolling until all members are loaded or time limit is reached\n",
      "    while True:\n",
      "        # Get the current page height\n",
      "        prev_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
      "        # Scroll to the bottom\n",
      "        scroll_to_bottom(driver)\n",
      "        # Wait for some time\n",
      "        time.sleep(2)\n",
      "        # Get the new page height after scrolling\n",
      "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
      "        # If the page height has not increased after scrolling, break the loop\n",
      "        if new_height == prev_height:\n",
      "            break\n",
      "        # If the time limit is reached, break the loop\n",
      "        if time.time() - start_time >= duration:\n",
      "            break\n",
      "    \n",
      "        # Calculate the remaining time\n",
      "        elapsed_time = time.time() - start_time\n",
      "        remaining_time = max(0, duration - elapsed_time)\n",
      "        # Calculate the wait time between scrolls\n",
      "        wait_time = min(max_wait_time, remaining_time)\n",
      "    \n",
      "        # Wait for the specified time\n",
      "        time.sleep(wait_time)\n",
      "    \n",
      "    # Now scrape the content using BeautifulSoup\n",
      "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
      "    \n",
      "    # Find all anchor tags containing member names\n",
      "    member_links = soup.find_all('a', class_='x1i10hfl x1qjc9v5 xjbqb8w xjqpnuy xa49m3k xqeqjp1 x2hbi6w x13fuv20 xu3j5b3 x1q0q8m5 x26u7qi x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xdl72j9 x2lah0s xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r x2lwn1j xeuugli xexx8yu x4uap5 x18d9i69 xkhd6sd x1n2onr6 x16tdsg8 x1hl2dhg xggy1nq x1ja2u2z x1t137rt x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m x1q0g3np x87ps6o x1lku1pv x1rg5ohu x1a2a7pz')  # Adjust class name accordingly\n",
      "    \n",
      "    # Filter href attributes and write them to the file\n",
      "    with open('group_members.txt', 'w', encoding='utf-8') as file:\n",
      "        for link in member_links:\n",
      "            # Extract the href attribute value directly\n",
      "            href_value = link.get('href')\n",
      "            # Write the href value to the file\n",
      "            file.write(href_value + '\\n')\n",
      "    \n",
      "    # Close the WebDriver session\n",
      "    driver.quit()\n",
      "    \n",
      "    \n",
      "    \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cn6vja/facebook_user_id/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping an Angular Website\n",
      "Text: Hello folks, I need to do some scraping for a project in my job, and I basically stumbled in a website made with AngularJS. Any suggestions or tips for the best way to scraping data?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cnf7oa/scraping_an_angular_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there a workaround to run extensions on Selenium's headless mode?\n",
      "Text: Hello, good day everyone.\n",
      "\n",
      "Has anyone been able to find a workaround to run extensions on Selenium's headless mode?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cn8nf3/is_there_a_workaround_to_run_extensions_on/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Extracting content from highly dynamic html files\n",
      "Text: How do you effectively extract content from highly dynamic html files? Pretty much every solution I have read about requires understanding class names or something. I have tried many things but have yet to find a silver bullet. Would love to hear how someone else does it. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cmr0vv/extracting_content_from_highly_dynamic_html_files/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Guidance On Walmart GraphQL Product Review Scraping?\n",
      "Text: Hello Everyone! I am partially new to web scraping and I was stuck when encountering GraphQL requests and responses. I understand normal URL scraping but I can't seem to get the code correct on the correct schema, header etc. Any advice and code would be great! I am trying to fetch review text from a Walmart product. I have done some digging and wrote some code but all of my attempts failed but at least I have made some effort. :)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cmptm4/guidance_on_walmart_graphql_product_review/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Exists this kind of tool?\n",
      "Text: Hi, usually I build a lot of new scrapers for clients who ask me to scrape specific product pages, retrieve data from maps, or extract all products from a page, and so on.\n",
      "\n",
      "When I start a new project, I typically begin by sending a curl request to the target page to see if it returns the entire response, or if it uses JavaScript loading. I also check if the API is open and if they have sitemaps.xml or sitemaps-us.xml files.\n",
      "\n",
      "My question is, does a tool exist that can do all of this automatically, like a basic scouting or recognition of the page, and return the response or store the XML/HTML files, including cookies and so on?\n",
      "\n",
      "Similar to the nmap  \n",
      " command, which can run basic scripts to identify open ports on a server, detect the technology or host being used, and so on.\n",
      "\n",
      "If such a tool doesn't exist, I'd like to create it and make it open-source. However, I don't want to waste my time if a similar tool already exists, so I'm wondering if someone with more expertise has already developed a tool like this in the past??\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cmiwzp/exists_this_kind_of_tool/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: YouTube channel scraping\n",
      "Text: I’m looking for a way to scrape YouTube searches for a list of channels. Basically all I want to do is to be able to search a specific topic (tech or golf for example) and then just get a list of all the channels that show up with over 20k subscribers. I’m a complete beginner and I don’t know the first thing about coding or anything so any help would be greatly appreciated.\n",
      "\n",
      "If I could also filter by only English speaking channels that would be very helpful too.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cmpon3/youtube_channel_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Python, BeautifulSoup: div class returns None even though there is text underneath? \n",
      "Text: Hey all,  \n",
      "hope all is well.\n",
      "\n",
      "I am fairly new to this, at the moment, I am trying to scrape a listing online, \n",
      "\n",
      "here is my code,\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    \n",
      "    url = '*link to listing*'\n",
      "    \n",
      "    html = requests.get(url)\n",
      "    detailed_soup = BeautifulSoup(html.text, \"html.parser\")\n",
      "    div = detailed_soup.find('span', class_='x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u')\n",
      "    \n",
      "    print(div)\n",
      "\n",
      "This returns None when printed, even though there is text underneath. \n",
      "\n",
      "The HTML is below.  \n",
      "Thanks for your help.\n",
      "\n",
      "`<div class=\"x1gslohp\"><div class=\"xexx8yu x1pi30zi x18d9i69 x1swvt13\"><div class=\"xz9dl7a x4uap5 xsag5q8 xkhd6sd x126k92a\"><div><span class=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u\" dir=\"auto\">Hello everyone,`\n",
      "\n",
      "`Selling my 2011 Honda Pilot, no issues with the vehicle what so ever.`\n",
      "\n",
      "`•Oil changes were performed every 5k miles.`\n",
      "\n",
      "`•Decent tires on the car`\n",
      "\n",
      "`Questions &amp; concerns feel free to reach out<!-- --> <div class=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m x1n2onr6 x87ps6o x1lku1pv x1a2a7pz\" role=\"button\" tabindex=\"0\"><span class=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x6prxxf xvq8zen x1s688f xzsf02u\">See less</span></div></span></div></div><div class=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m x1n2onr6 x87ps6o x1lku1pv x1a2a7pz\" role=\"button\" tabindex=\"0\"><div class=\"x1lq5wgf xgqcy7u x30kzoy x9jhf4c x6ikm8r x10wlt62 x1n2onr6\"><div><div class=\"x1n2onr6 xqtp20y x6ikm8r x10wlt62\" style=\"padding-top:calc(120 / 328 * 100%)\"><div class=\"x10l6tqk x17qophe x13vifvy\" style=\"background-image:url(https://external-lga3-1.xx.fbcdn.net/static_map.php?v=2052&amp;theme=default&amp;ccb=4-4&amp;size=328x120&amp;language=en_US&amp;scale=1&amp;zoom=11&amp;center=40.794982910156%2C-74.163208007812&amp;circle=weight%3A2%7Ccolor%3A0x4D6AA47f%7Cfillcolor%3A0x4D6AA41c%7C40.794982910156%2C-74.163208007812%7C2k&amp;_nc_client_id=marketplace_post_permalink&amp;_nc_client_caller=MarketplaceStaticMap.react);background-repeat:no-repeat;background-size:100% 100%;height:100%;width:100%\"></div><!--$--><div class=\"x71s49j x10l6tqk xmbx2d0 x70y0r9\"><div aria-haspopup=\"menu\" aria-label=\"View Map Info\" class=\"x1i10hfl x1qjc9v5 xjqpnuy xa49m3k xqeqjp1 x2hbi6w x9f619 x1ypdohk xdl72j9 x2lah0s xe8uvvx x2lwn1j xeuugli x16tdsg8 x1hl2dhg xggy1nq x1ja2u2z x1t137rt x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m x1q0g3np x87ps6o x1lku1pv x1a2a7pz xjyslct xjbqb8w x13fuv20 xu3j5b3 x1q0q8m5 x26u7qi x972fbf xcfux6l x1qhh985 xm0m39n x3nfvp2 xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x1n2onr6 x3ajldb x194ut8o x1vzenxt xd7ygy7 xt298gk x1xhcax0 x1s928wv x10pfhc2 x1j6awrg x1v53gu8 x1tfg27r xitxdhh\" role=\"button\" tabindex=\"0\"><i data-visualcompletion=\"css-img\" class=\"x1b0d499 xuo83w3\" style=\"background-image:url('https://static.xx.fbcdn.net/rsrc.php/v3/yS/r/BuZXp8KJR0F.png');background-position:0 -701px;background-size:auto;width:16px;height:16px;background-repeat:no-repeat;display:inline-block\"></i><div class=\"x1ey2m1c xds687c x17qophe xg01cxk x47corl x10l6tqk x13vifvy x1ebt8du x19991ni x1dhq9h xzolkzo x12go9s9 x1rnf11y xprq8jg\" role=\"none\" data-visualcompletion=\"ignore\"></div></div></div><!--/$--></div></div></div><div class=\"x1ey2m1c xds687c x17qophe xg01cxk x47corl x10l6tqk x13vifvy x1ebt8du x19991ni x1dhq9h x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m\" role=\"none\" data-visualcompletion=\"ignore\" style=\"inset: 0px;\"></div></div><div class=\"x14vqqas x11i5rnm xod5an3 x1mh8g0r\"><div><div class=\"x78zum5 xdt5ytf xz62fqu x16ldp7u\"><div class=\"xu06os2 x1ok221b\"><span class=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u x1yc453h\" dir=\"auto\"><span class=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x6prxxf xvq8zen x1s688f xzsf02u\">Belleville, NJ</span></span></div><div class=\"xu06os2 x1ok221b\"><span class=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x676frb x1nxh6w3 x1sibtaa xo1l8bm xi81zsa x1yc453h\" dir=\"auto\"><span class=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1nxh6w3 x1sibtaa xo1l8bm xi81zsa\">Location is approximate</span></span></div></div></div></div></div></div>`\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cmom1e/python_beautifulsoup_div_class_returns_none_even/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping and storing data online\n",
      "Text: I have been assigned a task to scrape a few websites, they mostly have the same data. The output is a CSV file for each website. The scripts are already built, but I am struggling with finding a service that would run the the scripts monthly as well as a storing those files with the scripts, Like how I would go about it offline. Any suggestions would help. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cmbk5y/scraping_and_storing_data_online/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Outage map URL scrapping\n",
      "Text: Greetings,\n",
      "\n",
      "I am involved with a work project that takes a [large map database](https://atlas.eia.gov/datasets/f4cd55044b924fed9bc8b64022966097/explore) of power companies, and I edit the map data in QGIS. All the URLs in the map database point to the power company outage map.\n",
      "\n",
      "I want to be able to scrape outage URLs to get the direct link to the outage map on some site, and use a URL pointing directly to the map, eliminating all the other distractions on the outage page.\n",
      "\n",
      "Example: [https://www.wwvremc.com/outage-center/](https://www.wwvremc.com/outage-center/)\n",
      "\n",
      "Is there CLI linux scappers, or online tools that do such a task?\n",
      "\n",
      "  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cmcprl/outage_map_url_scrapping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Daily google search volume using Pytrends\n",
      "Text: I am trying to obtain the daily search volume of certain keywords (basically company names from NASDAQ100 and NZX50) for the period from 15 Dec 2021 until 31 March 2024 for regions NZ and Aus. I am using pytrends and have included the python code to have 60 seconds interval and query in blocks of 90days.  Long story short, I got the results for NZX50 companies and it kinda matches with the Google trends website results. But when I did the same for NASDAQ100 companies, the search volumes do not match with google trends website. I see search volume showing for big companies like apple, netflix, alphabet etc. while for the other companies the volume shows zero. I was looking online and understand one possible explanation is cos Google may have scaled the results. But if so, is there a way to get absolute search volume? Or is this because of something else? Can someone help?  \n",
      "TIA!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cm2ym9/daily_google_search_volume_using_pytrends/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: API scraping\n",
      "Text: I'm not sure if I'm on the correct sub, so call me out if that's not the case. I want to scrap every data on the Nutritionix API but it's clearly forbidden in their ToS. What do I risk if I get caught and how do I make it not obvious? They offer a free API key for non commercial use (which is what I want), so I'm not really losing anything if I'm just banned except access to their data I guess\n",
      "URL: https://i.redd.it/i2zjbg2gdtyc1.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Hi, is there a equivalent to selenium-stealth for Java ?\n",
      "\n",
      "Text: I see a lot of topics about avoiding detection for python but less for Java. What are the best practices ? I have a particular interest in e-commerce snipping. Apprciate your help\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cluwnl/hi_is_there_a_equivalent_to_seleniumstealth_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Wizzair old version apk working on rooted device\n",
      "Text: \n",
      "URL: /r/scrapingtheweb/comments/1cle9zh/wizzair_old_version_apk_working_on_rooted_device/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Why shouldn’t I use Crawlee?\n",
      "Text: I just stumbled across [Crawlee](https://crawlee.dev) and, as an avid Python developer I wondered why I would ever use the Requests, BeautifulSoup, or Selenium libraries again if Crawlee seems to handle all the complex (to me) stuff like proxies and headless browsers. I suppose the benefit of Selenium is that you can get at data locked behind a login, and I’m now entirely sure Crawlee doesn’t do that. The other obvious drawback is you need to use JavaScript or TypeScript. \n",
      "\n",
      "What do y’all think? Am I missing something?\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cl9o3y/why_shouldnt_i_use_crawlee/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Walmart groceries canada\n",
      "Text: I'm looking to scrape Walmart groceries in Canada (all grocery/food products) and scrape them atleast once a week, if not more often.\n",
      "\n",
      "I'm scraping some other sites and I haven't needed to use any proxy services so far but from my research on Reddit, Walmart has pretty good bot detection. \n",
      "\n",
      "Last thing I want is to have my home IP blocked by Walmart, which would make local development a pain.\n",
      "\n",
      "I would love to hear from others who have or are scraping Walmart and figure what I can do to avoid getting blocked.\n",
      "\n",
      "1. Do I need proxy services \n",
      "2. Any information on rate limiting myself and how often is too often\n",
      "3. Do I use their graphql endpoint to scrape or use a headless/headful browser?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cl4wpd/scraping_walmart_groceries_canada/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Extract Pages from DLsite With 100% Quality\n",
      "Text: I purchased a manga on DLsite, but it's stream only so I can't download it. How do I extract its pages without losing quality?\n",
      "\n",
      "[https://www.dlsite.com/comic/work/=/product\\_id/BJ348612.html](https://www.dlsite.com/comic/work/=/product_id/BJ348612.html)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cl57wv/extract_pages_from_dlsite_with_100_quality/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping sportingindex to  get match urls\n",
      "Text: My purpose is to find match name and match specific urls in the site: [https://www.sportingindex.com/sports/en-GB/spread-betting/football/footballpopular/spr/c57](https://www.sportingindex.com/sports/en-GB/spread-betting/football/footballpopular/spr/c57)\n",
      "\n",
      "def get\\_match\\_urls(league\\_url):\n",
      "\n",
      "match\\_data = \\[\\]\n",
      "\n",
      "# Start a Selenium WebDriver session for Chrome\n",
      "\n",
      "driver = webdriver.Chrome()\n",
      "\n",
      "\n",
      "\n",
      "# Navigate to the league URL\n",
      "\n",
      "driver.get(league\\_url)\n",
      "\n",
      "\n",
      "\n",
      "try:\n",
      "\n",
      "# Wait for the cpi\\_\\_name elements to be present\n",
      "\n",
      "WebDriverWait(driver, 10).until(EC.presence\\_of\\_element\\_located((By.CLASS\\_NAME, 'cpi\\_\\_name')))\n",
      "\n",
      "\n",
      "\n",
      "# Find all elements with class \"cpi\\_\\_name\"\n",
      "\n",
      "match\\_elements = driver.find\\_elements(By.CLASS\\_NAME, 'cpi\\_\\_name')\n",
      "\n",
      "\n",
      "\n",
      "# Iterate over each match element\n",
      "\n",
      "for element in match\\_elements:\n",
      "\n",
      "match\\_name = element.text\n",
      "\n",
      "# Click on the match element to get the link\n",
      "\n",
      "element.click()\n",
      "\n",
      "# Get the current URL, which should be the URL of the match page\n",
      "\n",
      "match\\_url = driver.current\\_url\n",
      "\n",
      "match\\_data.append((match\\_name, match\\_url))\n",
      "\n",
      "# Go back to the league page to continue iterating over other matches\n",
      "\n",
      "driver.back()\n",
      "\n",
      "\n",
      "\n",
      "# Click on the specific element to get the second link\n",
      "\n",
      "specific\\_element = driver.find\\_element(By.XPATH, \"//div\\[@class='cpi\\_\\_name'\\]\\[contains(text(), 'Fulham v Man City')\\]\")\n",
      "\n",
      "specific\\_element.click()\n",
      "\n",
      "specific\\_match\\_url = driver.current\\_url\n",
      "\n",
      "match\\_data.append((\"Fulham v Man City\", specific\\_match\\_url))\n",
      "\n",
      "\n",
      "\n",
      "except Exception as e:\n",
      "\n",
      "print(\"Error:\", e)\n",
      "\n",
      "\n",
      "\n",
      "# Close the WebDriver session\n",
      "\n",
      "driver.quit()\n",
      "\n",
      "\n",
      "\n",
      "return match\\_data\n",
      "\n",
      "\n",
      "\n",
      "# Example usage:\n",
      "\n",
      "league\\_url = \"https://www.sportingindex.com/sports/en-GB/spread-betting/football/league/47/spr/c57\"\n",
      "\n",
      "match\\_data = get\\_match\\_urls(league\\_url)\n",
      "\n",
      "print(\"Match Data:\")\n",
      "\n",
      "for name, link in match\\_data:\n",
      "\n",
      "print(\"Name:\", name)\n",
      "\n",
      "print(\"Link:\", link)\n",
      "\n",
      "output:  \n",
      "name of match 1  \n",
      "link: \"league\\_url\"\n",
      "\n",
      "Not only thee loop is unsuccessful, but running only once.  \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cl057y/scraping_sportingindex_to_get_match_urls/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Proxy Management for Web Scraping Project\n",
      "Text: My project involves accessing a specific website that contains product information and extracting data from it.\n",
      "\n",
      "# User Blocking Prevention\n",
      "\n",
      "1. This website requires users to sign up to view the site's information.\n",
      "\n",
      "Therefore, I need to log in, but if a user attempts to access the site from various IP addresses instead of a single fixed IP, problems may arise.\n",
      "\n",
      "For example, let's say a user accessed the site from China one second ago and then from the United States the next second. Such a user would likely be blocked.\n",
      "\n",
      "Consequently, it is necessary to maintain a specific IP address to a certain extent.\n",
      "\n",
      "1. Additionally, if a user attempts to access the website too frequently using a single user ID, there is a possibility of getting blocked.\n",
      "\n",
      "I have created multiple user IDs on the target website.\n",
      "\n",
      "Each ID should access the website through a different IP address.\n",
      "\n",
      "In summary:\n",
      "\n",
      "* I need the ability to freely create around 100 to 300 proxies and remove the created proxies immediately when desired by the user.\n",
      "* The created proxies (IP addresses) should be maintained for a duration specified by the user and should be reusable.\n",
      "\n",
      "# Usage\n",
      "\n",
      "More than 6,000 requests occur each month.\n",
      "\n",
      "Each request is only used until the corresponding web page is loaded.\n",
      "\n",
      "# Scraping Method\n",
      "\n",
      "I use Python and Selenium for web scraping.\n",
      "\n",
      "(To log in to the website, I maintain cookie data using the pickle module.\n",
      "\n",
      "---\n",
      "\n",
      "Thank you for taking the time to read through my post. I would greatly appreciate any advice, recommendations, or insights you can provide 😊\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ckit0r/proxy_management_for_web_scraping_project/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I can't figure out how to decode this response\n",
      "Text: \n",
      "URL: https://i.redd.it/kwb0siwqzgyc1.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Which is better for long-term, production-level data extraction on OnlyFans - API Sniffing or Webscraping?\n",
      "Text: I am heavily torn between these two methodologies in building out an in-house data extraction pipeline for an OnlyFans-related SaaS.\n",
      "\n",
      "The system needs to extract the currently logged in user's data, such as their chats, sales, etc... (all with their explicit permission, it is what the app does). Everything is occurring in OnlyFans' website, and I am not yet aware of how they handle this entire dynamic - Are they strict? Is one method much more prone to detection/account banning than the other?\n",
      "\n",
      "API Sniffing is obviously much more performant, and I would absolutely love to use it as the method when it comes down to being able to scale and handle more data, programmatically so much cleaner, resulting in lower user wait times, but I am very worried the result of its possible detection will be much more swift and brutal than going the scraping route.\n",
      "\n",
      "Webscraping, being the more common approach, seems to have more possible approaches and defenses, and unsurprisingly much less performant in my tests.\n",
      "\n",
      "This data extraction is the core of the SaaS, so the method being as scalable, stable, and cost-effective as possible is extremely important in this decision.\n",
      "\n",
      "I plan on using robust proxy/ip rotating, etc... alongside both methods.\n",
      "\n",
      "Would love any thoughts or perspective.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ck6xee/which_is_better_for_longterm_productionlevel_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selinium and chromedriver: Yahoo finance detecting that I'm scraping\n",
      "Text: Hi, \n",
      "\n",
      "So currently scraping yahoo finance. When scraping I have to use their search bar on the main page. However they seem to be detecting that I'm scraping somehow, which causes java.net.SocketException: Connection reset. Is there anyway of getting around this?\n",
      "\n",
      "These are the options for my chromedriver:  \n",
      "Changing the page load strategy doesn't work. (normal and none)\n",
      "\n",
      "    options.addArguments(\"disable-infobars\");\n",
      "                options.addArguments(\"--disable-extensions\");\n",
      "                options.addArguments(\"--disable-gpu\");\n",
      "                options.addArguments(\"--disable-dev-shm-usage\");\n",
      "                options.addArguments(\"--no-sandbox\");\n",
      "                options.addArguments(\"blink-settings=imagesEnabled=false\");\n",
      "                options.addArguments(\"--headless\");\n",
      "                options.addArguments(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537\");\n",
      "                options.setPageLoadStrategy(PageLoadStrategy.EAGER);\n",
      "    \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cjyijy/selinium_and_chromedriver_yahoo_finance_detecting/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Page Error. Input field not being properly captured\n",
      "Text: I have an input field where I insert strings like this. When i click submit on the page through js, it's not working for some reason. I'm getting a field error \"The field Job Title is required and must have a value\". Tried events like down below, and clicks but it's not working. Anyone know how to fix this?\n",
      "\n",
      "        if (jobDescriptionInput) {\n",
      "            jobDescriptionInput.value = jobDescriptions[i];\n",
      "            jobDescriptionInput.dispatchEvent(new Event('input', { bubbles: true }));    }\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ck22bh/page_error_input_field_not_being_properly_captured/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Falsely flagged for web scraping\n",
      "Text: Hey everyone,\n",
      "\n",
      "  \n",
      "A device on my network (I'm assuming) is possibly web scraping because i keep getting access denied to many of the popular websites I go to. Like costco, homedepot, lowes, etc...\n",
      "\n",
      "Is there anyway for me to monitor this and see what device is causing this issue? I've tried contacting my ISP and they have been no help. I ran anti-virus scans on all laptops and PCs and got nothing, cleared caches, tried many different browsers. I am not web scraping and if a device on my network is i want to stop it.\n",
      "\n",
      "  \n",
      "Thanks for your assistance, if this type of question is not allowed feel free to block post\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cjqhpx/falsely_flagged_for_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: are levels.fyi and h1bdata.info scrapable? \n",
      "Text: i just started out so im not sure if my output is because of my code or im just denied, if they’re not, do you recommend any websites like them which i can scrape salary data from? its for a uni assignment\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cjvrck/are_levelsfyi_and_h1bdatainfo_scrapable/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Getting Page Error after button.click()\n",
      "Text: Hey Y'all. Can I get some help pls? Getting a Page error because of this code for some reason:\n",
      "\n",
      "if (button) { button.click() } - button exists and is getting clicked. Implementing this through an extension\n",
      "\n",
      "Idk why it's happening. Here's the specific error: Page Error Internal Server Error. (id: VPS|eb58994b-3c1c-43a5-bbfd-de3114256464) - Does anybody know what to do? Same for the other buttons on the page I'm working with:\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cjp5m2/getting_page_error_after_buttonclick/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to optimize and stabilize an OnlyFans Webscraper?\n",
      "Text: I've built a tool for OF models and agencies to use, which relies heavily on my extension/backend webscraping data from their profile such as - their fans, amounts paid, etc...\n",
      "\n",
      "Existing scrapers are not conducive to scraping private account data - where the bot utilizes user creds to gain access, then scrape.\n",
      "\n",
      "Currently it is inefficient and users have to wait 10-20 seconds for most features to complete, as I currently have scraping occuring on an on-demand basis, rather than storing their data from previous scrapes in a DB.\n",
      "\n",
      "Storing in a DB is an option, as our user agreement and ourselves clearly outline that, so perhaps utilizing that plus memoization would greatly improve user wait times for features such a mass messaging, and other data-heavy tasks.\n",
      "\n",
      "Also considering utilizing AWS for improved compute power.\n",
      "\n",
      "I'm not very experienced in this gray-area scene of webscraping as far as legalities go, so feel free to educate me on what my risks are here as well. Would really appreciate it. (I am aware of OF ToS)\n",
      "\n",
      "Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cjb1ub/how_to_optimize_and_stabilize_an_onlyfans/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help Needed: Scraping Data from Heritage Foundation Website\n",
      "Text: Hi everyone,\n",
      "\n",
      "I'm an economics student trying to extract data from a dynamic table on the Heritage Foundation website. The \"Download\" feature on the website doesn't provide data for all combinations of years and regions, so I'm hoping to scrape the data directly.\n",
      "\n",
      "**Website URL**: [https://www.heritage.org/index/pages/all-country-scores](https://www.heritage.org/index/pages/all-country-scores)\n",
      "\n",
      "\n",
      "\n",
      "**Data Points**: I need data for all available variables, for all years and regions (all countries). These are the names of the variables:\n",
      "\n",
      "Name\n",
      "\n",
      "Index Year\n",
      "\n",
      "Overall Score\n",
      "\n",
      "Property Rights\n",
      "\n",
      "Government Integrity\n",
      "\n",
      "Judicial Effectiveness\n",
      "\n",
      "Tax Burden\n",
      "\n",
      "Government Spending\n",
      "\n",
      "Fiscal Health\n",
      "\n",
      "Business Freedom\n",
      "\n",
      "Labor Freedom\n",
      "\n",
      "Monetary Freedom\n",
      "\n",
      "Trade Freedom\n",
      "\n",
      "Investment Freedom\n",
      "\n",
      "Financial Freedom\n",
      "\n",
      "\n",
      "\n",
      "**Project Description**: I need to extract the above-mentioned data for economic analysis for a class research project.\n",
      "\n",
      "I have some experience in statistical analysis using R and basic programming skills in Python, but I'm new to web scraping. Any guidance or assistance on how to scrape this data effectively would be greatly appreciated.\n",
      "\n",
      "Thank you in advance for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ciz9au/help_needed_scraping_data_from_heritage/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: My friend and I would like to dress up as stereotypical tourists to our area. I’d like to scrape Instagram public check-ins & use AI to generate the most accurate photo to best him \n",
      "Text: So I would like to use a tool to amalgamate Instagram public check-ins at all bars & restaurants, plus using these businesses official pages as well. \n",
      "\n",
      "Then, when I have the data, I would like to run it through AI to generate a handful of images.\n",
      "\n",
      "I don’t know where to begin, but what webscraping tool would be good for this?\n",
      "\n",
      "Do you think I could just narrow it by US Zip code and it would be able to find good photos? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cishl1/my_friend_and_i_would_like_to_dress_up_as/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Google Translations using Selenium\n",
      "Text: This is rather gonna be a short one. I was wondering, what is the point of paying Google Translation API, when you can simply scrape of the translations without paying anything? Of course this comes with a small performance tradeoff, but it's still doable right?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cif6nr/scraping_google_translations_using_selenium/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What does it mean when you see this in an output file?\n",
      "Text: I am attempting to scrape a site's sitemap.xml using bs4, requests and python3. When I attempt to crawl the XML tree of the sitemap I am not getting any results, so I grabbed response.text from request and it wrote the following code to a temp text file.\n",
      "\n",
      "    �\u001b�dSS�>Þ�\u000eK}~���N�\n",
      "    ����\u0004d����ߕ�C���\u00134�\u0017=Ο�\u0014���p��E��\fu��\u001d�؎�o�n[K�\u0005\u0019���Ẁ�&��n�)n\u0014�\u0003��J��u\u0018����9���g�u�T��\u000f\n",
      "    r�x�r�n��s����o��q|3������=`�@Itzxr��K�j�\u0006=����۲������?��e��.%b���g㴝?m�x\u000e��8\n",
      "    U�eJ\u0005e���z�\u0013m�����)\u0014\u0014^ޜ�q\u001aj]h\u001a��\u0001n�)|�S\u001c�_28��}�{x��j\f�6�(�\u0017�\u0014e\u0007�sm�    �Pc\"�E�8\u0004e��\n",
      "    CH��w�d\n",
      "    Ŝ�\u000b��'�3\u0013�i���C\u001c_ؒ�\u0012p0ꒌ�~X\u0016d[M��\u001fc\u001a����)ͽ��^���^��F�^vL\u0014\u0010�t3�\u0011\u0018�2��\u0015����Z�q�����$э�1���Y\u0014�+��6l��؃�\u001d�]�J9~\f\u0016!\u0013�i���:\u0015H0�$�\u0004H[�zބ��\n",
      "    ټ�\u0005\u001b\u00052��L��Y\u00052�\u000e�\u0011=\u0013�2��\u0012~��@�a�d�β� 1��ݴi�\n",
      "    �Ȳ׎`�yDK���\n",
      "    1`�(���v`<�v\u0014�Q���Z�L�\u0005m\u0013�IʑM\u0015hQ\u000f����\n",
      "    �\n",
      "\n",
      "This would explain why my code was unable to work itself through the XML on the site, but what would cause the XML to appear like this? When I view the sitemap.xml in the browser, it appears just fine.\n",
      "\n",
      "This is my code to grab the sitemap:\n",
      "\n",
      "        url = 'https://www.website.com/sitemap.xml'\n",
      "    \n",
      "        response = requests.get(url, headers=_headers())\n",
      "    \n",
      "        print(response.status_code)\n",
      "        \n",
      "        if response.status_code == 200:\n",
      "            with open(\"sitemap__.txt\", \"w\") as f:\n",
      "                f.write(response.text)\n",
      "\n",
      "The sitemap\\_\\_.txt file is what contains the text above. Any ideas?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cinkdd/what_does_it_mean_when_you_see_this_in_an_output/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Tw*tter Images\n",
      "Text: Hi! I had just started scraping when Tw*tter decided to change their rules and make it just that tiny bit harder to scrape accounts. I abandoned what I was doing in favor of other projects and just came back around to it.\n",
      "\n",
      "I specifically want to grab the images only of specific accounts for use in SD checkpoints/Loras etc. \n",
      "\n",
      "Is there a free way to do this? \n",
      "I tried searching and I only get older links.\n",
      "I don’t need someone to hold my hand I don’t think, but I’d just like to be pointed in the right direction. \n",
      "Thank you!\n",
      "\n",
      "(Sorry for the censorship, I’m a Facebook refugee and it’s typical etiquette in the groups I frequent.)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cipml2/twtter_images/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Crawling for specific HTML string... (Warning, I'm Dumb)\n",
      "Text: I'm trying to accomplish what seems like it should be a simple task at work. We have a client website where we need to inventory ALL forms on the site. There have been a variety of forms implemented over the years from native forms to embed forms from platforms like Cognito, Wufoo, Mail Chimp, etc. I need to find and catalogue all of them.\n",
      "\n",
      "Because of the unknowns, I can't just scrape for the embed codes of specific platforms, as I'll surely miss the unknown ones, and I can't just crawl for the word \"form\" as that will just get me a million results of pages that have the word form, instead of a form.\n",
      "\n",
      "After inspecting a sampling of known forms, I have noticed that ALL of them have a common HTML string - *method=\"post\".*\n",
      "\n",
      "I tried using Sitebulb to crawl the site, but it apparently can't look for specific strings, only words. So I could search for \"method\" or \"post\", but not method=\"post\".\n",
      "\n",
      "I've been googling all afternoon trying to find a no-code platform (remember, I'm dumb) that can do this, but I'm having no luck. I'm sure there are multiple platforms that can do this, but I'm not finding any that explicitly advertise this use case on their website.\n",
      "\n",
      "Anybody know of a platform or simple method to accomplish this?\n",
      "\n",
      "https://preview.redd.it/ulz8ls9b4xxc1.png?width=671&format=png&auto=webp&s=431188b8e3784d090a06666c0299d0d0ef95bcb6\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ci3myj/crawling_for_specific_html_string_warning_im_dumb/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping tripadvisor\n",
      "Text: Hi everyone, I'm trying to scrape tripadvisor with python and selenium but everytime I'm trying to connect I'm detected ad a bot, someone have some advice to avoid it?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1chu1s3/scraping_tripadvisor/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Monthly Self-Promotion Thread - May 2024\n",
      "Text: Hello and howdy, digital miners of /r/webscraping!\n",
      "\n",
      "The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n",
      "\n",
      "* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n",
      "* Maybe you've got a ground-breaking product in need of some intrepid testers?\n",
      "* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n",
      "* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n",
      "\n",
      "Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n",
      "\n",
      "Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1chjc0v/monthly_selfpromotion_thread_may_2024/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ideas on how to route traffic through my self-hosted proxy manager (Using Selenoid + Scraping Ticketmaster)\n",
      "Text: Goal: Scraping quickpicks from Ticketmaster event\n",
      "\n",
      "General system overview:\n",
      "\n",
      "My scraping system is integrated into an Airflow+Airbyte pipeline based off of this project (used it as a template starting point and modified to my needs): [https://github.com/nialloriordan/airbyte-airflow-scraper](https://github.com/nialloriordan/airbyte-airflow-scraper)\n",
      "\n",
      "Details:\n",
      "\n",
      "I am self hosting a proxy manager, which I want traffic routed through when I scrape my target pages.\n",
      "\n",
      "I am using Selenoid implementation of Selenium Hub. I am using a patched undetected\\_chromedriver: [https://gist.github.com/walidmujahid/b1dcfec64aaa8ed73b47deb64f5a4035](https://gist.github.com/walidmujahid/b1dcfec64aaa8ed73b47deb64f5a4035)\n",
      "\n",
      "Before using Selenoid, I would run the container standalone and I was successfully following this to use iptables and redsocks to route traffic to my proxy manager: [https://stackoverflow.com/questions/70971310/how-to-make-docker-container-connect-everything-through-proxy](https://stackoverflow.com/questions/70971310/how-to-make-docker-container-connect-everything-through-proxy)\n",
      "\n",
      "However, after integrating into Selenoid, this no longer functions as I have not figured out, if it is possible at all without making my own fork of Selenoid, how to be able to use it as I need the NET\\_ADMIN cap for the browser containers, and probably a couple other things that I don't seem to be able to set in the browsers.json file.\n",
      "\n",
      "So, instead of redsocks and iptables, I tested out using an extension; specifically, this: [https://github.com/ultrafunkamsterdam/undetected-chromedriver/issues/1306#issuecomment-1581842980](https://github.com/ultrafunkamsterdam/undetected-chromedriver/issues/1306#issuecomment-1581842980)\n",
      "\n",
      "This works in routing traffic through my proxy manager, but it seems to interfere with quickpicks being loaded in the scroll area, which (going by the screenshots) is not being loaded at all, which I need so I can scroll and the next quickpicks json batch from the network logs.\n",
      "\n",
      "I am hoping to get some ideas that can get me up and running (be that some way to use redsocks and iptables, a different extension, or something else). After searching google, I did come across this: [https://github.com/aerokube/selenoid/issues/435](https://github.com/aerokube/selenoid/issues/435) which gave me an idea of potentially using a vpn, but I've never used a vpn before and I am not sure how that would function and what complexities would be involved.\n",
      "\n",
      "  \n",
      "UPDATE May 13, 2024:\n",
      "\n",
      "1. I have successfully gotten the chrome extension to work. I found this and copied from this (using manifest v2 instead of v3):  [https://proxyempire.io/chromedriver-proxy-with-selenium-using-python/](https://proxyempire.io/chromedriver-proxy-with-selenium-using-python/)\n",
      "\n",
      "The extension code looks like this (I am just attaching the volume to my uc\\_chrome docker since I am calling selenoid is handling the launch of browser containers):\n",
      "\n",
      "  \n",
      "manifest.js:\n",
      "\n",
      "`{`\n",
      "\n",
      "`\"version\": \"1.0.0\",`\n",
      "\n",
      "`\"manifest_version\": 2,`\n",
      "\n",
      "`\"name\": \"Chrome Proxy\",`\n",
      "\n",
      "`\"permissions\": [`\n",
      "\n",
      "`\"proxy\",`\n",
      "\n",
      "`\"tabs\",`\n",
      "\n",
      "`\"unlimitedStorage\",`\n",
      "\n",
      "`\"storage\",`\n",
      "\n",
      "`\"<all_urls>\",`\n",
      "\n",
      "`\"webRequest\",`\n",
      "\n",
      "`\"webRequestBlocking\",`\n",
      "\n",
      "`\"webRequestAuthProvider\"`\n",
      "\n",
      "`],`\n",
      "\n",
      "`\"background\": {`\n",
      "\n",
      "`\"service_worker\": \"background.js\"`\n",
      "\n",
      "`},`\n",
      "\n",
      "`\"minimum_chrome_version\": \"22.0.0\"`\n",
      "\n",
      "`}`\n",
      "\n",
      "background.js:\n",
      "\n",
      "  \n",
      "`var config = {`\n",
      "\n",
      "`mode: \"fixed_servers\",`\n",
      "\n",
      "`rules: {`\n",
      "\n",
      "`singleProxy: {`\n",
      "\n",
      "`scheme: \"http\",`\n",
      "\n",
      "`host: \"104.236.74.186\",`\n",
      "\n",
      "`port: parseInt(24000)`\n",
      "\n",
      "`},`\n",
      "\n",
      "`bypassList: [\"localhost\"]`\n",
      "\n",
      "`}`\n",
      "\n",
      "`};`\n",
      "\n",
      "\n",
      "\n",
      "`chrome.proxy.settings.set({value: config, scope: \"regular\"}, function() {});`\n",
      "\n",
      "\n",
      "\n",
      "`function callbackFn(details) {`\n",
      "\n",
      "`return {`\n",
      "\n",
      "`authCredentials: {`\n",
      "\n",
      "`username: \"brd-auth-token\",`\n",
      "\n",
      "`password: \"aZcsxNj32AfWrgrBSPV6GWWrUnVFbtSm\"`\n",
      "\n",
      "`}`\n",
      "\n",
      "`};`\n",
      "\n",
      "`}`\n",
      "\n",
      "\n",
      "\n",
      "`chrome.webRequest.onAuthRequired.addListener(`\n",
      "\n",
      "`callbackFn,`\n",
      "\n",
      "`{urls: [\"<all_urls>\"]},`\n",
      "\n",
      "`['blocking']`\n",
      "\n",
      "`);`\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "2.  For anyone interested, while I was scouring the web, I came acorss this idea: [https://techetio.com/2023/12/24/a-seamless-authenticated-proxy-solution-for-selenium/](https://techetio.com/2023/12/24/a-seamless-authenticated-proxy-solution-for-selenium/)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1chiwy0/ideas_on_how_to_route_traffic_through_my/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is anyone currently pulling from a private Facebook Group in 2024?\n",
      "Text: >>\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1chqd6w/is_anyone_currently_pulling_from_a_private/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: We scrape friendly website that hosts company info?\n",
      "Text: As the title says, is there a website such as LinkedIn or Y Combinator that hosts a large amount of company information that is webscrape friendly?\n",
      "\n",
      "If not has anyone had success with LinkedIn/Y combinator in getting access to scrape?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1chplr0/we_scrape_friendly_website_that_hosts_company_info/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Avoid Scraping Personal Data?\n",
      "Text: Hello everyone! I am doing a web scraping project, and I would like to avoid scraping personal data as much as possible. Do you have any tips for me?   \n",
      "  \n",
      "My first idea was creating some tags that I can use as filters, but I didn't think very much about it yet. I read on the internet about data anonymization and pseudonymization, about respecting GDPR etc. Any help is greatly appreciated !!\n",
      "\n",
      "I don't know if this is relevant for the context, but I am scraping using BeautifulSoup, Requests and Selenium.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1chmv0w/avoid_scraping_personal_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best database structure for web scraping project\n",
      "Text: I want to build a kind of Social Blade clone. You can look for YouTube channels and see how their stats have changed.\n",
      "\n",
      "Currently I'm performing a daily scrape of the channels and storing all the information in one table.\n",
      "\n",
      "I'm using an Azure Function and Azure SQL Database for this project.\n",
      "\n",
      "Number of videos and number subscribers changes frequently, so I definitely want to track that daily.\n",
      "\n",
      "However, how to best deal with more static data such as display name and channel description? I'm currently scraping this daily as well. It is nice, because easy to spot when a change was made, but off course a lot of duplicate data. \n",
      "\n",
      "I'm currently scraping it all in 1 table, should I split it up and normalize the data? Have a separate table with subscribers and videos which changes daily and then a static table with channel info? What would be best practice?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1chfymh/best_database_structure_for_web_scraping_project/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: A web scraper for backlink detection?\n",
      "Text: I'm interested in creating my own SEO tool and part of this is backlink detection. I'm already aware that I need to follow polite scraping practices but I'm wondering if there's a most efficient way to handle this? I was planning to use this to verify backlinks for authoritative sites as well as protect against negative SEO attacks like SEMRush does. Any advice?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cguurm/a_web_scraper_for_backlink_detection/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is scraping behind a login legal even if it's against the terms of service?\n",
      "Text: Let's say I want to create an app that lists on one page a user's purchase history on different websites e.g. Amazon, eBay, BestBuy, etc. \n",
      "\n",
      "The user provides me with his login credentials to these sites so I can scrape his order history. Scraping is against these websites' terms of service but also terms of service aren't legally binding I imagine. \n",
      "\n",
      "Is it legal for me to scrape user's purchase data behind the login? The obvious answer is to play it safe and just dont do it in case it's illegal but I'm wondering if there are documented cases out there of it being illegal to scrape behind the login\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cgialo/is_scraping_behind_a_login_legal_even_if_its/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help scraping for insta and linkedin\n",
      "Text: I’ve seen people talk about how hard it is tho scrap on these two sites, but I want to learn how to get really good at it. I understand that will take a lot of time but I’m willing to learn.\n",
      "\n",
      "With that being said, I know close to nothing about scraping or coding other than the few hours I’ve spent scrolling through this Reddit. \n",
      "\n",
      "With the long term project I’m trying to create is simply a bot that can gather accounts that have certain attributes I’m looking for, so I can get in contact with them personally. The amount of accounts won’t be many at all and definitely not into the thousands as I wish to send them all personal messages.\n",
      "\n",
      " I don’t know if this will change in the future (how many accounts I wish to find) but I just wished to know how “risky” this idea is. It would like at most 50 or so accounts a day, but that’s being very very generous. Also if anyone can point me towards any beginners guide to scrapping that would be wonderful. Thanks! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ch8x2d/help_scraping_for_insta_and_linkedin/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Finding source of rotating proxy\n",
      "Text:  Hi i recently got some rotating proxies that look like this, they were customizable by region using country codes like RU,MX etc (to let user choose which regions the rotating proxy would cover). Does anyone know where these proxies could be sourced from/how they are made? any information would be greatly appreciated.  \n",
      "\n",
      "\n",
      "https://preview.redd.it/tbph0zn8tlxc1.png?width=943&format=png&auto=webp&s=fe7e24e39c38730e30a6326364a2302c42ecb4b7\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cgqqdp/finding_source_of_rotating_proxy/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is it illegal to make bots that watch websites for updates and possibly inform users when a change occurs?\n",
      "Text:  I wont be selling data directly, I will only be pointing out changes and linking them. Not that am worried about getting sued, since I live in a developing country.\n",
      "\n",
      "More so wondering if it's possible to get access to an MoR, assuming that I have all necessary requirements.\n",
      "\n",
      "MoRs such as Paddle, and LemonSqueezy.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cg7fyr/is_it_illegal_to_make_bots_that_watch_websites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape info from links in a list\n",
      "Text: Hey everyone,\n",
      "\n",
      "I’m having a crack at web scraping, it’s been pretty fun so far, I’m using python - requests, BeautifulSoup4 and pandas\n",
      "\n",
      "So far I’ve been scraping from one webpage but I’d like to take it a step further by following links within a webpage and scraping those.\n",
      "\n",
      "I'm particularly interested in learning how to scrape with the following process:\n",
      "\n",
      "1.Scrape a list of items (such as job posts on Indeed). (extract job title here)\n",
      "\n",
      "2.Navigate to each job postings url and extract the full job description. (Extract full job description here)\n",
      "\n",
      "3.Repeat this process.\n",
      "\n",
      "I've searched for tutorials on this specific workflow, but most resources I've come across only cover scraping job titles without delving into the job descriptions themselves.\n",
      "\n",
      "Could anyone point me towards tutorials or resources that demonstrate this step-by-step process such as a YouTube video? Any help would be greatly appreciated!\n",
      "\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cgcpeb/how_to_scrape_info_from_links_in_a_list/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: New to webscraping, Linkedin in particular and need some advice\n",
      "Text: Say I want to pull about 10000 records from email and account from LinkedIn through scrapping, how long do you think it will take me in terms of computing power, cost, resources, and time?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cg9584/new_to_webscraping_linkedin_in_particular_and/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Interactive Map Website Data\n",
      "Text: I've done some web scraping in the past, primarily with basic HTML websites, but I'm currently facing a challenge with a different kind of project. I'm trying to scrape data from an interactive map ([https://geo.sac-isc.gc.ca/cippn-fnpim/index-eng.html](https://geo.sac-isc.gc.ca/cippn-fnpim/index-eng.html)), and I'm struggling to figure out how and where the data is loaded from.\n",
      "\n",
      "From my previous experiences, I could easily find data embedded directly within the HTML or fetched through simple API requests. However, this map seems to load its data dynamically, and I'm not sure how to proceed with these kinds of structures.\n",
      "\n",
      "Could anyone provide some guidance or tips on how to approach scraping data from this type of interactive map? Any tools or techniques that would be particularly useful in this scenario?\n",
      "\n",
      "Any help would be appreciated.\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cg2mwb/scraping_interactive_map_website_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: gopro.com is detecting playwright requests as bot. Anyone has any idea how to get unblocked?\n",
      "Text: Need help bypassing gopro.com.\n",
      "gopro.com is detecting playwright requests as bots. Every single request is being blocked. I have taken care of basic requirements such as changing the user-agent and I am also using playwright stealth.\n",
      "It still is blocking my requests.\n",
      "Any help here is much apreciated.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cg7f3e/goprocom_is_detecting_playwright_requests_as_bot/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping product details from top E-commerce websites\n",
      "Text: I want to scrape product details like Product name, Brand name, Description, Image, Price, Discount  from E-Commerce stores like Amazon, Flipkart, Myntra etc.. How can I do this using Node.js? I can scrape data from one website using a code snippet specifically for scraping that site, but can we create a code snippet where we can fetch details like this from all the site? Please help! Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cg4wvi/scraping_product_details_from_top_ecommerce/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping Social media sites for Fake profiles, ie phishing profiles \n",
      "Text: Hey All, \n",
      "\n",
      "So i have seen so many mixed bag responses on this, i dont want to scrap the actually pages on sites but what i want to do is try scrap profile names with in a certain requirement.  So same page names / profile names to try weed out fake accounts that are effecting my small startup business , so i can simply submit a list of fake profiles to be banned or reported if that makes sense. I know twitter has locked down a lot of the scraping and have seen an alternative site with the same data as twitter but how up to date is it and is it as effective as scrapping twitter itself, can this still be done and can it be done for the likes of insta etc also ? Any insights or thoughts are much appreciated as i am just at the start of this project !\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cftydu/webscraping_social_media_sites_for_fake_profiles/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to reduce proxy bandwidth usage in playwright?\n",
      "Text: I am using a scraping browser proxy with playwright as I need to bypass captchas and blocks but I get charged based on bandwidth consumption. Most of the sites I visit have unnecessary resources being loaded that aren't relevant to the information I need to scrape like images and videos.\n",
      "\n",
      "What I've tried is intercepting requests and blocking them:\n",
      "\n",
      "      // set up browser session with proxy\n",
      "      await browser.route(\"**/*.{png,jpg,jpeg,webp,svg}\", (route) => route.abort());\n",
      "      await browser.route(/(analytics|fonts)/, (route) => route.abort());\n",
      "      await browser.route(\"**/*.css\", (route) => route.abort());\n",
      "      await browser.route(\"**/*.mp4\", (route) => route.abort());\n",
      "      await browser.route(\"**/*.mp3\", (route) => route.abort());\n",
      "      // visit site do stuff\n",
      "      bandwidthConsumed +=\n",
      "              x.requestBodySize +\n",
      "              x.requestHeadersSize +\n",
      "              x.responseBodySize +\n",
      "              x.responseHeadersSize;\n",
      "     console.log(bandwidthConsumed) // this value is the same regardless of blocking resources or not\n",
      "\n",
      "but it looks like the resources are still being requested and processed by the browser, which means that while they may not be displayed or utilized by playwright, they still consume bandwidth as they are processed by the proxy server and then aborted by Playwright. So, this doesn't help.\n",
      "\n",
      "Does anyone have any tips how I can rreduce bandwidth consumption?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cfpm3u/how_to_reduce_proxy_bandwidth_usage_in_playwright/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape job listings\n",
      "Text: Hey everyone,\n",
      "\n",
      "I'm diving into the world of web scraping and aiming to build a bot that can gather job listings from various websites and display them on my WordPress site. Specifically, I want to pull job postings from sites like Deloitte's career page (https://apply.deloitte.co.uk/UKCareers/) and showcase them on my platform.\n",
      "\n",
      "Here's my plan so far:\n",
      "\n",
      "1. Scanning and Extraction: I need to figure out how to scan the target website and extract the job listings into a structured format, preferably an Excel file.\n",
      "\n",
      "2. Integration with WordPress: Once I have the data, I'll use WP All Import to upload the Excel file to my WordPress site. This will automate the process of adding new job listings and managing existing ones.\n",
      "\n",
      "3. Regular Updates: To keep the job listings fresh, I'll set up the bot to repeat this process weekly, ensuring that I capture any new openings and remove outdated ones.\n",
      "\n",
      "Now, I'm seeking advice on how to tackle step 1. I understand that different websites may require different scraping methods, and I'm open to using frameworks or any tips you guys might have.\n",
      "\n",
      "While I'm aware of existing job boards and aggregators, I'm passionate about taking on this project myself and customizing the listings for my site.\n",
      "\n",
      "Any insights or recommendations would be greatly appreciated!\n",
      "\n",
      "Thanks in advance!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cfxj93/how_to_scrape_job_listings/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need 700 product data\n",
      "Text: Hello how can copy data from web pages like e-commerce websites and make it into a CSV \n",
      "\n",
      "The data that I want: product title, short description, description, product benefit and details. Image url. \n",
      "\n",
      "First time scraping data. \n",
      "\n",
      "Familiar with beautifulsoup, web scarping extension in chrome and Octoparse. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cfvodb/need_700_product_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Wow Tor is nice for an easy free proxy service\n",
      "Text: I don't know why I didn't think of using them earlier.\n",
      "\n",
      "Fricken two lines of code.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cfnv9j/wow_tor_is_nice_for_an_easy_free_proxy_service/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping racing results from website?\n",
      "Text: HI I have no coding experience so Im basically asking to be pointed int the right direction\n",
      "\n",
      "\"https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate=2024/04/28&Racecourse=ST&RaceNo=8\"\n",
      "\n",
      "Im looking at scraping results for all \"win odds\" and top 3 finishing positions, in inspect element I can easily find where the win odds and final places are. How would I got about scraping this into a excel/ data base somewhere. Just point me into the right directions cheers.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cfoieb/scraping_racing_results_from_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Amazon Store using Python and BS4 (No Selenium)\n",
      "Text: Based off the way that Amazon has designed their site, is it possible to scrape a product \"store\"? It looks certain categories for products are loaded asynchronously to defined slots on the page.  It appears as if Amazon calls these \"slots\" in a Store. Once you get to a category page, the products are again loaded asynchronously but there are typically just 8-9 on from there, but then no other async calls to load the rest on the page. \n",
      "\n",
      "Is it possible to scrape a store from Amazon using just BS4 in Python? Does anyone have any good examples or tutorials?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cfn10t/scraping_amazon_store_using_python_and_bs4_no/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Check my repo (I needed it so I published my code)\n",
      "Text: Idk if it's helpful to anyone but here. It's about generating gmail aliases.\n",
      "\n",
      "[https://github.com/brentspine/gmail-alias-generator](https://github.com/brentspine/gmail-alias-generator)\n",
      "\n",
      "\n",
      "# Gmail Alias Generator\n",
      "\n",
      "Your Gmail address can be \"scrambled\". This allows you to have 10k or up to unlimited addresses linked to one inbox. This is done by adding periods or plus signs at the end of the username. \n",
      "\n",
      "This repository helps you generate a big amount of gmail aliases automatically. \n",
      "\n",
      "## Table of Contents\n",
      "**[1. How do I run it?](#how-to-run)**<br>\n",
      "**[2. Walkthrough](#walkthrough)**<br>\n",
      "**[3. How does it work?](#how-does-it-work)**<br>\n",
      "**[4. Examples](#how-does-it-work)**<br>\n",
      "**[5. Troubleshooting](#trouble-shooting)**\n",
      "\n",
      "## How do I run it? <span id=\"how-to-run\"></span>\n",
      "You can run the code via the [Replit](https://replit.com/@brentspine/GenerateGmailAliases) project. Just go the link and click run at the top of the page.\n",
      "\n",
      "![output-onlinejpgtools](https://github.com/brentspine/gmail-alias-generator/assets/55391576/3d33ecf1-7fd0-48ac-86f2-d73a015d4427)\n",
      "\n",
      "Alternatively you can clone the code and run it on your own machine.\n",
      "\n",
      "In the CMD:\n",
      "```\n",
      "git clone https://github.com/brentspine/gmail-alias-generator\n",
      "```\n",
      "\n",
      "Run it:\n",
      "```\n",
      "cd gmail-alias-generator\n",
      "python main.py\n",
      "```\n",
      "\n",
      "## Walkthrough <span id=\"walkthrough\"></span>\n",
      "\n",
      "After running the program, it asks you to input a **valid** gmail address. You can also keep the field empty if you just want to test functionality\n",
      "\n",
      "![image](https://github.com/brentspine/gmail-alias-generator/assets/55391576/a7afc421-efea-4e61-8e13-47cd27e5076e)\n",
      "\n",
      "When the program is done generating your aliases you are prompted with some options you can choose from\n",
      "\n",
      "![image](https://github.com/brentspine/gmail-alias-generator/assets/55391576/db32e801-caec-4ff6-b13b-c130d5458cc3)\n",
      "\n",
      "You can generate as many aliases as you want by pressing \"1\" and pressing enter. <br>\n",
      "To copy and use your aliases you can either use option 4, which prints all addresses or option 3, which exports them to a file.\n",
      "\n",
      "If the file you want to export to already exists you can either choose the write or the append mode. The write mode wipes all contents of the file and overwrites them, while the append mode adds the new addresses to the .txt. Important to notice is, that it won't check for duplicates.\n",
      "\n",
      "![image](https://github.com/brentspine/gmail-alias-generator/assets/55391576/7529133c-219e-435e-a8de-f93465d6b5a9)\n",
      "\n",
      "\n",
      "## How does it work? <span id=\"how-does-it-work\"></span>\n",
      "\n",
      "**Adding dots in between the username**<br>\n",
      "&nbsp;&nbsp;-> When receiving mails, Google removes all dots from your address<br>\n",
      "&nbsp;&nbsp;-> This is why we can add random dots accross the address to \"scramble it\"<br>\n",
      "&nbsp;&nbsp;-> In this case we have the following rules<br>\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Don't add dots at the beginning or end and only before the @ sign<br>\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Don't surpass the threshold of 43 % dots<br>\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Don't have 2 dots in a row (exam..ple@​gmail.com)<br>\n",
      "&nbsp;&nbsp;-> Example: exam.plema.il@​gmail.com forwards to example.mail@​gmail.com\n",
      " \n",
      "**Alternating between domains**\n",
      "<br>&nbsp;&nbsp;-> @gmail.com and @googlemail.com are basically the same\n",
      "<br>&nbsp;&nbsp;-> We can still use both addresses on one service\n",
      "<br>&nbsp;&nbsp;-> Meaning we essentially double our alias amount\n",
      "<br>&nbsp;&nbsp;-> Example: example.mail​@googlemail.com forwards to example.mail@​gmail.com\n",
      "\n",
      "**Adding a + at the end of our username**\n",
      "<br>&nbsp;&nbsp;-> Gmail also has a built-in functionality for alias generation\n",
      "<br>&nbsp;&nbsp;-> We can add a + sign at the end of our username and everything between it and the @ will be ignored\n",
      "<br>&nbsp;&nbsp;-> Example: example.mail+g34d​@gmail.com forwards to example.mail​@gmail.com\n",
      " \n",
      "**Examples**\n",
      "<br>&nbsp;&nbsp;-> e.xamp.lemai.l+89dfgascb@​googlemail.com forwards to example.mail@​gmail.com\n",
      "<br>&nbsp;&nbsp;-> m.y.c.ooladdres.s+xyz69@​gmail.com forwards to my.cool.address@​gmail.com\n",
      "\n",
      "## Gmail alias list examples <span id=\"examples\"></span>\n",
      "\n",
      "I've given some examples in the how does it work section, but I've also generated some files as example outputs. You can find them [here](https://github.com/brentspine/gmail-alias-generator/tree/main/examples).\n",
      "\n",
      "\n",
      "## Still have problems or questions? <span id=\"trouble-shooting\"></span>\n",
      "\n",
      "If you still have problems you can open an [issue](https://github.com/brentspine/gmail-alias-generator/issues) or [contact me](https://linktr.ee/brentspine).\n",
      "\n",
      "<a href=\"https://www.buymeacoffee.com/brentspine\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/72239ca228f9e1e3cddaae6c43acb13e22c23f58fdfe78f2c3b44fb9879918d2/68747470733a2f2f696d672e6275796d6561636f666665652e636f6d2f627574746f6e2d6170692f3f746578743d427579206d65206120636f6666656526656d6f6a693d26736c75673d6272656e747370696e6526627574746f6e5f636f6c6f75723d46463546354626666f6e745f636f6c6f75723d66666666666626666f6e745f66616d696c793d436f6d6963266f75746c696e655f636f6c6f75723d30303030303026636f666665655f636f6c6f75723d464644443030\" data-canonical-src=\"https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;emoji=&amp;slug=brentspine&amp;button_colour=FF5F5F&amp;font_colour=ffffff&amp;font_family=Comic&amp;outline_colour=000000&amp;coffee_colour=FFDD00\" style=\"max-width: 100%;\"></a>\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cfjfjx/check_my_repo_i_needed_it_so_i_published_my_code/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Pointers scraping amazon.com with playwright?\n",
      "Text: I am using playwright in python. Any pointers to use playwright to crawl amazon.com at scale?I would like to avoid browser and TPS fingerprinting to avoid captchas.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cfilkf/pointers_scraping_amazoncom_with_playwright/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: My first crawler written in Go\n",
      "Text: \n",
      "URL: https://github.com/Sieep-Coding/web-crawler/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Which headless browser are you using ?\n",
      "Text: Hello guys,\n",
      "I'm actually using puppeteer-extra with stealth plugin to scrap data, it works pretty well but i'm looking for better option cause it's seems to leak some informations. I saw Ulixee but it doesn't seems to support fingerprint swap for now and it looks to be a pain to use it.\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cf2wbi/which_headless_browser_are_you_using/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Apps list by region\n",
      "Text: Not even sure if this is the right subreddit, but I am frustrated with this one. I am trying to scrape a list of all apps in a specific region (country) but since I couldn't find any that has such; I went to google advanced search to try and find the result by specifying the region and searching keywords such as \"app\".\n",
      "Didn't work very well. Any suggestions?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cfesbo/apps_list_by_region/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What web scraper for web search agent? \n",
      "Text: Hi everyone... \n",
      "\n",
      "I build an advanced RAG pipeline, and that include an agent that should get data from web, opening links from web search results... Anyway, I've zero past experience with web scraping, and my html knowledge is really basic. I'm going mad trying to extract the main text from web pages without lot of noise from tag, headers and other UI elements. \n",
      "As temporary solution, I added an llm agent \"in the middle\", using it to clean the scraped text... But that's slow, expensive (using cloud providers) and fondamentally inefficient. \n",
      "\n",
      "Someone can give me some tips/help? There is some library, repo or framework that may help me? \n",
      "\n",
      "Any kind of replay will be really appreciate! \n",
      "\n",
      "Thanks in advance for your time. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cf2ear/what_web_scraper_for_web_search_agent/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What data is my scraper sending as part of the request?\n",
      "Text: Is there an open source software / a website would that allow me to analyze the data my scraper is sending to the server? I would like to get all the info regarding headers, OS, hardware etc.\n",
      "I have tried Wireshark but it does not allow me to analyze much especially if it is a https requests (as it is encoded)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cex7kt/what_data_is_my_scraper_sending_as_part_of_the/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help bypassing recaptcha.\n",
      "Text: Hey everyone, I could really use some help with bypassing reCAPTCHA. I've been given the task of scraping LinkedIn Learning. My goal is to search for something and extract all the results. I've managed to log in and perform a search, but after a while, LinkedIn detected the scraping and threw up a reCAPTCHA. Is there any way to bypass it, or should I switch to another language or library? Currently, I'm using Puppeteer in my script. Any advice or suggestions would be greatly appreciated!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ceu1dv/need_help_bypassing_recaptcha/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is it legal to tamper with parameters on an open endpoint when web scraping?\n",
      "Text: Hi all,\n",
      "\n",
      "Building an aggregator that scrapes web data from various websites (think job openings). One of these sites has a page that calls an endpoint that allows you to set a starting index and an ending index of the data you want, with the page allowing you to go all the way up to the limit of the endpoint. Is it illegal for me to use these parameters to pull all of the data? The page only pulls data in increments of 100.\n",
      "\n",
      "Sorry if this question is stupid.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cep6l0/is_it_legal_to_tamper_with_parameters_on_an_open/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ever done arbitrage betting?\n",
      "Text: Hi, does anyone have any experience with arbitrage betting using a webscraping / crawling techniques ( if its still even possible ).  What was your experience? Did you reap big rewards?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cebwsv/ever_done_arbitrage_betting/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Where to find unofficial api's ?\n",
      "Text: Helloo folks currently looking to scrape some data from meta/instagram and snapchat . Saw few posts here talking about unofficial api's instead of full browser automation so how to find them? Should i try google dorking or just hangout in the network tab till something pops up ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ce5vz3/where_to_find_unofficial_apis/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Find first Webscrapping job?\n",
      "Text: Hey i‘d like to get my first web scrapping job. Where can i find one if i never had one in that field?\n",
      "What would be some cool webscrapping projects to show potential clients?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cegjge/find_first_webscrapping_job/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Facebook marketplace scraping\n",
      "Text: Web scraping Facebook marketplace. How do I get to stop webscraping once it reaches a certain point on the page? For e.g. “Results outside your search” how would I get it not to scrape past this. I’ve tried using the xpath and the css. It’s still giving me results past it!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cef74o/facebook_marketplace_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: why does this table return nothing?\n",
      "Text:  In scrapy shell I entered these 3 commands \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "     In [11]: fetch(\"https://www.ageofempires.com/stats/ageiide/\") \n",
      "    2024-04-27 13:36:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ageofempires.com/stats/ageiide/> (referer: None)  \n",
      "    \n",
      "    In [12]: response \n",
      "    Out[12]: <200 https://www.ageofempires.com/stats/ageiide/>  \n",
      "    \n",
      "    In [13]: response.css('table.leaderboard') \n",
      "    Out[13]: [] \n",
      "\n",
      " I'm not sure why it returns an empty list. as shown in the screenshot below there is a table with class=\"leaderboard\". \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "https://preview.redd.it/e5yb8cmoq0xc1.png?width=1452&format=png&auto=webp&s=5c19d96591879c9b1e25ff0785537a87da9b40b5\n",
      "\n",
      "&#x200B;\n",
      "\n",
      " Does anyone have any idea why this doesn't work? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ceds2k/why_does_this_table_return_nothing/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help with Facebook graph api \n",
      "Text: I created a bot with some features for personal use by couple groups , I used Facebook's graph API to pull the user's photo, name, and posts that contain photos/videos, but for a while now I have been trying to use this API to pull the user's bio. Is there a way to do that? to It doesn't matter if it's unofficial or getting my account banned.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cehofc/need_help_with_facebook_graph_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape the given site below?\n",
      "Text: I am looking to scrape this site: [https://golden.com/query/companies-in-the-nuclear-power-industry-VJJB4](https://golden.com/query/companies-in-the-nuclear-power-industry-VJJB4)\n",
      "\n",
      "The reason is that I can't find an option to create an account. The one there is not working and it is super expensive as well.\n",
      "\n",
      "Please help me understand how can I scrape this site and pull out the information. \n",
      "\n",
      "Thank you very much!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ceckoe/how_to_scrape_the_given_site_below/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Problem with headless mode in puppeteer on Windows\n",
      "Text: Hi . Recently i jumped to try web scrapping, but the first problem is here with puppeteer.\n",
      "When i use headless: false with the intention of see the GUI  the browser doesn't show. I tried change the browser from chromium to chrom with executablePath: 'path/to/chrome.exe but i get the same result. However if i open the resource monitor i see the process connected to my localhost runnig till termination of script. \n",
      "Anyone know how can i solve this ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ce9nrj/problem_with_headless_mode_in_puppeteer_on_windows/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: American Airlines scraper made in Python with only http requests\n",
      "Text: Hello comunity,\n",
      "\n",
      "Today I'll present to you pyaair, a scraper made pure on Python [https://github.com/johnbalvin/pyaair](https://github.com/johnbalvin/pyaair)\n",
      "\n",
      "Easy instalation\n",
      "\n",
      "  \\` \\` \\`pip install pyaair \\` \\` \\`\n",
      "\n",
      "  \n",
      "Easy Usage\n",
      "\n",
      "  \\` \\` \\` airports=pyaair.airports(\"miami\",\"\") \\` \\` \\`\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Always remember, only use selenium, puppeteer, playwright etc when it's strictly necesary \n",
      "\n",
      "Let me know what you think,\n",
      "\n",
      "thanks\n",
      "\n",
      "About me:\n",
      "\n",
      "I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience  \n",
      "\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ce1pm9/american_airlines_scraper_made_in_python_with/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Getting fake data in the response\n",
      "Text: Could you, please, give me advice on the following problem. \n",
      "\n",
      "The web resource which I was scraping on the regular basis for about a year (via direct REST API requests from Python) recently started to response with fake data instead of blocking by IP. And the problem is that I can't understand what rules or methods do they use to distinguish between valid users and scraping requests. \n",
      "\n",
      "I've tried from the same IP:\n",
      "\n",
      "* navigate to the page via browser - **good** data in the response\n",
      "* navigate to the REST endpoint  via browser  - **good** data in the response\n",
      "* generate request to the REST endpoint via Postman (even without spoofing user agent and other headers) - **good** data in the response\n",
      "* generate request to the REST endpoint via Python (with or without spoofing user agent and other headers) - **fake** data in response\n",
      "* generate request to the REST endpoint via Python having Wireshark as a local proxy  (with or without spoofing user agent and other headers) - **fake** data in response\n",
      "* generate request to the REST endpoint via Python having Wireshark as a local proxy with HTTPS packets decoded (with or without spoofing user agent and other headers) - **good** data in response\n",
      "\n",
      "I would appreciate for any help to understand how I can fix it and get it working again right via Python. \n",
      "\n",
      "Thank you very much!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cdpfjh/getting_fake_data_in_the_response/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is the \"Learning Scrapy\" book by Dimitrios Kouzis-Loukas too old for 2024?\n",
      "Text: I'd like to take a deep dive into the fundamentals of Scrapy for a project idea. The book seems very comprehensive which is appealing, but I'm worried that things have changed drastically since 2016.\n",
      "\n",
      "From what I can tell, the book came out in Jan 2016 and Scrapy didn't support python 3 until May 2016. \n",
      "\n",
      "Would The Python Scrapy Playbook be a better comprehensive source in 2024?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ce5t3z/is_the_learning_scrapy_book_by_dimitrios/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping vehicle images for a VMMR model\n",
      "Text: Existing datasets for VMMR, such as VMMRdb, only have vehicles up until ~2016. I’m scraping a car sale website for every make/model vehicle from 2016-2024. After combining with VMMRdb it will be the largest dataset available. I’m scraping all the image urls for the vehicle cover photos. I’ll download them all later. Their api(not public) returns search results in batches of 25 based on index, that’s why you see the index. It’s taken about 2 hours to scrape from 2016-2023. No proxys and I haven’t been rate limited once lmao.\n",
      "URL: https://i.redd.it/whtpdphx5ywc1.jpeg\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Suggest some AI for scraping\n",
      "Text: Okay! So, in this subreddit section, there are some hot posts about AI as a scraper. I'm here looking for your suggestion for some models or AI that can extract just the titles of news articles from different websites. Appreciate any insights!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cdoqzc/suggest_some_ai_for_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web navigation and Excel files formatting automation: Python or Power Automate with Azure Function or Virtual Machines?\n",
      "Text: Hello,\n",
      "\n",
      "I want to automate the following process and I don't know if I should use Python or Power Automate and then use a Virtual Machine or Azure Functions (maybe another approach?)? I have a 365 Microsoft Business Standard license.\n",
      "\n",
      "I want the following process to be fully automated and launched every day at a certain time without my intervention and my laptop turned on.\n",
      "\n",
      "1. Go to a website and log in with confidential credentials.\n",
      "2. Navigate different pages to download a .csv file and then save the downloaded file in a folder on my One Drive\n",
      "3. Format the downloaded file: add columns and format columns and data inside, also add the =image(url) function and copy paste in value the images, so they are no longer linked to the urls in the file. The final file should be converted to an .xlsx file.\n",
      "4. Copy and paste the data with the format just applied from the downloaded file to another existing Excel file saved on One Drive.\n",
      "5. Download images and videos from the URLs contained in the latest Excel file and save them on specific folders in One drive.\n",
      "\n",
      "Thanks in advance for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cdk3qg/web_navigation_and_excel_files_formatting/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Linkedin-collaborative-articles-Scrapping\n",
      "Text: I want to scrape  Linkedin collaborative articles with answers , I've already extracted all topic links and created csv, it will be great if anybody here can help me with that,\n",
      "\n",
      "[https://github.com/viveks-codes/Linkedin-collaborative-articles-Scrapping/tree/main](https://github.com/viveks-codes/Linkedin-collaborative-articles-Scrapping/tree/main)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cdhx2u/linkedincollaborativearticlesscrapping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: American airlines scraper made pure in Go\n",
      "Text: Hello Comunity\n",
      "\n",
      "Today I'll present to you American airlines scraper [https://github.com/johnbalvin/goaa](https://github.com/johnbalvin/goaa)\n",
      "\n",
      "I made it on pure Go with only using http requests, once again demostrating you don't need to use selenium, puppeteer, playwright or any other browser automation tool.\n",
      "\n",
      "You won't see anywhere on internet an scraper so efficient like this one, the ones I checked use selenium which consumes a lot of resources.\n",
      "\n",
      "A brief overview why to make your bots with plain http requests: \n",
      "\n",
      "- 99% more efficient, you don't need extra dependencies, processing the static files takes time and resources, and just to maintaining the the browser automation open consumes a lot of resources compared to just using plain http requests\n",
      "\n",
      "- 99% faster,you don't need to wait for all static files to load and process, all this adds up to how long the bot takes to finish\n",
      "\n",
      "- 99% cheaper, if you are using proxies, all static files will go through the proxy, and all websites has a lot of static files, you can use a smaller vm for your bots just by using plain http requests\n",
      "\n",
      "- 99% more scalable, if you were to use proxies with those browser automation tools, each time you create a new tab this consumes a lot of resources and when you are working with scalability in mind, you will quickly consume all your vm resources and you will need to increase your vm size\n",
      "\n",
      "- easier to maintain compared to those using browser automation tools, I mean look at the code, is so simple that you might wonder why other scrapers like this use those automations tools\n",
      "\n",
      "- you will eventually find hidden gems, like websites returning private data, for example once I found about 5 goverment websites returing private court documents from the server they were not displaying this private to the user, but the private data was there( those website still returning private data)\n",
      "\n",
      "Only use those browser automation tools when is strictly necessary  \n",
      "Tomorrow the python version will be released\n",
      "\n",
      "Let me know what you think, thanks\n",
      "\n",
      "About me: \n",
      "\n",
      "I'm full stack developer specialized on web scraping and backend, with 6-7 years of experience\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ccxeqz/american_airlines_scraper_made_pure_in_go/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: scraping for common likes on instagram?\n",
      "Text: I run a niche education page on instagram.  \n",
      "\n",
      "\n",
      "I want to reach out to people who regularly like my post.  \n",
      "\n",
      "\n",
      "Is it possible to scrape the likes from my reels and then run some script to find who has liked, say, more than 5 of my videos.\n",
      "\n",
      "Then I can use this list to personally DM them and make more content for my most engaged students  \n",
      "\n",
      "\n",
      "Thanks scraping peeps\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cd6fer/scraping_for_common_likes_on_instagram/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What's your favorite use of webscraping for a personal (not commercial) project?\n",
      "Text: For me it would be:\n",
      "\n",
      "* scraping a rental company website to see change in price over time based on the town/week/# of bedrooms for beach rentals\n",
      "* pulling out all of my Twitter bookmarks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ccwd8l/whats_your_favorite_use_of_webscraping_for_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Crunchbase scraping\n",
      "Text: Hi friends! I need help getting information from the crunchbase website. The data that I want to obtain automatically are the names and emails of the companies with the filters that interest me. Both the name and email of the company profiles are public. I have to do my college thesis in economics and I have to send a survey to these companies via email.\n",
      "\n",
      "There are several websites that offer the scraping code for crunchbase and even here on Reddit some scraping codes have been passed around but I need to modify it to get the commented data.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cd30x6/crunchbase_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do search engines like Startpage not get caught by captcha/IP limit etc?\n",
      "Text: Even ordinary users are immediately caught by bot protection when they do a lot of searches on Google.  Considering that IP supply is limited I wonder how do search engines like Startpage not get caught by captcha/IP limit etc?\n",
      "\n",
      "Startpage is a front end/proxy for google \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cczcr9/how_do_search_engines_like_startpage_not_get/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to deploy Python scraping project to the cloud\n",
      "Text: So I have built a Python scraper using requests and beautiful soup and would like to deploy it to the cloud.\n",
      "\n",
      "It fetches about 50 json files, it should do this every day (takes about 5 minutes).\n",
      "\n",
      "Preferably I can then convert this json data into a SQL database (about 2,000 rows every day) that I can use for my website.\n",
      "\n",
      "What's the easiest (and cheapest if possible, but ease of use is most important) way to accomplish those goals? If my only option is one of the big 3, then I'd prefer Azure, what exact features would I need?\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ccmhvd/how_to_deploy_python_scraping_project_to_the_cloud/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is it possible to do a scrapping project using only AI?\n",
      "Text: I'm talking from A to Z with no scraping/dev skills\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ccos0d/is_it_possible_to_do_a_scrapping_project_using/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Cloudflare vs Perimters vs Datadome\n",
      "Text: Hi guys, quick question here. Which one would you say is the hardest to bypass? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ccol3v/cloudflare_vs_perimters_vs_datadome/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Sending/Injecting custom sensor data (mouse,clicks), webrtc, webgl.\n",
      "Text: Target site: [https://finder.humana.com/finder/search?customerId=1](https://finder.humana.com/finder/search?customerId=1)\n",
      "\n",
      "Website seems to track user, using canvas figerprinting, tracking public ip, user actions and all sorts of stuff, the data is sent back to server based on the following headers it looks like.\n",
      "\n",
      "Not attaching full encoded header stings, these are just for reference.\n",
      "\n",
      "`YignxymxbxA:m7o60OdAbnoPaP0StLDPZZtiGIHIp9YwpxQCUOuETJpgc9`  \n",
      "`Yignxymxbx-B:1qbas`  \n",
      "`Yignxymxbx-C:AAD6jRSPAQAAvPLtdBrYeM1YaW6oRRIyT8g_11v-bE3ekxDd9AY8CA3Afw5A`  \n",
      "`Yignxymxbx-D:ABaAhIDBCKGFgQGAAYIQgISi-==`  \n",
      "`YignxymxbxF:A3e1kBSPAQAABgywoEjkF5hhuyYfionRUsJNzGcNPbQaEBhdx1H5TE5B2CZ3AditaXyucgzkwH8AAEB3AAAAAA==`  \n",
      "`Yignxymxbx-Z:q`\n",
      "\n",
      "What I did is using playwright and chromium, successfully sent the fake public IP data based on webrtc frame work, blocked webgl (used noised value as well), changed browser view port, change browser size, IP/Proxy, user agent etc for each individual request to the website, still the website seems to block requests.\n",
      "\n",
      "I did used Kameleo tool to create random profiles, each having rotating proxy, different value for webgl, different public proxy which gets tacked through the webrtc, user agent browser versions etc, still the site seems to track and block.\n",
      "\n",
      "Looks like they are looking at the user behaviour along with other things, what I am looking for is some way I can encode my own values for the above headers for user actions and rest of the things.\n",
      "\n",
      "I would higly appreciate anyone guiding me in the right direction. How can I construct similar values or debug how the website is creating these values, and then perhaps create these value of my own and scrape data without using any sort of browser rendering stuff. Thanks !\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ccns6y/sendinginjecting_custom_sensor_data_mouseclicks/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Intro to subscribing to graphql websocket endpoint to listen for changes and insert those into a database\n",
      "Text: data engineering guy who knows how to get data from databases directly, but not familar with web based methods.\n",
      "\n",
      "i would like to pull data from a wss graphql (hasura) endpoint and store in a local db. steps. send a message to initiate connection (get connetion\\_ack), then send another message with a subscribe to query.\n",
      "\n",
      "i can manually get this to work in postman, it's the automation, capturing json response, formatting and inserting into my local postgres db that making it so hard to figure out.\n",
      "\n",
      "any links/github code examples would be appreciated!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ccio2z/intro_to_subscribing_to_graphql_websocket/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Source HTML doesn’t match displayed HTML\n",
      "Text: I’m scraping a checkout page for a site and when I check its source html using chrome developer tools, I can see it doesn’t match the one displayed on my browser. The structure is the same but they use different currencies so the amount is different. When I try to scrape it using selenium, I get the html displayed in chrome developer tools, but not the one displayed in the browser. Does anyone know what’s the reason for the difference and how can I grab the values I actually want?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cc4el4/source_html_doesnt_match_displayed_html/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I scraped 1 Million Social Media Profiles\n",
      "Text: # Scraped 1M Social Media Profiles\n",
      "\n",
      "Hi, I just finished scraping 1 million social media profiles, and I want to share my knowledge.\n",
      "\n",
      "What did I scrape ? 1 million profiles of people from “BaseBook”.  \n",
      "Keep in mind this is just an overview and the entire system took 3 months to write using Python Clients, Php Servers, and MySQL databases on the server.\n",
      "\n",
      "# Know what you want\n",
      "\n",
      "Know the data you want in advance, don’t scrape data for the sake of hoarding it, this makes testing harder as there is no data to check to see if it was saved correctly. If you want text save text, if you want images save images. Also don’t parse the data as you find it, save the data and parse it later. You can’t afford to parse the same URL 2 or more times, if you save everything 1st time, you don’t waste URL scrapes.\n",
      "\n",
      "# Start Small but plan big\n",
      "\n",
      "Most scraping starts with a URL.\n",
      "\n",
      "Start small by parsing your own data on “Basebook” first, for each URL scrapped create a new folder with a random name.  \n",
      "In each folder have a metadata.txt file which would contain things like ( start date, start time and end time, the URL you started with, etc).\n",
      "\n",
      "The new urls you find, don’t follow them just put them into a file \"newurls.txt\".\n",
      "\n",
      "Also save any errors to an \"errors.txt\" file and make sure the errors make sense.\n",
      "\n",
      "\n",
      "\n",
      "https://preview.redd.it/jc4g95ystawc1.jpg?width=331&format=pjpg&auto=webp&s=763f70386616f6c6d3911d332daf223b350086c1\n",
      "\n",
      "**Verify Your Folder**\n",
      "\n",
      "When you scrape your 1st profile, double confirm all the data and images, confirm your metadata file has all the data it should. Confirm that each file was saved correctly and not corrupted, make sure each file can be opened correctly to.\n",
      "\n",
      "# Create a Job Manager\n",
      "\n",
      "This is one of the single most important pieces of code you will ever write, its ONE job is to take a URL, scrape it and report back, it must be bulletproof and never fail.  \n",
      "Create a script or a program that can take an input URL e.g. “[www.basebook.com/profile2222](http://www.basebook.com/profile2222)” and that creates the folder with a random name,  with all the data as outlined above (errorfile.txt, metadata.txt, newurls.txt).\n",
      "\n",
      "Test it using dummy urls that never resolved, URLS that are ip addresses etc.\n",
      "\n",
      "**Test Your Job Manager ( this can be called** [**jobmanager.py**](http://jobmanager.py) **or Parser.py) I renamed that file as I learned more :).** \n",
      "\n",
      "Start by inputting your profile URL into the jobManager.py (“[www.basebook.com/profile2222](http://www.basebook.com/profile2222)”)  \n",
      "This should create the random folder, with all the images and files that are part of that URL.  \n",
      "Check the starttime and endtime, this gives you a baseline of how long 1 would take, if it comes back as 30 seconds, then another URL’s duration is over say 2 minutes means your have an error in your parser, make sure your parser is always finishing the job, even if the job is terminated or crashed etc ?  \n",
      "It should handle all errors and fail gracefully.  \n",
      "Now you should have a job manager script that can handle ANY url and download all the data.\n",
      "\n",
      "**You need to speed up**\n",
      "\n",
      "After your 1st 10 or so URL downloads you will get a sense of how long it takes, maybe it takes 2 minutes per URL and maybe each URL folder is 5MB.\n",
      "\n",
      "That means it would take you at least 2 Million minutes to download the 1 million profiles, which is 3.8 years!\n",
      "\n",
      "Here is when you need to start thinking about distributed systems, you should have a web server with a DB etc and its job is to send new URLS to any client machine that wants to work on the scraping.  \n",
      "Then you can have 10 different machines working in 10 different places, and you won’t loose all the data if 1 machine goes down. It also helps you stop being blocked by the site you are scraping.\n",
      "\n",
      "  \n",
      "We will modify [Parser.py](https://Parser.py) to get URLS to parse from a webcall instead of us passing them.  \n",
      "For instance you should have a webserver [www.iliketoscrape.com/getjob.](http://www.iliketoscrape.com/jobgiver.py)php.  \n",
      "Its job is to return a JSON Object containing the new URL+jobID+foldername so your local machine can start parsing.\n",
      "\n",
      "  \n",
      "That way you can have 10 machines running in different locations all working on the shared go.  \n",
      "In order to create this special web call you need to create a server and SQL database.  \n",
      "And Remember that newurls.txt file ? well your script will now be calling a new webcall [www.iliketoscrape.com/postjob.](http://www.iliketoscrape.com/jobgiver.py)php, sending up the list of URLS that still have to be parsed.  \n",
      "Which will be sent back to other clients that call [www.iliketoscrape.com/getjob.](http://www.iliketoscrape.com/jobgiver.py)php\n",
      "\n",
      "The idea is that each time your jobManager creates the newurls.txt file, send each of those URLS to your SQL server into a Table called “url\\_table”.\n",
      "\n",
      "|jobID|url|foldername|processed|\n",
      "|:-|:-|:-|:-|\n",
      "|1|basebook.com/ahaha|sdadcarw|0|\n",
      "|2|basebook.com/asasas|sfsdfsdf|1|\n",
      "|3|basebook.com/asasas|ertertyved|2|\n",
      "\n",
      "Client (Parser.py) Calls: /postJob.php  and sends up the URLs from the entries in your newurls.txt file.  \n",
      "The server will accept these new URLS, and create a random foldername for them and save the data to the client. nothing is returned to the client.\n",
      "\n",
      "When the client wants a URL to parse, it calls /getjob.php, this gets a JOBID record that has not been started yet. It returns the URL, JOBID, foldername to the client.\n",
      "\n",
      "The client then starts parsing this URL using the [Parser.py](https://Parser.py) file we wrote earlier. When the URL is done parsing the client calls /updateJob and sends up the jobID. (remember the client also calls /postjobs.php for all new urls it found.\n",
      "\n",
      "Your JobManager  will send up the URLs to your webserver, and your web server will save them into your url\\_table as well as marking what URLS are completed. Your web server should also make sure no URL duplicated in the database etc.\n",
      "\n",
      "\n",
      "\n",
      "https://preview.redd.it/dpsmx59ztawc1.jpg?width=908&format=pjpg&auto=webp&s=e6c220a2dc26e117b1a1df8f49d606aa9ab08dc9\n",
      "\n",
      "Using this distributed system and 4 machines, we were able to download 40,000 user profiles a day, and it took 25 days to download 1 million profiles.  \n",
      "Each client machine had about 250,000 random folders, each folder being 1 URL it parsed.  \n",
      "Nightly each machine would backup its data onto external hard drives.\n",
      "\n",
      "Job Done ? No.\n",
      "\n",
      "# What to do with all the data ?\n",
      "\n",
      "Now that we had 1 million social media profiles we needed to make a search engine that could search for the data we wanted. I am still working on this !\n",
      "\n",
      "\n",
      "Edit: getting some feedback about why we collect this data. We are a face recognition team at FaceMRI.com , that specializes in large datasets for finding missing people etc.\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cbh787/i_scraped_1_million_social_media_profiles/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to submit solved captcha with requests?\n",
      "Text: Hello,\n",
      "\n",
      "I'm not sure about what we can and cannot talk about in this sub, so I'll keep it generic.\n",
      "\n",
      "I want to login into a website using python requests. I can get through the initial login page and get to a regular captcha page. I solve the captcha using a popular service, but I'm stuck at submitting the captcha. I always get response status 404.\n",
      "\n",
      "This the piece of code. Can someone offer some suggestions on how to overcome this? Thanks!\n",
      "\n",
      "P.S.: I'm using a proxy and a header\n",
      "\n",
      "`response = session.get(sign_in_url)`\n",
      "\n",
      "`soup = BeautifulSoup(response.content, 'lxml')`\n",
      "\n",
      "`form = soup.find('form', attrs={'name':\"signIn\"})`\n",
      "\n",
      "`inputs = form.find_all('input')`\n",
      "\n",
      "`payload = {}`\n",
      "\n",
      "`for form_input in inputs:`\n",
      "\n",
      "`try:`\n",
      "\n",
      "`payload[form_input[\"name\"]] = form_input['value']`\n",
      "\n",
      "`except KeyError:`\n",
      "\n",
      "`print('not a valid input')`\n",
      "\n",
      "`print()`\n",
      "\n",
      "`payload['email'] = user`\n",
      "\n",
      "`payload['password'] = password`\n",
      "\n",
      "`response = session.post(sign_in_url, data=payload)`\n",
      "\n",
      "`captcha_url = response.url`\n",
      "\n",
      "`soup = BeautifulSoup(response.content, 'lxml')`\n",
      "\n",
      "`form = soup.find('form', attrs={'action':\"verify\"})`\n",
      "\n",
      "`inputs = form.find_all('input')`\n",
      "\n",
      "`payload = {}`\n",
      "\n",
      "`for form_input in inputs:`\n",
      "\n",
      "`try:`\n",
      "\n",
      "`payload[form_input[\"name\"]] = form_input['value']`\n",
      "\n",
      "`except KeyError:`\n",
      "\n",
      "`print('not a valid input')`\n",
      "\n",
      "`print()`\n",
      "\n",
      "`captcha = soup.find('img', attrs={'alt':'captcha'})`\n",
      "\n",
      "`if captcha:`\n",
      "\n",
      "`captcha_image = captcha.get('src')`\n",
      "\n",
      "`solved_captcha = solver.normal(captcha_image)`\n",
      "\n",
      "`payload['cvf_captcha_input'] = solved_captcha['code']`\n",
      "\n",
      "`response2 = session.post(captcha_url, data=payload)`\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cc0tsv/how_to_submit_solved_captcha_with_requests/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I need help in webscraping,\n",
      "Text: Hello, I wanted to extract the no. Of followers of insta profiles, first it worked for a few usernames, but now it is showing errors (asking to log in something like redirected to insta login) I can give the script, please tell me if there is any way to bypass this login, if it is necessary then how to incorporate it in the code so that I don't have to login again and again if I'm using loops to extract for more than one username?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cc5ld3/i_need_help_in_webscraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping LinkedIn\n",
      "Text: I’m looking for either a (completely) free LinkedIn Sales Navigator scraper, or points on how to create my own - can anyone help?\n",
      "\n",
      "EDIT: Someone must know a free to use web scraper?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cbtrsu/scraping_linkedin/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Instagram: can you gather only new posts and reels automatically?\n",
      "Text: I'm wondering if anyone has experience or knowledge about scraping new posts and reels from an Instagram account. I'm interested in gathering content either daily or weekly from a specific account, and I do have access to that account if necessary.\n",
      "\n",
      "Is there a reliable way to automate this process? Any tips, tools, or methods you could recommend would be greatly appreciated. Thanks in advance for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cbr6ur/scraping_instagram_can_you_gather_only_new_posts/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best way to scrape recent fast food prices per city?\n",
      "Text: I want to scrape the mcdonalds menu items (just maybe 10) per city internationally (around 20 cities). Where should I start? Does google maps api allow me to filter menu photos then I can do processing to text?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cbl76j/best_way_to_scrape_recent_fast_food_prices_per/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Zillow scraper made pure in Python\n",
      "Text: Hello everyone., on today new scraper I created the python version for the zillow scraper.\n",
      "\n",
      "The library will get zillow listings and details from the listing.\n",
      "\n",
      "It is made on pure python with HTTP requests, so no selenium, puppeteer, playwright etc. or none of those automation libraries that I hate\n",
      "\n",
      "[https://github.com/johnbalvin/pyzill](https://github.com/johnbalvin/pyzill)\n",
      "\n",
      "`pip install pyzill`\n",
      "\n",
      "Let me know what ou think, thanks\n",
      "\n",
      "  \n",
      "about me:   \n",
      "I'm full stack develper specialized on web scraping and backend, with 6-7 years of experience\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cbkv58/zillow_scraper_made_pure_in_python/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Am I moving in right direction for my database?\n",
      "Text: Im building a scraper with several different points working together with Claude.ai\n",
      "\n",
      "I’m currently using supabase for this. I have a feeling that I’m throwing myself into a wall thinking how big this can get.\n",
      "\n",
      "Any suggestion if I should use something else? \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cblp6t/am_i_moving_in_right_direction_for_my_database/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I'd Like To Scrape The Video From The Live US Senate Floor Feed\n",
      "Text: I'm watching [this live senate feed](https://www.senate.gov/legislative/floor_activity_pail.htm) and I think the actual url is:\n",
      "\n",
      "[`https://www-senate-gov-media-srs.akamaized.net/hls/live/2096634/stv/stv042324/master.m3u8`](https://www-senate-gov-media-srs.akamaized.net/hls/live/2096634/stv/stv042324/master.m3u8)\n",
      "\n",
      "I use ffmpeg to pull that url and it does appear to detect/decode an h264:\n",
      "\n",
      "`Stream #0:0: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 90k tbn, 60 tbc`\n",
      "\n",
      "I specify the ffmpeg output as output.m4v and it does continue to write the file (live stream) and the file grows.   ffmpeg does not error out.     But the file is not playable.\n",
      "\n",
      "This specific URL will probably not be valid when the feed ends but does anyone know how to grab a feed like this?\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cbljd5/id_like_to_scrape_the_video_from_the_live_us/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Chrome extension scraping\n",
      "Text: I want to write a chrome extension that can visit a certain webpage and grab the html content of that webpage. Simply using fetch to get the content of the webpage doesn’t result in the expected output since some parameters are dynamically set when visiting the site. Is there a way I can use the chrome extension to simulate the user visiting the site and get the data? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cbgk0i/chrome_extension_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape amazon product pages with Playwright python without being detected?\n",
      "Text: I have some experience with coding but am quite new to the world of webscraping.\n",
      "\n",
      "I have a requirement where I have a few hundred amazon product URLs and would like to scrape them to obtain some webpage info. I am trying to scrape the info that is available in the public domain so it isn't illegal.\n",
      "\n",
      "I am using Playwright in python to do this and have come up with a working code. Some of the features include:\n",
      "\n",
      "Capability to crawl in headless and headed mode \n",
      "Capability to use chromium, Firefox, Webkit\n",
      "Change the user-agent randomly\n",
      "Use a couple of proxies freely available online\n",
      "Match real browser headers and change them randomly (using the most common headers such as accept-language)\n",
      "Device emulation (if necessary)\n",
      "\n",
      "Now after reading quite a bit, I understand that requests at scale can be TLS fingerprinted and also Browser Fingerprinted.\n",
      "With amazon, I am receiving captchas around 50% of the time and I am suspecting that this is due to some kind of fingerprinting.\n",
      "\n",
      "With Playwright, I believe TLS fingerprinting should not be an issue as the fingerprint matches that of a real browser and cannot be blacklisted.\n",
      "\n",
      "But, what about browser fingerprinting (such as viewport, hardware, OS, canvas, audio, plugins etc)? How do I randomly changes these values to avoid fingerprinting with Playwright? Would be grateful if folks can help me here. If there is access to some code snippets that can be used, would be grateful too.\n",
      "\n",
      "Should I also consider handling something else?\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cbhakp/how_to_scrape_amazon_product_pages_with/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need Help!!!\n",
      "Text: I need to scrap this website and  the problem is that the URLs are not structured.\n",
      "I'm using beautiful soup.\n",
      "https://www.collegedekho.com/colleges-in-india/\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cbanc1/need_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I scrape a website where soup.find_all() returns nothing?\n",
      "Text: Hey everyone. I am trying to scrape a website that has mlb gamelogs data for a specific player, however when I try to use requests and BeautifulSoup to find the tags where the data lies, nothing gets returned. Heres the Url: [https://www.fangraphs.com/players/corbin-burnes/19361/game-log?position=P&gds=&gde=&type=1&season=2024](https://www.fangraphs.com/players/corbin-burnes/19361/game-log?position=P&gds=&gde=&type=1&season=2024)  \n",
      "\n",
      "\n",
      "This website might not be possible to scrape, but I hope theres someone here that is very good at it that can help me figure this out. I am very new to this but here's what I am trying to do: There is a table at the bottom of the url that has game level pitching stats for a specific player (I eventually hope to scrape many season of data for many players) but I cannot figure out how to go into the table to retrieve the tabular data. It lies inside multiple table-wrapper and table-scroll divs, which are making me confused on how to access the actual <td> data that I need (at least thats where I think it is). Does anyone know how to can access the table and get the values inside? It seems simple but I think I am just not experienced enough to understand how to do it. Thanks! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cb9uup/how_can_i_scrape_a_website_where_soupfind_all/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: help a new scraper\n",
      "Text: Hi I'm trying to scarpe [https://alhisba.com/auctions/historical](https://alhisba.com/auctions/historical)\n",
      "\n",
      "couldn't find the right tool, used \"Web Scraper\" chrome extension but couldn't get the results.\n",
      "\n",
      "all the feilds in the picture attached\n",
      "\n",
      "https://preview.redd.it/adlrhwucu8wc1.png?width=1540&format=png&auto=webp&s=4f2a1db90c6f41323b381cfc69a254933f328ee6\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cb2t6l/help_a_new_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selenium In Facebook marketplace \n",
      "Text: Hi,\n",
      "I do list and sell stuff on marketplace. After some time , I like to delete my listing and reputting it .\n",
      "\n",
      "I was considering doing this with selenium. I was wondering if this could get banned given that I will run it 3 times a week for one listing \n",
      "\n",
      "Thanks !\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1caq92x/selenium_in_facebook_marketplace/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: The F*** \"too many request\"  problem 🥲\n",
      "Text: Hi, I am trying to pull data from a site via a brute force attack using tools like burpsuite or even pythone,  but this f**** 429 error \"too many attemps\"  or \"too m many request\"  always get me,  Although  i am changing the User Agent every time \n",
      "\n",
      "Can any one help with that?  \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cazhrm/the_f_too_many_request_problem/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any suggestions on API to give out supposed monthly page views per url? \n",
      "Text: Im scraping a few pages of my interest, im having difficult understanding what type of algorithm companies use for page view attribution.\n",
      "\n",
      "Any suggestion on a paid API that can do it on large scale? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1catheq/any_suggestions_on_api_to_give_out_supposed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Facebook marketplace scraper\n",
      "Text: Created a scraping script using python. it goes into incognito chrome browser and follows the fb link. Declines all pop ups and then clicks on the first listing. Extracts title, location and price. \n",
      "\n",
      "Perfect!\n",
      "\n",
      "What I can’t get to is the description or the time listed “2 hours ago” its in a span class auto=dir. any help? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1car8hf/facebook_marketplace_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Tough To Scrape Site\n",
      "Text: I'm trying to scrape this website which returns a table using an ASP.Net form as the input - [https://renewablesandchp.ofgem.gov.uk/Public/ReportViewer.aspx](https://renewablesandchp.ofgem.gov.uk/Public/ReportViewer.aspx?ReportPath=/DatawarehouseReports/CertificatesExternalPublicDataWarehouse&ReportVisibility=1&ReportCategory=2)\n",
      "\n",
      "I'm copying the headers and form data from the flow I see in the Networks tab when submitting the form manually (n.b. the page size is not set by default). I also grab the ASP.Net cookie that is returned in the `get_session_cookies` query. I've confirmed that this is the only cookie needed to scrape the data and that my headers exactly match.\n",
      "\n",
      "I then use the following code to try and query the service which returns a HTML snippet that is used to update the page (the data I want is in this snippet).\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup as bs\n",
      "\n",
      "def get_report_viewer_soup(session: requests.Session) -> bs:\n",
      "    \"\"\"Also grabs the ASP.Net session cookie\"\"\"\n",
      "    url = 'https://renewablesandchp.ofgem.gov.uk/Public/ReportViewer.aspx'\n",
      "    params = {\n",
      "        'ReportPath': '/DatawarehouseReports/CertificatesExternalPublicDataWarehouse',\n",
      "        'ReportVisibility': 1,\n",
      "        'ReportCategory': 2\n",
      "    }\n",
      "\n",
      "    r = session.get(url, params=params)\n",
      "    r.raise_for_status()\n",
      "\n",
      "    return bs(r.text, 'html.parser')\n",
      "\n",
      "def get_table(session: requests.Session, soup: bs):\n",
      "    url = 'https://renewablesandchp.ofgem.gov.uk/Public/ReportViewer.aspx'\n",
      "\n",
      "    params = {\n",
      "        'ReportPath': '/DatawarehouseReports/CertificatesExternalPublicDataWarehouse',\n",
      "        'ReportVisibility': 1,\n",
      "        'ReportCategory': 2\n",
      "    }\n",
      "\n",
      "    headers = {\n",
      "        'Accept': '*/*',\n",
      "        'Accept-Encoding': 'gzip, deflate, br',\n",
      "        'Accept-Language': 'en-US,en;q=0.9',\n",
      "        'Cache-Control': 'no-cache',\n",
      "        'Connection': 'keep-alive',\n",
      "        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
      "        'Host': 'renewablesandchp.ofgem.gov.uk',\n",
      "        'Origin': 'https://renewablesandchp.ofgem.gov.uk',\n",
      "        'Referer': 'https://renewablesandchp.ofgem.gov.uk/Public/ReportViewer.aspx?ReportPath=/DatawarehouseReports/CertificatesExternalPublicDataWarehouse&ReportVisibility=1&ReportCategory=2',\n",
      "        'Sec-Ch-Ua': '\"Chromium\";v=\"116\", \"Not)A;Brand\";v=\"24\", \"Google Chrome\";v=\"116\"',\n",
      "        'Sec-Ch-Ua-Mobile': '?0',\n",
      "        'Sec-Ch-Ua-Platform': '\"macOS\"',\n",
      "        'Sec-Fetch-Dest': 'empty',\n",
      "        'Sec-Fetch-Mode': 'cors',\n",
      "        'Sec-Fetch-Site': 'same-origin',\n",
      "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',\n",
      "        'X-Microsoftajax': 'Delta=true',\n",
      "        'X-Requested-With': 'XMLHttpRequest'\n",
      "    }\n",
      "\n",
      "    data = {\n",
      "        \"ReportViewer$ctl04$ctl03$txtValue\": \"REGO, RO\",\n",
      "        \"ReportViewer$ctl04$ctl03$divDropDown$ctl01$HiddenIndices\": \"0,1\",\n",
      "        \"ReportViewer$ctl04$ctl05$txtValue\": \"Aerothermal, Biodegradable, Biogas, Biomass, Biomass 50kW DNC or less, Biomass using an Advanced Conversion Technology, CHP Energy from Waste, Co-firing of Biomass with Fossil Fuel, Co-firing of Energy Crops, Filled Storage Hydro, Filled Storage System, Fuelled, Geopressure, Geothermal, Hydro, Hydro 20MW DNC or less, Hydro 50kW DNC or less, Hydro greater than 20MW DNC, Hydrothermal, Landfill Gas, Micro Hydro, Ocean Energy, Off-shore Wind, On-shore Wind, Photovoltaic, Photovoltaic 50kW DNC or less, Sewage Gas, Solar and On-shore Wind, Tidal Flow, Tidal Power, Waste using an Advanced Conversion Technology, Wave Power, Wind, Wind 50kW DNC or less\",\n",
      "        \"ReportViewer$ctl04$ctl05$divDropDown$ctl01$HiddenIndices\": \"0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33\",\n",
      "        \"ReportViewer$ctl04$ctl07$txtValue\": \"N/A, NIRO, RO, ROS\",\n",
      "        \"ReportViewer$ctl04$ctl07$divDropDown$ctl01$HiddenIndices\": \"0,1,2,3\",\n",
      "        \"ReportViewer$ctl04$ctl09$txtValue\": \"AD, Advanced gasification, Biomass (e.g. Plant or animal matter), Biomass using an Advanced Conversion Technology, Co-firing of biomass, Co-firing of biomass with fossil fuel, Co-firing of energy crops, Co-firing of regular bioliquid, Dedicated biomass, Dedicated biomass - BL, Dedicated biomass with CHP, Dedicated biomass with CHP - BL, Dedicated energy crops, Dedicated energy crops with CHP, Electricity generated from landfill gas, Electricity generated from sewage gas, Energy from waste with CHP, High-range co-firing, Low range co-firing of relevant energy crop, Low-range co-firing, Mid-range co-firing, N/A, Standard gasification, Station conversion, Station conversion - BL, Unit conversion, Unspecified, Waste using an Advanced Conversion Technology\",\n",
      "        \"ReportViewer$ctl04$ctl09$divDropDown$ctl01$HiddenIndices\": \"0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27\",\n",
      "        \"ReportViewer$ctl04$ctl11$txtValue\": \"England, Northern Ireland, Scotland, Wales\",\n",
      "        \"ReportViewer$ctl04$ctl11$divDropDown$ctl01$HiddenIndices\": \"2,4,5,7\",\n",
      "        \"ReportViewer$ctl04$ctl17$txtValue\": \"<ALL>\",\n",
      "        \"ReportViewer$ctl04$ctl17$divDropDown$ctl01$HiddenIndices\": \"0\",\n",
      "        \"ReportViewer$ctl04$ctl27$txtValue\": \"General, NFFO, AMO\",\n",
      "        \"ReportViewer$ctl04$ctl27$divDropDown$ctl01$HiddenIndices\": \"0,1,2\",\n",
      "        \"ReportViewer$ctl04$ctl31$txtValue\": \"Issued, Revoked, Retired, Redeemed, Expired\",\n",
      "        \"ReportViewer$ctl04$ctl31$divDropDown$ctl01$HiddenIndices\": \"0,1,2,3,4\",\n",
      "        \"ReportViewer$ctl04$ctl37$txtValue\": \"<ALL>\",\n",
      "        \"ReportViewer$ctl04$ctl37$divDropDown$ctl01$HiddenIndices\": \"0\",\n",
      "        \"ReportViewer$ctl04$ctl13$ddValue\": \"1\",\n",
      "        \"ReportViewer$ctl04$ctl19$ddValue\": \"4\",\n",
      "        \"ReportViewer$ctl04$ctl21$ddValue\": \"3\",\n",
      "        \"ReportViewer$ctl04$ctl23$ddValue\": \"3\",\n",
      "        \"ReportViewer$ctl04$ctl25$ddValue\": \"3\",\n",
      "        \"ReportViewer$ctl04$ctl39$ddValue\": \"1\",\n",
      "        \"ReportViewer$ctl04$ctl35$ReportViewer_ctl04_ctl35\": \"rbTrue\",\n",
      "        \"ReportViewer$ctl04$ctl15$ReportViewer_ctl04_ctl15\": \"rbTrue\",\n",
      "        \"ReportViewer$ctl04$ctl29$txtValue\": \"\",\n",
      "        \"ReportViewer$ctl04$ctl29$cbNull\": \"on\",\n",
      "        \"ReportViewer$ctl04$ctl33$txtValue\": \"\",\n",
      "        \"ReportViewer$ctl04$ctl33$cbNull\": \"on\",\n",
      "        \"__VIEWSTATEGENERATOR\": \"75CF6949\",\n",
      "        \"hdnCookieConsent\": \"\",\n",
      "        \"hdnCookieAcceptanceRefreshDate\": \"\",\n",
      "        \"hdnCookieAcceptanceRefreshDay\": \"\",\n",
      "        \"hdnCookieAcceptanceRefreshMonth\": \"\",\n",
      "        \"ReportViewer$ctl03$ctl00\": \"\",\n",
      "        \"ReportViewer$ctl03$ctl01\": \"\",\n",
      "        \"ReportViewer$ctl10\": \"ltr\",\n",
      "        \"ReportViewer$ctl11\": \"standards\",\n",
      "        \"ReportViewer$AsyncWait$HiddenCancelField\": \"False\",\n",
      "        \"ReportViewer$ToggleParam$store\": \"\",\n",
      "        \"ReportViewer$ToggleParam$collapse\": \"false\",\n",
      "        \"ReportViewer$ctl08$ClientClickedId\": \"\",\n",
      "        \"ReportViewer$ctl07$store\": \"\",\n",
      "        \"ReportViewer$ctl07$collapse\": \"false\",\n",
      "        \"ReportViewer$ctl09$ScrollPosition\": \"\",\n",
      "        \"ReportViewer$ctl09$ReportControl$ctl04\": \"100\",\n",
      "        \"__ASYNCPOST\": \"true\",\n",
      "        \"ReportViewer$ctl04$ctl00\": \"View Report\",\n",
      "        \"ScriptManager1\": \"ScriptManager1|ReportViewer$ctl04$ctl00\",\n",
      "        \"ReportViewer$ctl05$ctl00$CurrentPage\": \"1\",\n",
      "        \"ReportViewer$ctl09$VisibilityState$ctl00\": \"ReportPage\"\n",
      "    }\n",
      "    \n",
      "    view_state_and_event_validation_inputs = {\n",
      "        input_elem['id']: input_elem['value'] \n",
      "        for input_elem \n",
      "        in soup.find_all('input', type='hidden')\n",
      "        if 'id' in input_elem.attrs and input_elem['id'] in ['__VIEWSTATE', '__EVENTVALIDATION']\n",
      "    }\n",
      "    data.update(view_state_and_event_validation_inputs)\n",
      "\n",
      "    r = session.post(url, params=params, headers=headers, data=data)\n",
      "    r.raise_for_status()\n",
      "    return r\n",
      "\n",
      "with requests.Session() as session:\n",
      "    soup = get_report_viewer_soup(session)\n",
      "    r = get_table(session, soup)\n",
      "\n",
      "r.text\n",
      "```\n",
      "\n",
      "However, this returns Validation Errors for each of the elements in the form, e.g. for the first item `\"ReportViewer$ctl04$ctl03$txtValue\": \"REGO, RO\"`\n",
      "\n",
      "> \"NullValueText\":\"Null\",\"PostBackOnChange\":true,\"RelativeDivId\":null,\"TextBoxDisabledClass\":null,\"TextBoxDisabledColor\":\"#ECE9D8\",\"TextBoxEnabledClass\":null,\"TextBoxId\":\"ReportViewer_ctl04_ctl03_txtValue\",\"TriggerPostBackScript\":function(){__doPostBack('ReportViewer$ctl04$ctl03','');},\"ValidationMessage\":\"Please enter a value for the parameter \\u0027Scheme:\\u0027. The parameter cannot be blank.\",\"ValidatorIdList\":[]}, null, null, $get(\"ReportViewer_ctl04_ctl03\")); });\n",
      "\n",
      "What additional configuration is required to query this site succesfully? Any help would be much appreciated.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1cawtru/tough_to_scrape_site/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to automate file upload through chrome extension scrape?\n",
      "Text: Basically, I’m scraping the current page I’m not based on my chrome extension, and am clicking a button to open the windows file upload GUI through coding. However, I don't know how to upload a file through search through coding. Does anybody here know how to do such a thing? Btw can't use selenium cus it opens a new browser, which I don't want\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1caqtvy/how_to_automate_file_upload_through_chrome/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping restaurant emails…\n",
      "Text: Im trying to scrape company name, email, and phone from Square restaurant sites like this: https://www.firedpizzas.com/\n",
      "\n",
      "First im searching: pizza gmail site:square.site \n",
      "\n",
      "Then im just scrolling and manually grabbing the email and phone from the “location and hours” section which 75% of square sites have.\n",
      "\n",
      "Is there a better way to do this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ca8zwq/scraping_restaurant_emails/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is it legal to scrape YouTube subtitles to use them in a language learning app?\n",
      "Text: Hello! \n",
      "\n",
      "I've found many resources, such as youglish.com and Filmot.com, that scan YouTube channels and allow users to search for phrases. Currently, I'm working on a similar language learning application and would like to give users the ability to see how phrases are used in context and learn pronunciation. To do this, I plan to scan some YouTube channels, download timestamps and subtitles, and then organize searches based on this data. In the app, I intend to display excerpts from the downloaded subtitles and video clips to show how the phrase is used in a sentence. Is it legal to share excerpts from subtitles with users? On one hand, it seems like it is, as there are many websites that do the same. Furthermore, Google itself indexes sites. I've also studied the structure of YouTube's robots.txt. At the moment, to obtain subtitles, I'm using the package [https://github.com/jdepoix/youtube-transcript-api](https://github.com/jdepoix/youtube-transcript-api), which accesses [https://www.youtube.com/api/timedtext](https://www.youtube.com/api/timedtext) and is not listed in the Disallow list.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c9ybpg/is_it_legal_to_scrape_youtube_subtitles_to_use/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is puppeteer-extra-plugin-stealth still working?\n",
      "Text: Run a few tests with puppeteer and stealth plugin. There are numerous online bot tests that are detecting it. This used to work for me a while ago. \n",
      "\n",
      "For example:\n",
      "\n",
      "[https://www.browserscan.net/en/bot-detection](https://www.browserscan.net/en/bot-detection)\n",
      "\n",
      "[https://fingerprint.com/products/bot-detection/](https://fingerprint.com/products/bot-detection/)\n",
      "\n",
      "  \n",
      "I see that the last update on npm [https://www.npmjs.com/package/puppeteer-extra-plugin-stealth](https://www.npmjs.com/package/puppeteer-extra-plugin-stealth) was a year ago, it also looks that this is not maintained actively anymore.\n",
      "\n",
      "Does someone know anything about this?\n",
      "\n",
      "  \n",
      "Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c9i4k3/is_puppeteerextrapluginstealth_still_working/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Anyone uses AI to dynamically overcome obstacles and challenges and page changes?\n",
      "Text: Just wondering if anyone uses an AI to scrape.\n",
      "\n",
      "So if the page changes or there are Captchas or other challenges, it will automatically solve it?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c9kgfo/anyone_uses_ai_to_dynamically_overcome_obstacles/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping a page in R\n",
      "Text: I'm trying to scrape the table from the following webpage:https://www.nasdaq.com/market-activity/stocks/aaa/dividend-history\n",
      "\n",
      "I'm doing so with rselenium in R. However I'm finding that all the actual values of the table are coming up empty. Here's the code I'm using:\n",
      "\n",
      "    library(RSelenium)\n",
      "    rD <- rsDriver(browser = 'firefox', port = 4833L, chromever = NULL)\n",
      "    remDr <- rD[[\"client\"]]\n",
      "    remDr$navigate(paste0(\"https://www.nasdaq.com/market-activity/stocks/aaa/dividend-history\"))\n",
      "    Sys.sleep(11)\n",
      "    html <- read_html(remDr$getPageSource()[[1]])\n",
      "    df <- html_table(html_nodes(html, \"table\"))\n",
      "\n",
      "If I try another url on the same website it works:\n",
      "\n",
      "    library(RSelenium)\n",
      "    rD <- rsDriver(browser = 'firefox', port = 4833L, chromever = NULL)\n",
      "    remDr <- rD[[\"client\"]]\n",
      "    remDr$navigate(paste0(\"https://www.nasdaq.com/market-activity/stocks/a/dividend-history\"))\n",
      "    Sys.sleep(11)\n",
      "    html <- read_html(remDr$getPageSource()[[1]])\n",
      "    df <- html_table(html_nodes(html, \"table\"))\n",
      "\n",
      "I'm not sure why it works for one url but not the other. Hoping someone can explain what's going on and how I get the info in the table.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c9duf7/scraping_a_page_in_r/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping an eBay listing and automatically exporting/updating a google sheet?\n",
      "Text: Does anyone have experience with tracking prices of specific eBay listings on google sheets in a way that it can constantly be updated? \n",
      "\n",
      "I want to essentially track the data of a single seller’s listing to watch the price fluctuations.\n",
      "\n",
      "I am familiar with python webscraping and google sheets but I can’t say I’ve done them in tandem.\n",
      "\n",
      "TIA\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c8y66r/scraping_an_ebay_listing_and_automatically/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Google Voice Auto Message\n",
      "Text: Hi all, first post here and might not be the right subreddit but please let me know if there’s a better sub to ask this question. \n",
      "\n",
      "I’m wanting to send some text messages thru Google voice every hour. I already have the program to click around and send the messages. To protect my Gmail account from getting banned I want to make an alternate account just for Google voice. Do you guys think Google would detect or ban a brand new account that immediately starts sending a few messages an hour? I’m planning to only send about 5 an hour for 12 hours a day so it’s nothing too crazy. Thanks! \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c94s0p/google_voice_auto_message/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: New to Coding, I need a web scraper for Idealista, it would be cool to learn, but is this just really time inefficient?\n",
      "Text: What do people think?\n",
      "\n",
      "Find a service or follow a guide?\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c8v3fi/new_to_coding_i_need_a_web_scraper_for_idealista/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is my website being \"webscraped\"?\n",
      "Text: So i know nothing about Webscraping or its purposes, but figured this community might be a good place to ask this question.\n",
      "\n",
      "Background: I'm the marketing coordinator for a Chamber of Commerce and like to use statistics to help sell our membership packages that include our marketing benefits. One benefit is a rotating banner ad on our online directory, so I like to throw out statistics related to our web traffic. \n",
      "\n",
      "Situation: According to our site host, over 100k searches have been conducted on our directory, which on paper seems great. But we're a community of 45k people (120k in the county). What used to be 50-100 searches a day has turned into 500, 600, up to 1,000 searches per day, and our stats show a random spike around the same time each month, resulting in upwards of 6,000 search or more in a single day. \n",
      "\n",
      "For gits and shiggles, I asked Meta's AI what could be causing this spike and it suggested webscraping (among other things like scheduled bots or crawlers). Does the evidence provided suggest webscraping?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c83k86/is_my_website_being_webscraped/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Python Wrapper For Meta AI (Llama 3)\n",
      "Text: https://github.com/Strvm/meta-ai-api\n",
      "\n",
      "With the release of Meta's latest LLM (Llama 3),\n",
      "\n",
      "here is just a small wrapper to interact with the new MetaAI chat bot assistant with Python (https://www.meta.ai/), which is running the newly release Llama 3 model.\n",
      "\n",
      "Another nice thing is that its directly connected with Bing/Google so you will be able to get the latest informations and the ability to get the sources from the response.\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      "from meta_ai_api import MetaAI\n",
      "\n",
      "ai = MetaAI()\n",
      "response = ai.prompt(message=\"Whats the weather in San Francisco today?\")\n",
      "print(response)\n",
      "```\n",
      "result:\n",
      "\n",
      "```json\n",
      "{\n",
      "   \"message\":\"The weather in San Francisco, California, today includes ¹:\\nNo precipitation with skies ranging from clear to cloudy\\nWind speeds range from 0 to 21 miles per hour\\nTemperatures range from 52 to 69 degrees Fahrenheit\\nPlease note that the weather forecast is continually changing ² ³ ⁴.\\n\",\n",
      "   \"sources\":[\n",
      "      {\n",
      "         \"link\":\"https://www.wolframalpha.com/input?i=San+Francisco+weather+today\",\n",
      "         \"title\":\"WolframAlpha\"\n",
      "      },\n",
      "      {\n",
      "         \"link\":\"https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629\",\n",
      "         \"title\":\"San Francisco, CA Weather Forecast - AccuWeather\"\n",
      "      },\n",
      "      {\n",
      "         \"link\":\"https://weather.com/weather/tenday/l/San+Francisco+CA+USCA0987:1:US\",\n",
      "         \"title\":\"10-Day Weather Forecast for San Francisco, CA\"\n",
      "      },\n",
      "      {\n",
      "         \"link\":\"https://weather.com/weather/tenday/l/Inverness+CA?canonicalCityId=61b2ebcaa5e78eebca92d21eaff7a0439eb081e8e60287fca37af4186f8242b7\",\n",
      "         \"title\":\"10-Day Weather Forecast for Inverness, CA\"\n",
      "      }\n",
      "   ]\n",
      "}\n",
      "```\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c81a89/python_wrapper_for_meta_ai_llama_3/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: No experience webscraping; wanted to webscrape Twitter; how?\n",
      "Text: Hello, I am a complete beginner when it comes to webscraping. We have a research project that needs earthquake tweets from various Twitter accounts or social bots that tweet earthquake details. Such as:  \n",
      "[https://twitter.com/phivolcs\\_dost](https://twitter.com/phivolcs_dost)\n",
      "\n",
      "The purpose of the research is to identify accounts or social bots that tweet inaccurate details regarding the seismic events.\n",
      "\n",
      "I wanted to Webscrape specific tweets from a specific account or page. And if possible, it should be time-specified, like \"From December 1, 2023, to March 1, 2024,\" and should have the keywords \"earthquake.\" and \"Philippines\" on it.\n",
      "\n",
      "Data points:  \n",
      "1. Tweet Text  \n",
      "2. Timestamp (date and time)  \n",
      "3. No. of Views or Likes\n",
      "\n",
      "Would you guys share some codes (github), articles, or tutorials for me who is a complete newbie? I would really appreciate it.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c7xwiy/no_experience_webscraping_wanted_to_webscrape/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there anyway to webscrape from current browser I opened manually?\n",
      "Text: Basically, I have a browser currently open and I want to webscrape with it through code. How do I do this? In some youtube videos with selenium, they had to re-open the browser through another session, but I don't want to do it.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c81wnn/is_there_anyway_to_webscrape_from_current_browser/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can you make a full-time income Webscraping?\n",
      "Text: Greetings, I'm curious if Webscraping can provide a full-time income. If it is possible, could you please tell me where to start studying the requisite skills?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c728vv/can_you_make_a_fulltime_income_webscraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping multiple \"layers\" of FB comments\n",
      "Text: I'm wondering if anyone's familiar with a method of scraping all comments from a post on a public FB page? I have not been able to scrape multiple layers of comments (i.e. comments in reply to other comments), but maybe you know of a solution? Any help is much appreciated, as it's part of my thesis. \n",
      "\n",
      "Cheers. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c6zlaw/scraping_multiple_layers_of_fb_comments/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping G Maps Data to find retail stores\n",
      "Text: Hi,\n",
      "I'm trying to find a way to scrape Google maps data to get information on retail stores of a particular name in my country. I just need the Name, Address, GPS Coordinates, State, City and Avg Reviews. \n",
      "\n",
      "Is there a way I can do this using Google Maps API? I'm expecting less than 25k results. Is there a way to do this for free?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c6whta/scraping_g_maps_data_to_find_retail_stores/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: LinkedIn Profile urls\n",
      "Text: Hi everyone,\n",
      "\n",
      "I'm looking to extract LinkedIn profile URLs for individuals working at specific companies, and then use a service to gather more detailed information about these profiles. What would be the best approach for this?\n",
      "\n",
      "I've tried using search engines like the Bing Search API, Google Search API, and Brave Search API, specifying the website domain (site:linkedin.com/in/), but the results yielded only about 300 records. However, I need approximately 10 million profile URLs.\n",
      "\n",
      "I am particularly interested in data from employees of companies, which generally isn't included in existing LinkedIn profile databases.\n",
      "\n",
      "Any suggestions would be greatly appreciated. Thanks in advance!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c6zolx/linkedin_profile_urls/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Extract search results (posts) from a private Facebook group of which I'm a member using web scraping techniques\n",
      "Text: Hi everyone i want to know if there's a way to extract a private group search results ( POSTS) on facebook that i'm member of , is there any open source Facebook API i can use? any suggestions please\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c7010k/extract_search_results_posts_from_a_private/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Avoid account ban\n",
      "Text: I am scrapping a website which i need to be logged. What can I do to avoid getting banned? I would be scrapping every 5 minutes (doing 100 clicks every 5 minutes).\n",
      "\n",
      "Any ideas to avoid ban? Thanks\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c6j1rd/avoid_account_ban/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Advices on Scaling Scrapers?\n",
      "Text: If you had to scrape lots of data, how do you scale scrapers, where do you keep the state and logic so scrapers wont be scraping the same thing?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c67hsp/advices_on_scaling_scrapers/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How could I scrap job from multiple pages on GlassDoor? Not just the first page\n",
      "Text: I'm working on a GlassDoor job scraper. Right now I have a loop going through 5 URLs (each a different job title search) and have it exporting to a CSV. It's working great so far!  \n",
      "\n",
      "\n",
      "The only issue I am having is that I can only pull the first 30 results since that is all the is loaded by default. There is a \"Show More\" button that loads more jobs to the list, not moves to a whole 2nd page. I'm struggling with how to pull those jobs as well as the original 30 for all 5 links.  \n",
      "\n",
      "\n",
      "From my understanding I have to sent the network request to load more, but I have no idea how to do that, this is my first project. If anyone could help me figure out how to pull more jobs that would be great!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c6ehd4/how_could_i_scrap_job_from_multiple_pages_on/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape web page\n",
      "Text: Dear all!  \n",
      "I need your support please.  \n",
      "\n",
      "\n",
      "I'd like to obtain all the pdf of this webpage: [https://servicio.mapa.gob.es/regfiweb#](https://servicio.mapa.gob.es/regfiweb#)  \n",
      "\n",
      "\n",
      "Once you enter at the webpage:\n",
      "\n",
      "* Select: Buscadores.\n",
      "* Select: Productos.  \n",
      "\n",
      "\n",
      "And arrives to the table below:\n",
      "\n",
      "https://preview.redd.it/t2rn2rjim0vc1.png?width=1183&format=png&auto=webp&s=e543252b2a3d2f64fcc9e06703aac5c401cada7b\n",
      "\n",
      "My code is the next one at the spider:  \n",
      " \n",
      "\n",
      "`import scrapy`  \n",
      "\n",
      "\n",
      "`class SimpleSpider(scrapy.Spider):`  \n",
      " `name = \"simple\"`  \n",
      " `#allowed_domains = [\"x\"]`  \n",
      " `start_urls = [\"https://servicio.mapa.gob.es/regfiweb#\"]`  \n",
      " `def parse(self, response):`  \n",
      " `for book in response.css('.col'):`  \n",
      " `title = book.css('span ::text').get()`  \n",
      " `link = response.urljoin(`  \n",
      " `#book.css('a.pdf ::attr(href)').get()`  \n",
      " `book.css('a::attr(href)').get()`  \n",
      "`)`  \n",
      " `yield {`  \n",
      " `'Title' :title,`  \n",
      " `'file_urls' : [link]`  \n",
      "`}`\n",
      "\n",
      "But I'm not obtaining annything, I think I've problems at the title and the book codes to obtain the pdf link.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Do you know why?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c66ufb/scrape_web_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: consequences to web scraping every minute/hour/day\n",
      "Text: Let's say I want to scrape a website every minute. Is that viable? Or will my IP address likely be banned? What if it was every hour instead? What if it was every day?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c5s5yt/consequences_to_web_scraping_every_minutehourday/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: [Open-source] Chrome extension to automatically scrape website tables/lists\n",
      "Text: Rows.com open-sourced RowsX, a Chrome extension web scraper.\n",
      "\n",
      "It is under the MIT License so that anyone can fork and contribute, including to their open bounties program.\n",
      "\n",
      "Repo: [https://github.com/rows/X](https://github.com/rows/X)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c5jjsr/opensource_chrome_extension_to_automatically/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selenium for Web Apps?\n",
      "Text: Hi! I was wondering if it is possible to integrate a Selenium script to a web app. For example, the client submits a request for scrapping some data that is only possible to scrape with a Selenium Script or other web automation tool. Then, on the server side, the selenium script executes and gets the info. Is something like that possible? Is it conventional? Or what is a common practice for this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c5h3lw/selenium_for_web_apps/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Hashtag scraping for research project (X/IG/FB)\n",
      "Text: Hi there,\n",
      "\n",
      "I am after some advice please. I am working on a research project that aims to investigate two hashtags, preferably on X. After having done some digging it seems that there's no free/legal way to do that with X's new API  anymore (I have been looking at tweepy and snscrape). \n",
      "\n",
      "I want to propose to the two primary researchers of the project to switch to a different platform, such as Instagram or Facebook, but I can't find much information on how hard or easy it is to scrape either of them - lots of outdated articles etc. Are IG and FB just as bad as X or would it be feasible to do it there? Any help would be much appreciated.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c5m77y/hashtag_scraping_for_research_project_xigfb/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to Obtain Data for Journalist Discovery\n",
      "Text: Hey everyone,\n",
      "\n",
      "I'm currently working on developing a platform to assist startups in pitching journalists for media coverage, and I could really use some advice on obtaining the necessary journalist data to make it happen.\n",
      "\n",
      "As part of our efforts to build a comprehensive Journalist Discovery Module, we're looking to gather essential data to facilitate the identification and connection with relevant journalists. Here's a list of the data we need:\n",
      "\n",
      "1. Email Addresses of Journalists\n",
      "2. Recent Articles Written by Journalists (with publication details and dates)\n",
      "3. Social Media Profiles of Journalists (e.g., Twitter, LinkedIn)\n",
      "4. Topics Covered by Journalists\n",
      "\n",
      "If you've got any ideas how we can access this data, I'd be eternally grateful for your guidance!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c5k51g/how_to_obtain_data_for_journalist_discovery/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Capturing session tokens in headless selenium\n",
      "Text: My goal is to extract a session token from the network response header after querying a JS form. \n",
      "\n",
      "I am using Selenium with headless Chrome to capture network traffic and extract the token from JSON.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "https://preview.redd.it/q4f54wj8psuc1.png?width=375&format=png&auto=webp&s=955882f6a4cf03f5cfc7a1cb67f577114d56713c\n",
      "\n",
      "The token is generated after querying a JS form, so I simulate this with a static selenium function to reproduce the necessary request.  The session token itself is in the response header in the network tab. I extract it using a python function like this:\n",
      "\n",
      " `def fetch_token(logs):`    \n",
      "\n",
      " `for entry in logs:`         \n",
      "\n",
      "`log = json.loads(entry[\"message\"])[\"message\"]`     \n",
      "\n",
      "`return log['params']['request']['headers']['Token']` \n",
      "\n",
      "So far this works, but not all the time. I THINK occasionally the website throttles my requests and the token can't be found. I've worked around this with a simple while loop that will retry several times until the token is found. I containerized this using docker and it seems very reliable. The token is captured first time, sometimes by the 2nd or 3rd retry. \n",
      "\n",
      "https://preview.redd.it/7uk9zcvdqsuc1.png?width=591&format=png&auto=webp&s=3cc7a9dec1c1f9d9904e381052ccc698c6e9971f\n",
      "\n",
      "I think okay, problem solved. Deploy to AWS ECR and execute with lambda. I get hit with errors realting to devtools not found. I add more chrome driver options (I used [this git](https://github.com/umihico/docker-selenium-lambda) as a reference) and it seems to get the function to run but its reliability has been really poor. Like, token is captured 50% of the time. \n",
      "\n",
      "My dockerized version with fewer options seems to work fine and I've been scratching my head about if thats really the cause of poor performance. \n",
      "\n",
      "Any advice on what to do here? Any pointers would help and save me a lot of hair.\n",
      "\n",
      "TIA!  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c5ahhk/capturing_session_tokens_in_headless_selenium/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Filtering websites for AI\n",
      "Text: What are the tags, classes, ... you always filter out to remove any irrelevant content for downstream work with AI (e.g. LLMs, classifiers,...)? \n",
      "\n",
      "Are there any great parsers out there to parse the website content beyond the Mozilla one? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c56guv/filtering_websites_for_ai/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any way to find the key of a specific item in a value of json\n",
      "Text: Any way to find the key of a specific item in a value of a json file. Basically, what I mean by key is the key of the hashmap of which the item I'm using for data is in the value of that key, and the key of that key, and the key of that key, and so on. It's kind hard to look at the lines through json. Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c52873/any_way_to_find_the_key_of_a_specific_item_in_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for Web Scrapping Dev. and need your advice to structure Job Desc. to post on freelance platforms.\n",
      "Text: Hi, I need a custom scrapper which will go and scrape job post from around 1,000 different websites. This data should flow to Airtable and needs to be updated at least once a week to reflect if job is still active or closed. I need to have ability to maintain this new tool by myself going forward. I looked at No Code Web Scrapping solutions but those are very pricey and most of them have subscription model. It seems that building custom scrapper is my best option. I'm thinking to post this job on some Freelance platform like Freelancer or Upwork but not sure what skill my future Dev. should have. Should I look for Python Dev. with expertise in Web Scrapping? Are there any other skills my Dev. should have to successfully complete this? Also what is a good price range for this kind of job?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      " \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c4y2tk/looking_for_web_scrapping_dev_and_need_your/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to skip position in queue\n",
      "Text: Guys, I'm from a football team (Boca Juniors), from argentina.\n",
      "\n",
      "I pay the membership, but when you want to go to the stadium, the club notifies you that X day you need yo be at Y time in your pc and go through the virtual queue (it seems like it uses queue it software).\n",
      "\n",
      "Im frustrated because 5 minutes go by and all the tickets are gone. I talked to a guy that charges u 50 USD and he gets the ticket for you (he is legit)\n",
      "\n",
      "He basically *skips* this queue, and so he gets the ticket for his clients and himself, while mortals like me dont.\n",
      "\n",
      "Id like to code something like this to avoid paying 50 usd every time I wanna go. I already pay my monthly membership.\n",
      "\n",
      "What can I do? I dont know where to start.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c51e3e/how_to_skip_position_in_queue/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Where to begin Web Scraping\n",
      "Text: Hi I'm new to programming as all I know is a little Python, but I wanted to start a project and build my own web scraper. The end goal would be for it to monitor Amazon prices and availability for certain products, or maybe even keep track of stocks, stuff like that. I have no idea where to start or even what language is best for this. I know you can do it with Python which I initially wanted to do but was told there are better languages like JavaScript which are faster then Python and more efficient. I looked for tutorials but was a little overwhelmed and I don't want to end up going down too many rabbit holes. So if anyone has any advice or resources that would be great! Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c4jd72/where_to_begin_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do you approach website monitoring?\n",
      "Text: If I want to monitor a website for changes (it might be new text on the website or a new link on a collections page), how would you approach it? \n",
      "\n",
      "1. Take the entire content and hash it. \n",
      "2. Store the relevant parts and see if they match or something new pops up (e.g. a new link)? But then how would you deal with changes in the path structure the website uses? (e.g. additionally storing webpage hashes and comparing)?  \n",
      "\n",
      "\n",
      "I would love to find a robust solution. Any tips and tricks are welcome. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c56j9r/how_do_you_approach_website_monitoring/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do you go about finding certain pages on websites?\n",
      "Text: I have to look for certain information on websites, which is typically on certain parts of the website (e.g. CEO name in the impressum, product / service portfolio on product / services sites).   \n",
      "\n",
      "\n",
      "But the path is always different.   \n",
      "\n",
      "\n",
      "How would you go about finding the correct path / paths? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c56g11/how_do_you_go_about_finding_certain_pages_on/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping Knowledge Chart\n",
      "Text: I have been webscraping for around 3-4 years\n",
      "\n",
      "I am quite familiar with Selenium, Beautiful Soup and some other libraries, but I have largely learnt webscraping as a way to get what I wanted for a particular project.\n",
      "\n",
      "If someone could give a concept chart of webscraping from basic to advanced concepts i would be grateful.\n",
      "\n",
      "I have tried to Google this, I mostly find stuff that I already know and lot of it seems like the basics so it isn't very useful\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c51d2c/webscraping_knowledge_chart/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is it feasible/possible to scrape Workday?\n",
      "Text: I've been learning a bit about web scraping but I found this one website that uses workday for it's career page and when I scrape it there's almost nothing there.\n",
      "\n",
      "I know it's all being loaded afterwords, but I can't seem to find any resources for how to scrape a website like this.\n",
      "\n",
      "Is scraping websites like workday possible?\n",
      "\n",
      "[For example](https://walmart.wd5.myworkdayjobs.com/WalmartExternal)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c55n3h/is_it_feasiblepossible_to_scrape_workday/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: URL not pointing to the right page for scraping\n",
      "Text: Hi, I'm trying to scrape URLs like this page: \n",
      "\n",
      "[https://salesweb.civilview.com/Sales/SaleDetails?PropertyId=1273528127](https://salesweb.civilview.com/Sales/SaleDetails?PropertyId=1273528127) (see image) \n",
      "\n",
      "https://preview.redd.it/fuqx34qlkpuc1.png?width=2085&format=png&auto=webp&s=9766712ef795a15678bb5524c241c8fda43439cc\n",
      "\n",
      "\n",
      "\n",
      "using a no-code platform but it seems when the scraping bot clicks on that URL, the intended URL automatically reverts to this URL: [https://salesweb.civilview.com/Sales/SaleDetails?](https://salesweb.civilview.com/Sales/SaleDetails?PropertyId=1273528127) which is the homepage thereby not able to scrape what I intended. How do I stop this from happening? Hoping someone can help. Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c4xm07/url_not_pointing_to_the_right_page_for_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: **I'm new to this** Why is GlassDoor giving me a 403 Status?\n",
      "Text: I am trying to scrape job info from glass door. Similar code structure worked fine for LinkedIn, but I'm having issue here. I'm getting a Status Code of 403 so I'm guessing I'm running into some sort of wall on the GlassDoor back end. I was wondering if anyone could help me figure out why.\n",
      "\n",
      "Code:\n",
      "\n",
      "    #Define URL\n",
      "    url = \"https://www.glassdoor.com/Job/united-states-it-entry-level-jobs-SRCH_IL.0,13_IN1_KO14,28.html\"\n",
      "    \n",
      "    #Retrieve HTML Content\n",
      "    response = requests.get(url)\n",
      "    html = response.content\n",
      "    \n",
      "    #Parae HTML\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "    \n",
      "    #Pull Job Titles\n",
      "    jobListing = soup.find_all('div', {'class': 'jobCard JobCard_jobCardContent__X81Ew'})\n",
      "    for div in jobListing:\n",
      "        #Using this to test if it finds the div\n",
      "        print(\"Found Div\")\n",
      "    else:\n",
      "        print(\"Didn't find Div\")\n",
      "    #    for a in div.find_all('a', {'class': 'JobCard_jobTitle___7I6y'}):\n",
      "    #        print(a.text.strip())\n",
      "    \n",
      "    if response.status_code == 200:\n",
      "        print(\"Yes\")\n",
      "    else:\n",
      "        print(response.status_code)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c4ulqu/im_new_to_this_why_is_glassdoor_giving_me_a_403/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trying to create a docker image of my web crawler\n",
      "Text: I've built a web crawler and scraper with selenium and python and would like to set up a docker image mainly to learn since there will be like two computers that will use it. So far I've tested my script and it works like a charm... except in headless mode and it seems I'm forced to make it work in headless mode in docker (or so I've understood). So if there is someone with experience in this subject I would gladly take your help\n",
      "\n",
      "The options I use so far are:\n",
      "\n",
      "        opciones = ChromeOptions()\n",
      "        opciones.add_argument(\"--incognito\")\n",
      "        opciones.add_argument(\"--no-sandbox\")\n",
      "        opciones.add_argument(\"--disable-dev-shm-usage\")\n",
      "        opciones.add_argument(\"--remote-debugging-port=9222\")\n",
      "        opciones.add_argument(\"--disable-gpu\")\n",
      "        opciones.add_argument(\"--ignore-certificate-errors\")\n",
      "        opciones.add_argument(\"--headless\")\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c4xiwz/trying_to_create_a_docker_image_of_my_web_crawler/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Implications of scraping Workday?\n",
      "Text: This probably did get asked before or was discussed. But I got tired of job searching. I put together a python script that uses selenium to scrape the title url and description of jobs from engineering companies I'm interested in. Some use workday, some don't. For work day, people have already made scripts to scrape it since it's mostly the same html format. Other companies I'm doing manually. I added my own filter to only get me things that match my resume or my experience. Amount of times I've opened a job posting only to find out \"10 years experience minimum\"... Anyways the script is meant to scrape and give me best matches. What implications might this have specifically when targetting companies under workday? I fear getting IP banned or something and then I'm out of luck finding a job for sure lol. Let's say I'm scraping 20 workday companies every 3 days or so, will I fly under the radar? I know there's methods we can use like slowing the bot down, randomizing it, maybe using a VPN but honestly not sure how effective they are. Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c4l59t/implications_of_scraping_workday/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Httrack: How to download only a websites directory (nothing else)?\n",
      "Text: \n",
      "Hi, i am currently using HTTRACK in order to scrape a website, however i  want to download and view only a certain portion of a website, like a directory.\n",
      "\n",
      "I'll set [example.com](http://example.com) for instance. I want httrack to scrape stuff specifically from: [https://www.example.com/directory](https://www.example.com/directory), but not from the entirety of https://www.example.com.\n",
      "\n",
      "How do i do that?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c4vs0c/httrack_how_to_download_only_a_websites_directory/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Limited return of scraped text?\n",
      "Text: When I scrape the corresponding webpage I receive (in my terminal)  different results most iterations. One scrape may return pages 1-50 and the next scrape may return pages 79-99.  The document I am trying to scrape has 99 pages. I am unsure of if my problem is related to how much my terminal will output or possibly the .evaluate() method having a limit on what it can scrape or output. I don't think it is a problem where the scraper is executing prior to page load because of the .wait\\_for\\_load\\_state('networkidle') method. Any help would be appreciated.\n",
      "\n",
      "\n",
      "\n",
      "    import asyncio\n",
      "    from playwright.async_api import async_playwright\n",
      "    \n",
      "    async def scrape_page_text(page):\n",
      "        try:\n",
      "            # Wait for the content to fully load\n",
      "            await page.wait_for_load_state('networkidle')\n",
      "            \n",
      "            # Evaluate JavaScript code to get the text content of the body\n",
      "            page_text = await page.evaluate('document.querySelector(\"body\").innerText')\n",
      "            return page_text\n",
      "        except Exception as e:\n",
      "            print(\"Error scraping page text:\", e)\n",
      "            return None\n",
      "        \n",
      "    async def main():\n",
      "        async with async_playwright() as p:\n",
      "            browser = await p.chromium.launch(headless=False)\n",
      "            page = await browser.new_page()\n",
      "            await page.goto(\"https://www.sec.gov/edgar/search/#/dateRange=custom&entityName=crm&startdt=2023-03-16&enddt=2024-04-10&filter_forms=10-K\")\n",
      "    \n",
      "            try:\n",
      "                # clicks the initial link/pauses code\n",
      "                await page.click(\"a.preview-file\", timeout=60000)\n",
      "                input(\"Enter to continue\")\n",
      "    \n",
      "                # waits for modal to appear\n",
      "                await page.wait_for_selector('.preview-file', timeout=60000)  # Wait for the modal to appear\n",
      "                await page.click('.btn.btn-warning')\n",
      "    \n",
      "                # Wait for the new tab to open\n",
      "                new_page = None\n",
      "                for _ in range(30):  # Try waiting for up to 30 seconds\n",
      "                    new_page = await browser.contexts[0].wait_for_event('page', timeout=1000)\n",
      "                    if new_page:\n",
      "                        break\n",
      "    \n",
      "                if new_page:\n",
      "                    # Capture the new URL\n",
      "                    new_url = new_page.url\n",
      "                    print(\"New URL:\", new_url)\n",
      "    \n",
      "                    # Scrape text from the new tab\n",
      "                    page_text = await scrape_page_text(new_page)\n",
      "                    print(\"Page text:\", page_text)\n",
      "                    print(\"Page content scraped successfully from the new tab.\")\n",
      "                else:\n",
      "                    print(\"Timeout waiting for the new page.\")\n",
      "    \n",
      "            except TimeoutError:\n",
      "                print(\"Modal did not appear or took too long to appear.\")\n",
      "            except Exception as e:\n",
      "                print(\"An error occurred:\", e)\n",
      "    \n",
      "            await browser.close()\n",
      "            print(\"Browser closed.\")\n",
      "    \n",
      "    # Run the async function\n",
      "    if __name__ == \"__main__\":\n",
      "        asyncio.run(main())\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c4o936/limited_return_of_scraped_text/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is Amazon Jobs unscrapable?\n",
      "Text: I've been trying to scrape this site for the past few days (for a personal project. Still a beginner): [https://www.amazon.jobs/en/search?base\\_query=&loc\\_query=#](https://www.amazon.jobs/en/search?base_query=&loc_query=#) \\- but I can't get the data I want. I've been trying different ways to obtain it dynamically, but to no avail. Also, when I click disable JS, then enable JS, there are no requests after I refresh the page for some reason. Does anyone know how to go about this if possible? Let me know if this post isn't allowed, and I'll take it down. Tried scrapy, beautiful soup, normal get requests and nothing works btw\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c44svr/is_amazon_jobs_unscrapable/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Use API or Scape Page?\n",
      "Text: Previously I was able to reverse-engineer and utilize their API to get all the data I needed. Since then, they've made some changes and now I can no longer access API because of cloudflare. Cloudflare also blocks the request from Postman.\n",
      "\n",
      "My question is, I've discovered this package https://github.com/zfcsoftware/puppeteer-real-browser from browsing this subreddit. I am curious if this could be used to access the API or does this package work by loading the page and scraping its elements? If the latter, that process would be slower than directly accessing their API. I wonder, if there is away to get past cloudflare and utilize API requests. Any ideas?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c41j0r/use_api_or_scape_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Video blocked in Selenium, trying to scrape m3u8 programmatically\n",
      "Text: [https://hd2watch.tv/watch-series/under-the-gun-hd-108058/1430602](https://hd2watch.tv/watch-series/under-the-gun-hd-108058/1430602)  \n",
      "\n",
      "\n",
      "\\-Firefox- Video player works, with ublock origin installed I can see m3u8 in network tab  \n",
      "\\-Chrome- Video player works, with ublock origin installed I can see m3u8 in network tab  \n",
      "\n",
      "\n",
      "Without ublock I can't see m3u8 in network tab  \n",
      "\n",
      "\n",
      "Video player blocked totally in Selenium, can't see m3u8 in network tab.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c3uekk/video_blocked_in_selenium_trying_to_scrape_m3u8/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I extract\n",
      "Text: I'm building a scraper for a real estate website. I have the scraping logic for the details page. Now i need to build the logic to go through all provinces/municipalities and identify the listings which then my logic for the details page can scrape.\n",
      "\n",
      "Now my question... how can I identify the total available values that this \"intelligent\" search field in the screen shot is querying? \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "[https://www.immobiliare.it](https://www.immobiliare.it)\n",
      "\n",
      "https://preview.redd.it/hdy0wh02weuc1.png?width=1161&format=png&auto=webp&s=e4d13737e38b4acb774744d124777c6f1bd5bba1\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c3pi5f/how_can_i_extract/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to get Authorization token?\n",
      "Text: Hello. I am working on Italy basketball league website. There is a way to reach stats json file by doing request to specific page.\n",
      "\n",
      "But there is one problem. They request Authorization token, which is temporary and changes I think each hour or couple of hours.\n",
      "\n",
      "Is there a way to get that token? Thanks in advance!\n",
      "\n",
      "https://preview.redd.it/usrf0mvfseuc1.png?width=1274&format=png&auto=webp&s=c0fac24f2605d5a34fd6530b1f1fadcfeb4e4b75\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c3p200/how_to_get_authorization_token/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Instagram DM messages\n",
      "Text: I get a lot for instagram DM request messages and hidden messages that I would like to scrape. Anyone accomplish this or a tool to do it? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c3ltun/instagram_dm_messages/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: OCR alternatives \n",
      "Text: I'm scraping some game data in an android emulator using pyautogui + screenshots + pytesseract (its the only way I found to do it).\n",
      "\n",
      "The thing is, pytesseract is terrible when talking about consistency. If I run it in the same image 10 times it sure will be giving different results at least once.\n",
      "\n",
      "Do you guys work with any other OCR tools? For some reason easyocr doesn't work in my PC.\n",
      "\n",
      "If you have any other suggestions for scrapping data in a mobile game instead of using OCR I would like to hear. Proxy doesn't seems to work for this case.\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c3a7lr/ocr_alternatives/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Instagram followers?\n",
      "Text: Unfortunately, a little while ago xemailextractor was shut down, which is tragic. \n",
      "Are there any alternatives now to scraping emails of followers on instagram of a certain page within the niche for cold email purposes?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c333r7/scraping_instagram_followers/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Legality of using scraped star ratings\n",
      "Text: Hi all,\n",
      "\n",
      "Im currently playing around with some ideas that involve aggregated \"star\" ratings like you would find on eg Apple Podcasts. As far as I understood, scraping them is not a big issue. But what about using them in another service (eg for sorting/filtering)?\n",
      "\n",
      "Appreciate any insights or hints where to read up on this, thx!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c3c1iu/legality_of_using_scraped_star_ratings/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best way to run Selenium on personal machine\n",
      "Text: Hi guys,\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I have multiple selenium scripts running on my personal machine (windows). Problem is that every now and then the selenium window needs to be closed and reopened and it gets pretty annoying since I cannot open it in headless mode and I need to maximize the window as well.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Is there anyway to run the script 'silently'? I considered creating a VM by using virtualbox but it eats up quite a lot of RAM (4gb or a windows instance).\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Any help is much appreciated! Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c36nh5/best_way_to_run_selenium_on_personal_machine/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Database with publishers\n",
      "Text: Hey everyone,\n",
      "\n",
      "Is it possible to use web scraping to build a database (including website name and URL) containing all blogs and publishers from a specific country?\n",
      "\n",
      "But how can I distinguish between publishers such as blogs, online magazines, online newspapers, etc., and companies that maintain private blogs?\n",
      "\n",
      "I'm specifically interested in identifying publishers that accept advertising, rather than companies that host their own blogs and are not interested in advertising.\n",
      "\n",
      "How are these extensive databases typically created?\n",
      "\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c383h4/database_with_publishers/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Solving Captchas using bs4 and Scrapy + 2Captcha\n",
      "Text: If  you follow the documentation, you can easily solve the captchas using scrapy. \n",
      "\n",
      "But for bs4, the documentation doesn't seem to work. \n",
      "\n",
      "Let's solve recaptcha using bs4;\n",
      "\n",
      "The documentation has:\n",
      "solver = solver.recaptcha(sitekey=sitekey, url=url)\n",
      "\n",
      "This does not work for bs4. Use this instead:\n",
      "\n",
      "solver = solver.solve_captcha(site_key=sitekey, page_url=url)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c34mr3/solving_captchas_using_bs4_and_scrapy_2captcha/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What is a good way to share scraped data that is updating every 5-10 minutes?\n",
      "Text: Anybody tried sharing scraped data that is constantly updating?\n",
      "\n",
      "I run a group chat on telegram that sends members messages any time new information (tasks) is detected from one of 10-15 websites. The issue is that the messages are the only way for members to track the tasks. I've cleaned the information and made a universal format (each site posts their information differently). Each task is now a row in a spreadsheet. The members are not tech savvy. I want to make a large table that has every task. I've tried google sheets but there are limits on update frequency that make it impractical.\n",
      "\n",
      "Ideally, the table would be able to be filtered. I'd also like the ability to update frequently.\n",
      "\n",
      "Anybody have experience with something like this? If not do you have suggestions?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c2of23/what_is_a_good_way_to_share_scraped_data_that_is/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What database should I choose? Scraping bookmaker odds\n",
      "Text: Hello scraper brothers and sisters!\n",
      "Need some expert advice :-) I am scraping a betting site for their odds every hour. What database would you suggest I use? What are you using, and why? I am likely going to access (rw) the data at the same time through the use of threads. Interested to hear what you are doing!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c2y6mp/what_database_should_i_choose_scraping_bookmaker/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking to scrape emails\n",
      "Text: Hello All,\n",
      "\n",
      "Looking for a tool to scrape emails from profiles that have specific hashtags or interest areas on their profiles. An example would be youtubers that are writers etc. Plenty of tools seems to exist for pulling emails from linkedin but cant seem to find any good ones for IG, Youtube etc.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c2d0mg/looking_to_scrape_emails/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is AI really replacing web scraper\n",
      "Text: I see many top web scraping companies using AI scraper. Have you guys tried using them.  Do you really think they work perfectly?\n",
      "Will we be replaced?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c1xh1g/is_ai_really_replacing_web_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help with this function in Puppeteer to scrape some links in multiple pages.\n",
      "Text: Hello,\n",
      "\n",
      "So for personal project I am working on a fictional travel site that scrapes some info from this site here: [https://www.giardinodininfa.eu/collections/giardino-di-ninfa](https://www.giardinodininfa.eu/collections/giardino-di-ninfa)\n",
      "\n",
      "The first step is to scrape the links for all the various available dates on all 5 of the pages. Unfortunately my function `linksToScrape` does not seem to be working well. It appears it gets stuck in an infinite loop and I don't know why. The function `linksCurrentPage` works as intended and scrapes the link of the current page. However  using console.logs it seems the conditional if and else statement inside the do...while loop do not seem to be activated at all and I can't tell  why.\n",
      "\n",
      "Can anybody help?\n",
      "\n",
      "    async function linksToScrape () {\n",
      "    \t\tlet collectionOfLinks = [];\n",
      "    \n",
      "    \t\tlet lastPage = false;\n",
      "    \n",
      "    \t\tdo {\n",
      "    \t\t\tcollectionOfLinks = collectionOfLinks.concat(await linksCurrentPage());\n",
      "    \t\t\tconsole.log(collectionOfLinks);\n",
      "    \t\t\tconst nextLink = await page.$('.pagination > li:last-child a');\n",
      "    \t\t\tconsole.log(await page.evaluate(x => x.href, nextLink)); \n",
      "    \t\t\tif (!nextLink) {\n",
      "    \t\t\t\tlastPage = true;\n",
      "    \t\t\t}\n",
      "    \t\t\tconsole.log(lastPage);\n",
      "    \t\t\telse {\n",
      "    \t\t\t\tawait nextLink.click();\n",
      "    \t\t\t\tawait page.waitForNavigation();\n",
      "    \t\t\t\tconsole.log(page.url());\n",
      "    \t\t\t}\n",
      "    \t\t}\n",
      "    \t\twhile (!lastPage) \n",
      "    \t\t\n",
      "    \t\treturn collectionOfLinks;\n",
      "    \n",
      "    \t\tasync function linksCurrentPage () {\n",
      "    \t\t\tconst availableLinks = await page.$$eval('ul.grid > li a', \n",
      "    \t\t\t\t\tarr => arr.map(x => x.href));\n",
      "    \t\t\treturn availableLinks;\n",
      "    \t\t}\n",
      "    \t}\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c2e3pm/need_help_with_this_function_in_puppeteer_to/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Does anyone now how hes managed to automate TikTok on iPhones? (need that bot)\n",
      "Text: \n",
      "URL: https://v.redd.it/3it79x11r2uc1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Telegram Bot for NSE Stocks \n",
      "Text: Hey everyone, I have created a Telegram bot that provides various financial services such Stock Metrics, Portfolio Updates and Undervalued Stocks. \n",
      "\n",
      "The link is: https://t.me/nsebuffet_bot\n",
      "\n",
      "I have created it on account of there being too much information, this bot provides you key information that saves your time and gives sufficient information to make an analysed decision \n",
      "\n",
      "Currently the bot is available from 9:30 AM to 3:30 PM (IST), so there's a good chance it might not respond if you send a request outside that time. \n",
      "\n",
      "Any feedback would be very valuable, thanks in advance!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c25tiu/telegram_bot_for_nse_stocks/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Companies on Linkedin\n",
      "Text: Is there a way I can get all of the companies + urls from Linkedin (or any alternative source)?   \n",
      "\n",
      "\n",
      "I've explored scraping using selenium, etc, but don't want to get banned.\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c1upbm/companies_on_linkedin/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping through hidden APIs\n",
      "Text: Hi \n",
      "I want to scrape a website that has a hidden API.\n",
      "The site has a request that responds with a lot of data.\n",
      "I think that the site is calculating that data and then displays to the user. I can scrape that data but I don't know what is happening later. Any tips what I need to search?\n",
      "The data I get is ADS data with cost and roas.\n",
      "I assume the data is then calculated and displayed the sum of it.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c1mfh6/scraping_through_hidden_apis/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can I scrap this Amazon url without violating ToS?\n",
      "Text: I am trying to make an app that can scrape user's public wishlists if they want to, but I know this can be against the ToS. So I have looked at [https://www.amazon.com/robots.txt](https://www.amazon.com/robots.txt) and and don't see the wishlists with the url \"hz/wishlist/ls\" (which mine was when I tested sharing it) listed anywhere in the file. I'm guessing they just probably haven't updated the robot.txt file after updating the url. If I don't use EtaoSpider, GPTBot, or CCBot (where they disallow all), am I in the clear? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c1iznn/can_i_scrap_this_amazon_url_without_violating_tos/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selenium browser instance can't play video\n",
      "Text:  [https://hd2watch.tv/watch-series/the-valley-hd-107215/1424842](https://hd2watch.tv/watch-series/the-valley-hd-107215/1424842)  \n",
      "I keep getting error This video file cannot be played.(Error Code: 102630)    \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c1jevq/selenium_browser_instance_cant_play_video/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selling Web Scraped Data\n",
      "Text: I am looking for a good marketplace to sell data I have scraped from the web, it ranges from job sites to contacts to product info from various retailers. I have this information on a weekly basis going back years. Is there anywhere to actually sell this? I have checked out databoutique.com and they look perfect but I have no idea the actual demand for their data and I don’t want to go through the entire process just to get 0 orders. Any advice would be greatly appreciated!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c0y2us/selling_web_scraped_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any suggestions for data that's in demand for AI/ML?\n",
      "Text: I'm looking to scrape some data with the aim of gaining new insights or monetizing. Considering the fast growth of AI it seems like a no brainer to scrape data to be used for training models.\n",
      "\n",
      "Does anyone have an idea of what type of data is in demand?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c10vea/any_suggestions_for_data_thats_in_demand_for_aiml/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Site blocks based on IP using undetected_chromedriver\n",
      "Text: What are my next step if a site blocks undetected_chromedriver based on IP but not a regular Chrome session?  Even doing the captcha manually still gets blocked…\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c0vsp7/site_blocks_based_on_ip_using_undetected/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Struggling to fill in a login form\n",
      "Text: Hi all,\n",
      "\n",
      "I'm trying to automate logging in to [mybell.bell.ca](https://mybell.bell.ca) to download my bills each month.\n",
      "\n",
      "I can successfully load the page, and fill the login form with my credentials, but the credentials are not accepted. It says that the credentials are invalid. I have quadruple-checked that they are valid - I can see what is typed into the login form, and it is correct.\n",
      "\n",
      "If I manually type the credentials into the login form in the chromedriver window, the login is successful. \n",
      "\n",
      "If I copy and paste my username/password from the python script and paste them into the chromedriver window, the login is successful.\n",
      "\n",
      "However, no matter what I try, I can't get python to fill them in a way that is accepted.\n",
      "\n",
      "I have tried a straight `element.send_keys(\"my password\")` \\- the text appears in the input box but it is not accepted when logging in.\n",
      "\n",
      "I have also using an ActionChain like this, to slowly type the username/password:\n",
      "\n",
      "    def type_characters(elem, text):\n",
      "        actions = ActionChains(driver)\n",
      "        actions.move_to_element(elem)\n",
      "        actions.click()\n",
      "        actions.perform()\n",
      "        for character in text:\n",
      "            actions = ActionChains(driver)\n",
      "            actions.send_keys(character)\n",
      "            print(character)\n",
      "            actions.perform()\n",
      "            time.sleep(random.uniform(0.2,0.5))\n",
      "\n",
      "But neither seem to be accepted. I have also tried filling the inputs with Javascript:\n",
      "\n",
      ">driver.execute\\_script(\"document.getElementById('\"+id+\"').value = '\"+text+\"';\");\n",
      "\n",
      "Again, the text appears in the <input> but it is not accepted.\n",
      "\n",
      "Looking for any suggestions or things I can try. This one has got me stumped. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c0uh4o/struggling_to_fill_in_a_login_form/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape Anaconda Product Page\n",
      "Text: Need help scraping the product page of Anaconda with multiple combination of variants [https://www.anacondastores.com/fishing/fishing-line/braid-line/shimano-kairiki-8-braid-line-150-metre-spool/90140299](https://www.anacondastores.com/fishing/fishing-line/braid-line/shimano-kairiki-8-braid-line-150-metre-spool/90140299) colour and size \n",
      "\n",
      "  \n",
      " \n",
      "\n",
      "from selenium import webdriver  \n",
      "from selenium.webdriver.common.by import By  \n",
      "from selenium.webdriver.support.ui import WebDriverWait  \n",
      "from selenium.webdriver.support import expected\\_conditions as EC  \n",
      "\\# Initialize the WebDriver  \n",
      "driver = webdriver.Chrome()  # Or any other webdriver you prefer  \n",
      "\\# Navigate to the URL  \n",
      "driver.get(\"https://www.anacondastores.com/fishing/fishing-line/braid-line/shimano-kairiki-8-braid-line-150-metre-spool/BP90140299\")  \n",
      "\\# Wait for style picker to be clickable and click on it  \n",
      "style\\_picker = WebDriverWait(driver, 10).until(EC.element\\_to\\_be\\_clickable((By.CSS\\_SELECTOR, '.js-variant-style-picker')))  \n",
      "\\# style\\_picker.click()  \n",
      "\\# Wait for size picker to be clickable  \n",
      "size\\_picker = WebDriverWait(driver, 10).until(EC.element\\_to\\_be\\_clickable((By.CSS\\_SELECTOR, '.js-variant-size')))  \n",
      "\\# Gather all the size elements  \n",
      "sizes = driver.find\\_elements(By.CSS\\_SELECTOR, '.js-variant-size')  \n",
      "\\# Iterate through each size element and click on it  \n",
      "for size in sizes:  \n",
      " \\# Click on the size element  \n",
      " size.click()  \n",
      "   \n",
      " \\# Wait for the title and prices to be visible after selecting the size  \n",
      " title = WebDriverWait(driver, 10).until(EC.visibility\\_of\\_element\\_located((By.CSS\\_SELECTOR, '.pdp-title'))).text.strip()  \n",
      " price = WebDriverWait(driver, 10).until(EC.visibility\\_of\\_element\\_located((By.CSS\\_SELECTOR, '.product-info .price-was .amount'))).text.strip()  \n",
      " club\\_price = WebDriverWait(driver, 10).until(EC.visibility\\_of\\_element\\_located((By.CSS\\_SELECTOR, '.product-info .price-now .amount'))).text.strip()  \n",
      "   \n",
      " \\# Print the gathered data for each size  \n",
      " print(\"Title:\", title)  \n",
      " print(\"Price:\", price)  \n",
      " print(\"Club Price:\", club\\_price)  \n",
      " print(\"Selected Size:\", size)  \n",
      " print(\"\\\\n\")  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c0zw5i/scrape_anaconda_product_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Facebook GraphQL scraper\n",
      "Text: A Facebook crawler program doesn't need target access tokens, payments like other sites, or even personal account logins.\n",
      "\n",
      "URL: https://github.com/FaustRen/FB_graphql_scraper\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for scraping solution\n",
      "Text: Hi,\n",
      "\n",
      "So my only experience with scraping is a few months ago I was using octoparse for a work project to scrape an email list together from a website for marketing. BUT, aside from work I am building a job board website. It's a pretty niche industry and I only want to scrape job postings from like 20-30 companies and would like the data to flow into airtable or something of the sort and get updated like every 24hrs. Does anyone have any recommendations?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1c0bws7/looking_for_scraping_solution/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: TikTok API - Trending\n",
      "Text: I've seen this API endpoint used for numerous reasons online (mainly in Github repos).\n",
      "\n",
      "tiktok(dot)com/api/recommend/item\\_list/\n",
      "\n",
      "Anyone know what it actually does? \n",
      "\n",
      "The TikTok API ([https://github.com/davidteather/TikTok-Api](https://github.com/davidteather/TikTok-Api)) is using it to get \"Trending\" videos, however the way I understand it shows the data from the FYP of a user\\_id you specify. Running it without a user\\_id does not work.\n",
      "\n",
      "This endpoint is not officially documented as far as I've been able to search, so figured maybe someone on here has been scraping TikTok and will be able to help?\n",
      "\n",
      "I'm already pulling user specific videos, what I'm after are actual \"trending\" videos in the past 24h\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bzq7w4/tiktok_api_trending/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I made a blog post about how to scrape almost any website using Playwright and Jupyter\n",
      "Text: https://preview.redd.it/u82pjvza2gtc1.png?width=863&format=png&auto=webp&s=d750b864898a2fb14954ba2b4db7dc4b6f166fab\n",
      "\n",
      "In this blog I cover the tech stack I used to scrape social media and retail websites, and I provide code examples how to:\n",
      "\n",
      "- Setup Playwright browser automation library which I find easier to use than Selenium;\n",
      "\n",
      "- How to log in to website manually using Jupyter, then automate the rest of the workflow;\n",
      "\n",
      "- How to reduce chance of being blocked by using Firefox headless browser;\n",
      "\n",
      "- How to intercept network responses using Playwright, store them in a HAR file and find the data you need in the file;\n",
      "\n",
      "- How to scrape websites with infinite scroll;\n",
      "\n",
      "- How to use Prefect library to orchestrate multiple scrappers, log their output and retry on failure;\n",
      "\n",
      "- How to use boto3 library to do distributed web scrapping using AWS.\n",
      "\n",
      "Here is the blog link: [https://mangodata.io/blog-post/web-indexing](https://mangodata.io/blog-post/web-indexing)\n",
      "\n",
      "I appreciate any feedback or suggestions!\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bzqkcz/i_made_a_blog_post_about_how_to_scrape_almost_any/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Real estate scraping 40+ sites\n",
      "Text: I want to know if it is possible to write a webscraper using python that can be used to scrape any real estate website. I have a webscraper for two websites, but both sites have a different logic, while still having some (small) similarities. So far my webscraper can also only deal with \"page 1\". I have to figure out how to go to the next page and stuff. But before that, I just want to know if what I'm trying to do is possible. If not, then I guess I'll just have to write a scraper for each site.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1byuinl/real_estate_scraping_40_sites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: struggling to scrape h3\n",
      "Text:     Borrowed this code from someone i found with a simlar issue. Any help eoulkd be appriciated thanx :D\n",
      "    \n",
      "    \n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    URL = \"https://www.empireonline.com/movies/features/best-movies-2/\"\n",
      "    \n",
      "    response = requests.get(URL)\n",
      "    website_html = response.text\n",
      "    \n",
      "    # print(website_html)\n",
      "    \n",
      "    soup = BeautifulSoup(website_html, \"html.parser\"\n",
      "    \n",
      "    \n",
      "    all_movies = soup.find(name=\"h3\", class_=\"jsx-4245974604\")\n",
      "    \n",
      "    print(all_movies)import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    URL = \"https://www.empireonline.com/movies/features/best-movies-2/\"\n",
      "    \n",
      "    response = requests.get(URL)\n",
      "    website_html = response.text\n",
      "    \n",
      "    # print(website_html)\n",
      "    \n",
      "    soup = BeautifulSoup(website_html, \"html.parser\"\n",
      "    \n",
      "    \n",
      "    all_movies = soup.find(name=\"h3\", class_=\"jsx-4245974604\")\n",
      "    \n",
      "    print(all_movies)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bzac0u/struggling_to_scrape_h3/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I built my own custom static site generator and Google SERP scraper\n",
      "Text: It's a public showcase of top web developer portfolios as indexed by Google on irregular dates.ch, so I understand how the popular ones work and had a blast doing it! I recommend any code newbies do the same thing to learn and grow in their development career.\n",
      "\n",
      "[https://github.com/crock/worksauce](https://github.com/crock/worksauce)\n",
      "\n",
      "The generated website is viewable at [https://worksauce.com](https://worksauce.com) \n",
      "\n",
      "It's a public showcase of top web developer portfolios as indexed by Google at irregular dates.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bz84ns/i_built_my_own_custom_static_site_generator_and/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: System glitches when in webscraping\n",
      "Text: Hi, this question might be not directly related to scraping. But it has confused me a lot.\n",
      "\n",
      "I have been using selenium and pyautogui to scrape a -> website. The script mimics how humans would interact with the cursor, open the webpage -> click some buttons ->download the pdf files -> close pdf views.  I am doing it on a Mac and a linux. \n",
      "\n",
      "The script and the running environment are the same. But I keep noticing some inconsistent system errors, not sure what is the cause. \n",
      "\n",
      "For example:\n",
      "\n",
      "- (Mac) in one run, have date time as 05 is fine, but in another run, it would reject 05, but only takes 5.\n",
      "\n",
      "- (linux) I use subprocess \\`pkill okular\\` to close the browser. But it is only able to close it properly if I just rebooted my computer.\n",
      "\n",
      "- (Mac/Linux) Between different runs, there are different number of PDF files downloaded. But the scripts are exactly the same.\n",
      "\n",
      "- both computers get locked after a random period even if the computer has been set to no-sleep mode and the cursor has been moving around the whole time.\n",
      "\n",
      "I understand this indicates my script has lots to be improved. But I feel these problems are beyond a single script. I am really curious what cause all these inconsistency. \n",
      "\n",
      "Thanks for your help.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bytcmi/system_glitches_when_in_webscraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Receiving 404 Target redsky API\n",
      "Text: Hey r/webscraping r/WebScrapingTools r/thewebscrapingclub r/webscrapping\n",
      "\n",
      "I have recently been tasked with scraping data from Target and I am using the Target Redsky API. It seems to work for a few requests then there after I received an IP block. I have gone through the discussions in the subreddit but I can;t find a solution to ensure my bot scrapes all the product categories without been blocked. I will be glad to receive help.\n",
      "\n",
      "Here are the headers I am using for my bot. Scraping with python.\n",
      "\n",
      "    headers = {'authority': 'redsky.target.com',\n",
      "     'method': 'GET',\n",
      "     'path': '/redsky_aggregations/v1/web/plp_search_v2?key=9f36aeafbe60771e321a7cc95a78140772ab3e96&category=k4uyq&channel=WEB&count=24&default_purchasability_filter=true&include_dmc_dmr=true&include_sponsored=true&new_search=false&offset=1176&page=%2Fc%2Fk4uyq&platform=desktop&pricing_store_id=2069&store_ids=2069%2C2784%2C1872%2C3241%2C1794&useragent=Mozilla%2F5.0+%28Windows+NT+10.0%3B+Win64%3B+x64%29+AppleWebKit%2F537.36+%28KHTML%2C+like+Gecko%29+Chrome%2F120.0.0.0+Safari%2F537.36&visitor_id=018EA5ADA06C0201977511924105B48E&zip=40000',\n",
      "     'scheme': 'https',\n",
      "     'Accept': 'application/json',\n",
      "     'Accept-Encoding': 'gzip, deflate, br',\n",
      "     'Accept-Language': 'en-GB,en-US;q=0.9,en;q=0.8',\n",
      "     'Cookie': 'TealeafAkaSid=vf8zcL4xLeF3i96wNUqH4LFYZx_5cduP; visitorId=018EA5ADA06C0201977511924105B48E; sapphire=1; UserLocation=40000|18.940|72.840|MH|IN; ffsession={\"sessionHash\":\"1e2a71650a737f1712176617394\"}; fiatsCookie=DSI_2069|DSN_Durham Renaissance Pkwy|DSZ_27713; accessToken=eyJraWQiOiJlYXMyIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiIzZDhlYTA1ZS01YjQxLTRhNWYtYmNiZS0zNDg4NDEyNzU3N2MiLCJpc3MiOiJNSTYiLCJleHAiOjE3MTI2MDgwMTcsImlhdCI6MTcxMjUyMTYxNywianRpIjoiVG...',\n",
      "     'Origin': 'https://www.target.com',\n",
      "     'Referer': 'https://www.target.com/c/facial-tissue-household-essentials/all-deals/-/N-tv7qhZakkos',\n",
      "     'Sec-Ch-Ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
      "     'Sec-Ch-Ua-Mobile': '?0',\n",
      "     'Sec-Ch-Ua-Platform': '\"Windows\"',\n",
      "     'Sec-Fetch-Dest': 'empty',\n",
      "     'Sec-Fetch-Mode': 'cors',\n",
      "     'Sec-Fetch-Site': 'same-site',\n",
      "     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.3'}\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bytxez/receiving_404_target_redsky_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help! Money spent on scraping? \n",
      "Text: Anyone here training AI models or scraping data to build any AI tools? Do you build your own scraper or spend money on scrapers? How much money do you spend on any scraping tools per month? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1byhq99/need_help_money_spent_on_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: need help: bypassing the login dialog popup when scraping Instagram with selenium\n",
      "Text: I am trying to scrape profile information including: Name, Bio, and 10 Post Captions using selenium. For the first few times, the scraping works fine, but I guess the browser then recognises the web driver behaviour and then constantly shows a login popup for each and every activity on the page.\n",
      "\n",
      "I don't want to enter my login information every time I run my program and a new browser instance is created.\n",
      "\n",
      "I've heard that the session ID helps the browser recognise if you're logged in. Is this correct? If it is, is there a way I can always start my browser session with the session ID set in it so that I don't have to login every time.\n",
      "\n",
      "If there is some other approach, please do tell.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1byqzgx/need_help_bypassing_the_login_dialog_popup_when/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Getting Indeed Candidates that have applied for my job posting. NOT scrapping for jobs.\n",
      "Text: Seems like every post about scalping indeed is to get job information. However, I am interested in the other side of this. I would like to get the candidates into my database for further use. Does anyone have a tutorial or video on getting through indeed login and downloading candidates?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1byl7f8/getting_indeed_candidates_that_have_applied_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help parsing a dyanic table in python\n",
      "Text: I'm trying to parse the lower table in this [Power BI Dash](https://app.powerbi.com/view?r=eyJrIjoiNzA0MGM4NGMtN2E5Ny00NDU3LWJiNzMtOWFlMGIyMDczZjg2IiwidCI6IjM4MmZiOGIwLTRkYzMtNDEwNy04MGJkLTM1OTViMjQzMmZhZSIsImMiOjZ9&pageName=ReportSection). The code to provided by this [github user](https://github.com/ajeet214/Web_Scraping_with_Selenium/blob/main/app_powerbi_com.py). I removed the state and job code as I'm not trying to click on anything in the first table, only scrape the 2nd. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I can't seem to parse the entire table even after scrolling. I imagine the wait time is the issue, but i adjust the time.sleep function and I still can't parse the data.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "My code for parsing is below. It only collects the first 41 rows or so.  \n",
      "\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "from bs4 import BeautifulSoup  \n",
      "\\# Transition to BeautifulSoup  \n",
      "soup = BeautifulSoup(driver.page\\_source, \"html.parser\")  \n",
      "rows = soup.find\\_all('div', attrs={'role': 'row'})  \n",
      "rows\\_data = \\[\\]  # Initialize rows\\_data to collect data from all rows  \n",
      "for row in rows\\[1:\\]:  \n",
      "cells = row.find\\_all('div', attrs={'role': 'gridcell'})  \n",
      "cell\\_texts = \\[cell.get\\_text() for cell in cells\\[1:\\]\\]  # Skip the first cell  \n",
      "rows\\_data.append(cell\\_texts)  \n",
      " print(len(rows\\_data))  \n",
      "headers = table.find\\_elements(By.CSS\\_SELECTOR, \"div\\[role='columnheader'\\]\")  \n",
      "h=\\[\\]  \n",
      "h.pop\\[0\\]  \n",
      "for header in headers:  \n",
      "h.append(header.text.strip())  \n",
      "\n",
      "\n",
      "\\# Capture headers again after scrolling  \n",
      "column\\_headers = table.find\\_elements(By.CSS\\_SELECTOR, 'div\\[role=\"columnheader\"\\]')  \n",
      "\\# Initialize a list to hold the text of each column header  \n",
      "header\\_texts = \\[\\]  \n",
      "for header in column\\_headers:  \n",
      " \\# Extract and append the text from each header  \n",
      "header\\_texts.append(header.text)  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1byha0i/help_parsing_a_dyanic_table_in_python/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Data extraction project\n",
      "Text: Hello guys!\n",
      "\n",
      "I've got a challenge at my university to build a model that extracts products from. 500+ furniture stores.\n",
      "\n",
      "Here are the guidelines:\"## Guidelines  \n",
      "A good approach that works well with such extraction problems is to create a NER (Named Entity Recognition) model and train it to find your entities (you have one entity, ‘PRODUCT’).  \n",
      "\\- In order to create such a model you need training data, you can also extract that from the input pages.  \n",
      "\\- Crawl \\~100 pages from the list above & extract the text from it.  \n",
      "\\- Find a way to tag some sample products from these texts.  \n",
      "\\- Train a new model from the examples you just made.  \n",
      "\\- Use it to extract product names from some new, unseen pages.  \n",
      "Please use any programming language, toolset or libraries you're comfortable with or find necessary, especially if you know it will be better or more interesting.  \n",
      "We recommend using the Transformer architecture from the \\[\\*\\*sparknlp\\*\\*\\]874df20d1d77) library or the huggingface \\[\\*\\*transformers\\*\\*\\](  library.\"\n",
      "\n",
      "I have little experience with web crawling. I\\`m not that scared about the NER part, rather then crawling the \\~100 pages in order to train my model. I have tried to scrape with beautifulsoup but I don't know how to make it so I don't have to input manually the site selectors in order to get the correct product name.\n",
      "\n",
      "If you have any suggestions, please let me know <3\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1byfz0x/data_extraction_project/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: New to Web Scrapping... Need Help.\n",
      "Text: Hello everyone.\n",
      "\n",
      "My cousin has a very extensive Manga collection, but is not good with computers.  \n",
      "He asked me if it was possible to scrape the following website manga series:\n",
      "\n",
      "[https://comicvine.gamespot.com/big-comic-spirits-1-no-11-1980/4000-579481/](https://comicvine.gamespot.com/big-comic-spirits-1-no-11-1980/4000-579481/)\n",
      "\n",
      "Basically, I need the Issue Details, the Creators and a way to auto select the next book and repeat the process. Ideally, I would want each book information on 1 row with the columns on top for each piece of info.\n",
      "\n",
      "I tried self teaching myself Python and adding the recommended extensions, but just keep getting met with with constant errors and frustration.\n",
      "\n",
      "There will be other series from this website that I would be wanting to repeat this same process for as well that he has.\n",
      "\n",
      "Any help would be greatly appreciated as I am in way over my head here lol.\n",
      "\n",
      "Below is an example of the first 2 that I manually put together for reference of what I am trying to accomplish.\n",
      "\n",
      "https://preview.redd.it/8xyhqxpri4tc1.png?width=1279&format=png&auto=webp&s=e1752e246ced674d1551ea1d12d3780b268fb821\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1byfnb1/new_to_web_scrapping_need_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping a game wiki via 'fetch' in typescript, but I keep getting a 403 error\n",
      "Text: I would like to scrape this page:  [View source for Quest:The Final Countdown - Wizard101 Wiki (wizard101central.com)](https://wiki.wizard101central.com/wiki/index.php?title=Quest:The_Final_Countdown&action=edit)\n",
      "\n",
      "I need to get the entire html.I copied the network call as a curl in the Edge Network tools, and ran it through Postman. Everything looked good.\n",
      "\n",
      "I then copied the network call as a fetch and inputted it in my application, but I keep getting a 403 response.\n",
      "\n",
      "Does anyone have any suggestions?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Edit: Here is how my fetch request is looking like:  \n",
      "\n",
      "\n",
      "    fetch(\"https://wiki.wizard101central.com/wiki/index.php?title=Quest:The_Final_Countdown&action=edit\", {\n",
      "      \"headers\": {\n",
      "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
      "        \"accept-language\": \"en-US,en;q=0.9\",\n",
      "        \"cache-control\": \"max-age=0\",\n",
      "        \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"123\\\", \\\"Not:A-Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"123\\\"\",\n",
      "        \"sec-ch-ua-mobile\": \"?0\",\n",
      "        \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
      "        \"sec-fetch-dest\": \"document\",\n",
      "        \"sec-fetch-mode\": \"navigate\",\n",
      "        \"sec-fetch-site\": \"none\",\n",
      "        \"sec-fetch-user\": \"?1\",\n",
      "        \"upgrade-insecure-requests\": \"1\",\n",
      "        \"cookie\": \"_ga=GA1.1.1590498887.1712499356; __utma=32368312.1590498887.1712499356.1712499356.1712499356.1; __utmc=32368312; __utmz=32368312.1712499356.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); ncmp.domain=wizard101central.com; cf_clearance=g6IhtZUERUJ1VCliedjOiZvxG5SiEjQ.nbDSCg8JDoE-1712499355-1.0.1.1-64d3ucX383Is.4o2GCSqQZyv3LJEjMaFf.KO40RdtgiqhA_I39eAS2.c0lrqten9IWThH.1gd8_C.UCacFZUAQ; __gads=ID=3b582b6a3b636695:T=1712499356:RT=1712499356:S=ALNI_MbAr4JN2_BaxSBypYJgEKps9eUzsg; __gpi=UID=00000a19d4955c95:T=1712499356:RT=1712499356:S=ALNI_MZDMh_SnZw_s2-GZCRNj6G5f41p4g; __eoi=ID=55573e6c049e88cc:T=1712499356:RT=1712499356:S=AA-AfjbXNvPtKopp9mRX5Nyq7V8h; na-unifiedid=%7B%22TDID%22%3A%22c586c458-1fa1-49b9-a5a5-21ffec35552f%22%2C%22TDID_LOOKUP%22%3A%22TRUE%22%2C%22TDID_CREATED_AT%22%3A%222024-03-07T14%3A15%3A59%22%7D; na-unifiedid_cst=TyylLI8srA%3D%3D; __utmb=32368312.3.9.1712499407936; _ga_TVB2JDQN4F=GS1.1.1712499356.1.1.1712499408.8.0.0; cto_bundle=gZHyrl96bDJwN2ljdyUyQkd4RDRCWmthJTJGWEhMcDRiT3F1JTJCWWRMOGhsM3dQc254M2ZTeHZOWHlmSkVWYml6Z2tuT0paWEpFWHp4M2VsU2dURFBEODZjQkhRNHZPeURRMDBiU3NsUlFSdHBVam1FYXhkUUZGS2hkN2hCUE9GZjhrWjg3bm1UdlAxZkdWOEx3NUppTEw1WXBJVHklMkZVcGxFemxNQ3AwRFJ4MnhBMkRwTm1pWSUzRA\"\n",
      "      },\n",
      "      \"referrerPolicy\": \"strict-origin-when-cross-origin\",\n",
      "      \"body\": null,\n",
      "      \"method\": \"GET\"\n",
      "    });\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1by4049/scraping_a_game_wiki_via_fetch_in_typescript_but/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to get past cookie dialogue on this web site with Selenium?\n",
      "Text: Hello. I am trying to scrape [www.verbformen.de](https://www.verbformen.de) but my Python script using Selenium can't get past the cookie dialogue. I am unable to locate the accept button, no matter what I tried. The cookie dialogue shows up a little late after website loads, I tried explicit and implicit wait strategies of selenium, I even tired sleeping the python script for a duration but no matter what, I couldn't click the \"accept\" button. Can anybody here help me?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1by6fgx/how_to_get_past_cookie_dialogue_on_this_web_site/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Instagram profile scraping\n",
      "Text: I'm working on a project for a client that requires me to iterate through all of their IG followers (1.2 million) and extract email, phone where possible. I've seen a couple of different api's, one the brings public email and the other business email, phone, etc. I've been testing tools for the past couple of weeks and I believe I have the basic structure - library that can handle the request, proxies, and the last item would be accounts. In my research I'm deducing that to properly handle these requests I need to be logged in there either purchase some IG accounts or create them (I'd go the purchase route). What I'm trying to get a sense of is that logic in utlizing a set of accounts, timing (randomness), and high level understanding of how many accounts I'd need to procure if I'm looking to parse 1.2 milliong profiles. I'm a developer so I don't mind doing the work if someone can point me in the right direction and give me some insight into the account handling and request timing. TY. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bxmyzl/instagram_profile_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping local real estate websites\n",
      "Text: In my country there is a lot of different real estate websites (somewhere around 18). And this idea was bothering me for a while couldn’t find some relevant information or running projects.\n",
      "\n",
      "What I am trying to create one big real estate website by scraping those little ones. And not sure if I could get sued for that or not. \n",
      "\n",
      "Appreciate your insights! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bxlrvz/scraping_local_real_estate_websites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping soccer matches\n",
      "Text: I have a subscription to a service where i can watch soccer matches, and rewatch past ones. I want to download all the matches from one particular season for a project but I don't know where to begin. The app also blocks screen recordings, so I can't manually record each one (although I hope I could find a solution that doesn't involve going through tens of 90 minute matches manually). Any help is appreciated!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bxp9s1/scraping_soccer_matches/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Azure DI and GPT\n",
      "Text: Has anyone tried scraping websites with Azure Document Intelligence and GPT? Document Intelligence seems to do a good job converting screenshots into markdown and GPT can semantically parse the content. Thoughts?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bxr6p1/azure_di_and_gpt/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I'm learning Web Scraping and I'm seeking insight and helps into otto.de website\n",
      "Text: Hey there,\n",
      "\n",
      "I'm diving into the world of web scraping and currently exploring how websites function, with a focus on otto.de. I'm trying to extract data from their platform as part of my learning process.\n",
      "\n",
      "Take, for instance, this product link: [Loco Bird Schneidebrett 3er Set](https://www.otto.de/p/loco-bird-schneidebrett-3er-set-1x-33x22cm-1x-28x22cm-1x-15x22cm-bambus-holzschneidebretter-3-st-holz-brett-fuer-die-kueche-antibakterielles-schneidebrett-aus-holz-S0B2Q0F0/#variationId=S0B2Q0F0RJRS). My goal is to retrieve seller data, which appears when clicking \"Verkäufer\" (Seller).\n",
      "\n",
      "However, I'm puzzled about how this data is generated. Despite thorough searching, I haven't been able to locate its source. Do I need to simulate a click each time to fetch this data? Clicking reveals a link like this: [https://www.otto.de/partner-details/partner/1083252?asLayer=true](https://www.otto.de/partner-details/partner/1083252?asLayer=true), but I'm unsure where these links originate from.\n",
      "\n",
      "If anyone could shed light on this, it would be greatly appreciated!\n",
      "\n",
      "Looking forward to your insights. Thanks in advance!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bx7r1b/im_learning_web_scraping_and_im_seeking_insight/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscrapping project\n",
      "Text: Hello everyone, for my final semester at university I must do complex project starting with obtain data using scraping techniques and with that I should use ML, DL, RL and other things. \n",
      "\n",
      "I come here with my head just to ask for projects ideas that have complexity on the scraping part of the websites.\n",
      "\n",
      "Thank you!!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bwwca4/webscrapping_project/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Error (0x490)\n",
      "Text: Error was occurring on chrome so i switched to opera with no avail. I already checked all my drives. I disconnect my usbs and reconnected them. I deleted all my vpns.. Idk what to do.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bx47iv/error_0x490/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Unsure about webscraping legality and prosecution\n",
      "Text: Hey,\n",
      "\n",
      "I'm new to web scraping and have now prepared my first major project.\n",
      "\n",
      " I want to continuously download all the data from an online forum (i.e. one day at a time) and collect it for scientific analysis. However, I am still concerned about the legality of web scraping. Perhaps you can help me with your experience:\n",
      "\n",
      "Q1: The T&Cs of the forum do not explicitly prohibit scraping, however it is also not clearly stated that it is allowed. It is also important that I want to use a user account to be able to scrape the GraphQL endpoint of the forum - I could also scrape the same information without a user account (from the HTML), but I would need significantly more requests. Do you think it would be legal to scrape the GraphQL interface under these conditions?\n",
      "\n",
      "Q2: **What is the likelihood of being prosecuted for web scraping?** (based in Germany, if this is important) How often have you seen this happen in general? Are the IPs traced in the event of scraping or are they simply blocked?\n",
      "\n",
      "Q3: For my project, it makes sense to have many clients working via proxies. In this case, would you choose a proxy provider with anonymous payment or can you rely on privacy?\n",
      "\n",
      "Sorry again for the long text and thanks in advance for all the answers!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bwzws5/unsure_about_webscraping_legality_and_prosecution/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Get linked-in post text from url\n",
      "Text: Hello, i'm new to this group 😺\n",
      "\n",
      "  \n",
      "I'm working on a SAAS website, and we need to get the text from whatever post coming from linked-in, i've searched how to do it, and it seems that it's just too complicated to do this using linked-in api services and they are very limited probably for security reasons.\n",
      "\n",
      "  \n",
      "What i'm currently doing is, user inputs the <iframe> provided by linked-in (for example \"**<iframe src=\"https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:7181727451201302529\" height=\"972\" width=\"504\" frameborder=\"0\" allowfullscreen=\"\" title=\"Publicación integrada\"></iframe>**\"), and then on the server, i get the \"src\" value and make a request and then i get the text.\n",
      "\n",
      "Now this is kind of uncomfortable for users, so the next idea i have is user would input the **actual** post url (for example \"https://www.linkedin.com/feed/update/urn:li:activity:7181999020259643392/\"), and then on the server i'll modify the string and add the \"**/embed**\" route to again access its text.\n",
      "\n",
      "  \n",
      "I'm doing this because it's simple and i don't want to pay crazy money for other apis that'd do this for me. My question would be, does this count as \"web-scrapping\" ? is this legal ? would i have problems legally if i use this approach to get whatever \"text\" post from linked-in ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bwosvy/get_linkedin_post_text_from_url/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: PerimeterX update frequency? \n",
      "Text: Hey there\n",
      "\n",
      "Scaraping an ecomm site which is protected by perimeterx. \n",
      "\n",
      "Currently I’m not getting blocked (using rotating proxy).\n",
      "\n",
      "Curious if folks have experiences where perimeterX updates are released frequently? Wondering if I need to expect monthly trouble shooting or if I’m safe for a while\n",
      "\n",
      "Thanks \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bwnnky/perimeterx_update_frequency/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do I web scrape website info with multiple pages quickly?   \n",
      "Text: How do I web scrape website info with multiple pages quickly?\n",
      "\n",
      "I want the data of top 100 songs for multiple months. I have found some chrome extension but i have to insert new selectors for every new page.\n",
      "\n",
      "Specifically ( song title/artist name/ streaming score/ distribution company)\n",
      "\n",
      "I need to use the data for my uni research to run a regression.\n",
      "Any advice? I do not know how to write code.\n",
      "URL: https://circlechart.kr/page_chart/onoff.circle?serviceGbn=S1040&termGbn=month&hitYear=2011&targetTime=03&nationGbn=K&year_time=\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: (How) do you test your code?\n",
      "Text: Been trying 2 small scraping projects by now in Python. Kinda wanted to know if the code actually worked so I test it out after every 'major' part of the task I have to do\n",
      "\n",
      "For example I'll have a scraper that gets likes and views from a site's posts and there's first the step logging in. So I'll test out going to the login page once I made it, test out inputting my username/pass, test out going to the right page etc. And sometimes when the code fails I'll have to test again.\n",
      "\n",
      "I was wondering if others just code it and don't test as much. Since you know it could be seen as heavy scraping if you have to test like 10 times in a coding session, being possibly blocked from the site. Or don't you think it makes a difference if you test it once or 10 times?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bwdvgo/how_do_you_test_your_code/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Tinder.com scraping\n",
      "Text: I would like to automate scraping of [Tinder.com](https://Tinder.com). I can manually copy the json data from the [https://api.gotinder.com/v2/recs/core?locale=en](https://api.gotinder.com/v2/recs/core?locale=en) request/response then run it thru my custom php script to extract the data i need such as name, birth\\_date, bio and profile picture. So basically I would have to send that request automatically after I say like/nope to several profiles so new profiles generate. Any thoughts?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bw3eaa/tindercom_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Robots txt file implications\n",
      "Text: what's the legal limit stemming from a site's robots txt file? \n",
      "\n",
      "this is my situation: robots file says it's cool everything's allowed, but user's terms and conditions say no to web scraping\n",
      "\n",
      "does anyone have an authoritative and or informed take on this? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bw373h/robots_txt_file_implications/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: BeautifulSoup retrieving HTML from wrong date?\n",
      "Text: Edit: Realized the issue is with Requests, not BeautifulSoup\n",
      "\n",
      "I'm using Requests to scrape data from Yahoo Finance and noticed the key values in my .get() output don't match the ones I can see live on Yahoo Finance. I noticed the following code added to the end of the .get() output after the closing </html> tag:\n",
      "\n",
      "<!-- FS Wed 13 Mar 05:03:43 UTC 2024 device:desktop \\\\\\\\\\\\\\[finance.yahoo.com/desktop/quote/MSFT, finance.yahoo.com/desktop/quote/MSFT/index.html\\\\\\\\\\\\\\] via:https/1.1 media-router-ui7002.canary.media.gq1.yahoo.com (ApacheTrafficServer \\\\\\\\\\\\\\[cMsSf \\\\\\\\\\\\\\]) yrid:fshqXvnS FS --> <!-- YahooFailsafe -->\n",
      "\n",
      "This is leading me to believe the discrepancy is due to Requests pulling data from March 13th rather than today (April 4th). Is this an anti-scraping measure from Yahoo Finance or something else?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bw4qm2/beautifulsoup_retrieving_html_from_wrong_date/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I be notified when a self-updating website is updated??\n",
      "Text: Disclaimer: complete newbie here.  But I think my goal is rather modest, to the point that it barely qualifies as webscraping?!\n",
      "\n",
      "Let's say there's a self-updating website such as C\\*\\*\\*\\*sList, which one wants to keep an eye on.  Just want to automate reloading the website periodically and be notified when there's a new post at the top of the list.  For example, to make an audible tone when there's a new post advertising a desired widget, say.\n",
      "\n",
      "Is there a straightforward way to accomplish this??\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bvxuis/how_can_i_be_notified_when_a_selfupdating_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How much should I charge a client?\n",
      "Text: Dear Webscrappers,\n",
      "\n",
      "A client reached out to me on Upwork asking if I could scrape the data from SEC website (because I had experience with scrapping data from SEC in the past).\n",
      "\n",
      "He needs an excel document that supposed to contain files of particular type distinguished by names of people related to the file. For example one file may contain three names on it, so this file has to be parsed three times (one name per entry).\n",
      "\n",
      "Document must contain 34 columns, means I have to modify  the code the way it would scan every file and fill the information according to every column.\n",
      "\n",
      "I am a beginner at web scrapping so I have no idea how many hours it would take for me. Client insists that it'd be a fixed price, so I said $800. Now I have a feeling I scared him.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "How much would you charge for this work?\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bw1118/how_much_should_i_charge_a_client/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web scraping architecture: How to design a proper queue?\n",
      "Text: Hey,\n",
      "\n",
      "I'm working on a scraper to regularly get product prices from a variety of web shops. I already have the working scraper that takes information from a redis queue. The scraper is currently able to process 16 links at the same time (using 4 browser sessions across 4 servers).\n",
      "\n",
      "The problem I am facing now is to implement a logic on how I could fill this queue based on my specifications:\n",
      "\n",
      "* A link has a priority. Depending on the priority, the link should be scraped more often/regularly. Ideally, at some point, I would like to specifiy something like \"30s\" (every 30 seconds) or \"45m\" (every 45 minutes) etc.\n",
      "\n",
      "My backend logic (for handling proxy rotations, scrape information, etc.) is written in PHP (Laravel). My idea was to run a job every 10 seconds to re-fill the queue. But at this point I am very unsure how to fill it so that some links get in there more often than others.\n",
      "\n",
      "My initial thought was to give each link a priority from 1-10 and put exponentially more links in the queue based on that (2 twice as much as 1, 3 twice as much as 2, ...). But that is probably not the best approach.\n",
      "\n",
      "I'd be very thankful for some guidance on this. I imagine many of you already went through something like this.\n",
      "\n",
      "Cheers\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bvhf9z/web_scraping_architecture_how_to_design_a_proper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Tmall scraping\n",
      "Text: Hey, does anyone know how to bypass tmall login wall? Or use mtop api without auth.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bvnfwn/tmall_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is it possible to webscrape this? Is there another way to go about this?\n",
      "Text: [https://authorities.loc.gov/cgi-bin/Pwebrecon.cgi?DB=local&PAGE=First](https://authorities.loc.gov/cgi-bin/Pwebrecon.cgi?DB=local&PAGE=First)\n",
      "\n",
      "I want to get back authorized headings only. \n",
      "\n",
      "I was thinking since the results are displayed in a format like csv/sql query it wouldn't be too hard to filter them out with only authorized headings in the first column. The problem is getting all the data. \n",
      "\n",
      "Is webscraping the way to go? Is it legal? \n",
      "\n",
      "How would I webscrape this? Cause it looks like I'd have to enter in terms manually, maybe for each letter, and then go through all the results. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bv9ecn/is_it_possible_to_webscrape_this_is_there_another/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Where to even begin scraping discord or telegram?\n",
      "Text: I believe that these two apps represent a big portion of significant internet data that isn’t indexed by search engines. I want to learn more about them and what servers/ groups are popular. Where do I even start? Is there an equivalent of top 1000 domains for discord or telegram?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bunh8i/where_to_even_begin_scraping_discord_or_telegram/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there a package for google scholar scraping?\n",
      "Text: Hey,\n",
      "\n",
      "I was wondering if there is a package for google scholar scraping?\n",
      "\n",
      "i know there is scholar, but only takes information from researchers and returns their results.\n",
      "\n",
      "I need a scraper that takes any search term and returns a dataframe with results, citations and a link.\n",
      "\n",
      "I couldn´t find such a package, which is why I created my own function. The function is slow and needs some improvements but at least it runs through all pages without http 429 errors or any other errors causing it to stop.\n",
      "\n",
      "Is there a better faster alternative?\n",
      "\n",
      "Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bv3ww7/is_there_a_package_for_google_scholar_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping recently offline websites?\n",
      "Text: Hey guys. Hope all is well. Do you know of anyway to scrape websites that recently went offline? Like an example the website went offline in the past month, I cold call them to sell them a new website\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bv1wdy/scraping_recently_offline_websites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I'm having problems scraping URLs from Facebook.\n",
      "Text: To preface, I have written many working scripts to scrape Facebook, so I'm aware of the challenges...\n",
      "\n",
      "If you have had experience with scraping React front end websites, and have any idea what's going on here, please do leave a comment!\n",
      "\n",
      "**The aim**\n",
      "\n",
      "I'm having issues scraping a particular set of data from this particular endpoint:\n",
      "\n",
      "'[https://www.facebook.com/search/posts/?q=](https://www.facebook.com/search/posts/?q=)\\[any search term\\]'\n",
      "\n",
      "To simplify the problem, I can say that I am looking to collect the URLs of the posts that are listed on the page, not exhaustively, just whatever posts are visible on the page at the time the script is executed.\n",
      "\n",
      "The URLs should look something like:\n",
      "\n",
      "`/facebook\\\\.com\\\\/(?<userName>\\[a-zA-Z0-9\\_\\_\\\\-\\\\.\\]\\*)\\\\/posts\\\\/(?<postId>\\[a-zA-Z0-9\\]\\*)/`\n",
      "\n",
      "**The Problem**\n",
      "\n",
      "After the page first loads, before I scroll down or anything like that, I can see several posts.\n",
      "\n",
      "If I run this code in the console: (This snippet just lists all the \"anchor.href's\" that match the regular expression above and are not the \"/search/\" URL)\n",
      "\n",
      "    Array.from(document.querySelectorAll(\"a\"))\n",
      "    .map(i => i.href)\n",
      "    .filter(href => /facebook\\.com\\/(?<userName>[a-zA-Z0-9__\\-\\.]*)\\/posts\\/(?<postId>[a-zA-Z0-9]*)/.test(href) && !href.includes(\"facebook.com/search/\"))\n",
      "\n",
      "Then no URLs will be returned. Then, if you hover over a datestamp of any post and rerun the JavaScript snippet, then that post URL will exist...\n",
      "\n",
      "So before I can scrape all the URLs, I first need to simulate hovering over all the post datestamp's, which is impossible because they don't appear in the DOM elements returned by my query:\n",
      "\n",
      "    const feedElement = document.querySelector(`div[role='feed']`);\n",
      "    const posts = feedElement.children;\n",
      "\n",
      "The reason the datestamp is not there is because it is ??nested?? in the anchor tag that doesn't appear until you interact with the datestamp?!\n",
      "\n",
      "**Question**\n",
      "\n",
      "What the hell is going on here Facebook?!\n",
      "\n",
      "How can I simulate the action of hovering over all the datestamps of all the posts, when they don't appear until you hover over them....\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1buxkka/im_having_problems_scraping_urls_from_facebook/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help in Finding the \"Turnit\" Ride API Documentation\n",
      "Text: I'm trying to find the API documentation provided by [turnit](https://turnit.com/)(Turnit Ride) coz I want to integrate & build using their api. I have tried contacting , but they're not responsive nor helpful.\n",
      "\n",
      "I have found a website(https://webshop-dax.michiganflyer.turnit.com/) that uses the turnit api to enable users book tickets.\n",
      "\n",
      "While booking a ticket on the site, I was inspecting the network to locate hidden APIs and yes, the site uses the turnit api(https://webshop-dax.michiganflyer.turnit.com/api/v2021/en-us/journeys/search).\n",
      "\n",
      "So my goal is to find the documentation of the Turnit API and I have failed to find it. Need help on finding it. Please help as we have been stagnated because of not having this API documentation. Your help is highly appreciated.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bur619/help_in_finding_the_turnit_ride_api_documentation/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I'm not sure how to check if the items listed have changed from the next day?\n",
      "Text: [https://www.saksoff5th.com/c/men/thom-browne](https://www.saksoff5th.com/c/men/thom-browne)\n",
      "\n",
      "I'm using beautifulsoup but I don't know how to get a handle on each item. Each item seems to have data-pid but I don't know how to get a handle on this div class? The div class does not seem to have div id.\n",
      "\n",
      "<div class=\"product bfx-disable-product standard\" data-pid=\"0400020833674\" data-tile-pid=\"0400020833674\">\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1buuz44/im_not_sure_how_to_check_if_the_items_listed_have/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: ASP.NET scraping - is Crawlee viable?\n",
      "Text: Is Crawlee usable on ASP.NET (ViewState) sites?\n",
      "\n",
      "If not, is there something recommended other than Scrapy?  \n",
      "JavaScript is more appealing than Python.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bujq90/aspnet_scraping_is_crawlee_viable/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help on bypassing a captcha firewall\n",
      "Text: I'm trying to scrape websites that have stock market data on it ([https://www.wsj.com/market-data/quotes/US/AAPL](https://www.wsj.com/market-data/quotes/US/AAPL)) Theoretically, my code should work, but I encountered a problem when trying to run the automation as WSJ employs a captcha to catch bots. Anyone can suggest any resources/videos to help me out? Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bu4f48/help_on_bypassing_a_captcha_firewall/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape location information from Instagram / other social media sites\n",
      "Text: I am looking to start an Instagram webscraping project that would require post information (actual post itself, comments if possible, number of likes if possible, etc.) within a specific geographic location (city /county limits). Ideally, I would like to be able to map the concentration of these posts. Is this possible? I have previous experience with webscraping non-social media sites and heat map creation.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bu79s4/how_to_scrape_location_information_from_instagram/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Modern and minimalistic Scrapy alternative?\n",
      "Text: I am in need of a “framework” right now. I have tried Scrapy after avoiding it for a long time and I really liked the general structure of it. Its concurrent pool system to send requests at the same time is really performant, and it is superior to just using \\`asyncio.gather\\` on a fixed amount of requests. Items and middlewares as well as yielding next \\`Request\\` objects from parsers make a ton of sense, but it just feels old. It has similar ways to get job done such as having both yield+inline\\_requests and async support for fetching data and when I have issues most of the results seem to be from 4 to 9 years ago. I would like to have a minimal async framework that doesn't have legacy elements.\n",
      "\n",
      "Please keep in mind that Selenium, Playwright, or plain BeautifulSoup are not replacement to Scrapy as they are not frameworks. They can be integrated to Scrapy or used on their own without Scrapy, but they don't manage concurrent requests, retries, logger or structural items and middlewares on their own.\n",
      "\n",
      "I am also open to suggestions based on other programming languages/runtimes aside from Python such as Go, Rust or Node.js as long as they meet the criteria.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bucezu/modern_and_minimalistic_scrapy_alternative/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What web scraping task would you like to see AI automate/do?\n",
      "Text: Ok so here's a quick tl;dr.\n",
      "\n",
      "My friend and I built this really cool tool (or at least I think so). It's basically a free Large Language Model (LAM) designed to take actions on your behalf with natural language prompts and theoretically automate anything. For example, it can schedule appointments, send emails, check the weather, and even connect to IoT devices to let you command it – you can ask it to publish a website or call an Uber for you. You can integrate your own custom actions, written in Python, to suit your specific needs, and layer multiple actions to perform more complex tasks. When you create these actions or functions, it contributes to the overall capabilities of Nelima, and everyone can now invoke the same action. Right now, it's a quite limited in terms of the # of actions it can do but we're having fun building bit by bit.\n",
      "\n",
      "I'm tryin go to integrate more webscraping related functions but I'm not sure what would resonate with the web scraping community. For example, I created an action that retrieves html content and summarizes a website's page. \n",
      "\n",
      "Since anyone can come and integrate actions, I'm wondering whether you guys would have any good suggestions of what you would like to see the LAM do or whether you would like to contribute in creating functions so that it can become better overall for webscraping related tasks.\n",
      "\n",
      "For now, it uses Python 3 (Version 3.11), and the environment includes the following packages: BeautifulSoup, urllib3, requests, pyyaml.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bu5vyb/what_web_scraping_task_would_you_like_to_see_ai/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I need help with Web Scraping an interactive page\n",
      "Text: Hello Folks.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I'm trying to gather information from this website ([https://cead.spd.gov.cl/estadisticas-delictuales/](https://cead.spd.gov.cl/estadisticas-delictuales/)) in order to create a database for a personal project. I've done web scraping before, but never with such an interactive page where tables are generated upon interaction with filters.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Unfortunately, the page is not very user-friendly in terms of information retrieval. If I want to know the number of crimes for a specific region, along with the gender and age of the victim, and the geographical location, I would have to download 3 different Excel files and then merge them, repeating this process for all regions and all types of crimes. It's CRAZY.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Any help and advice would be greatly appreciated.\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1btg5xa/i_need_help_with_web_scraping_an_interactive_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Monthly Self-Promotion Thread - April 2024\n",
      "Text: Hello and howdy, digital miners of /r/webscraping!\n",
      "\n",
      "The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n",
      "\n",
      "* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n",
      "* Maybe you've got a ground-breaking product in need of some intrepid testers?\n",
      "* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n",
      "* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n",
      "\n",
      "Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n",
      "\n",
      "Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bt0rcy/monthly_selfpromotion_thread_april_2024/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to add text to Google lens search?\n",
      "Text: The Google lens api accepts only image as input. However, in the Google app (I've tested it in iOS app) it's possible to search with both image and text in the query:\n",
      "\n",
      "https://preview.redd.it/7mgw2mry0wrc1.jpg?width=960&format=pjpg&auto=webp&s=406b2228d5f1cdc226283c6d63e79766e532af74\n",
      "\n",
      "Is there a way to create such a query, that accepts both image and text, with API?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bt6ylx/how_to_add_text_to_google_lens_search/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: HELP: How to check if URLs are working just fine after cleaning them up?\n",
      "Text: I have thousands of websites in our database, and some of them contain query parameters that we would like to remove if they would still work fine. So, with a \\`[https://www.google.com/search?q=hello](https://www.google.com/search?q=hello)\\` we would like turn that into \\`[https://www.google.com/search](https://www.google.com/search?q=hello)\\`. But it's possible that some of the parameters are necessary. How do I do the checking?\n",
      "\n",
      "My initial plan is to use \\`requests\\` library and if the response.header returns a status code of 200s or 300s. But some of the websites are protected by Cloudflare, and so they would return 403 or 404.\n",
      "\n",
      "Any suggestions? Thanks a lot in advance.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bt6ill/help_how_to_check_if_urls_are_working_just_fine/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to update options of Selenium Firefox webdriver while it is running?\n",
      "Text: I want to change some options of the webdriver while it is created. Is it possible to do it?\n",
      "\n",
      "    options = Options()\n",
      "    options.set_preference(...)\n",
      "    driver = webdriver.Firefox(options=options)\n",
      "    # ...\n",
      "    # change options and pass it to web driver here.\n",
      "    # ...\n",
      "    driver.quit()\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bteevv/how_to_update_options_of_selenium_firefox/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Why rating is not getting scraped, the site is \"http://web.archive.org/web/20211118192644/https://www.imdb.com/search/title/?groups=top_250&sort=user_rating%2Cdesc&start=51\", code is below\n",
      "Text:  \n",
      "\n",
      "import scrapy  \n",
      "from scrapy.linkextractors import LinkExtractor  \n",
      "from scrapy.spiders import CrawlSpider, Rule  \n",
      "\n",
      "\n",
      "class BestMoviesSpider(CrawlSpider):  \n",
      " name = \"best\\_movies\"  \n",
      " allowed\\_domains = \\[\"web.archive.org\"\\]  \n",
      " start\\_urls = \\[\"http://web.archive.org/web/20220608180458/https://www.imdb.com/search/title/?groups=top\\_250&sort=user\\_rating,desc\"\\]  \n",
      " rules = (  \n",
      " Rule(LinkExtractor(restrict\\_xpaths=\"//h3\\[@class='lister-item-header'\\]/a\"), callback=\"parse\\_item\", follow=True),  \n",
      " Rule(LinkExtractor(restrict\\_xpaths=\"(//a\\[@class='lister-page-next next-page'\\])\\[2\\]\"))  \n",
      ")  \n",
      " def parse\\_item(self, response):  \n",
      " yield {  \n",
      " 'title': response.xpath(\"//div\\[@class='sc-94726ce4-2 khmuXj'\\]/h1/text()\").get(),  \n",
      " 'year': response.xpath(\"//span\\[@class='sc-8c396aa2-2 itZqyK'\\]/text()\").get(),  \n",
      " 'rating': response.xpath(\"//div\\[@class='inline-block ratings-imdb-rating'\\]/strong/text()\").get(),  \n",
      " 'duration': response.xpath(\"//li\\[@class='ipc-inline-list\\_\\_item'\\]/text()\").get(),  \n",
      " \\#'genre': response.xpath(\"//h1\\[@class='sc-b73cd867-0 fbOhB'\\]/text()\").get(),  \n",
      " 'movie\\_url': response.url,  \n",
      "}\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bt6xm3/why_rating_is_not_getting_scraped_the_site_is/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Roblox account Webscraper trouble shooting\n",
      "Text:  trying to write a scraper for finding roblox accounts with their name and bio using Beautiful Soup. To get the bio i could read the span tag of <span class=\"profile-about-content-text linkify\" ng-non-bindable=\"\">...</span> but am unaware as to how to just get the content of the span tag. Any direction as to how I can achieve this?\n",
      "\n",
      "fyi the \"profile-about-content-text linkify\" is how I initially tried to do it but it wouldn't return any results from the webpage...\n",
      "\n",
      "this is what i have written so far:\n",
      "\n",
      "'''\n",
      "\n",
      "url = f\"https://www.roblox.com/users/{userNo}/profile\"  \n",
      "result = requests.get(url)  \n",
      "doc = BeautifulSoup(result.text, \"html.parser\")  \n",
      "\n",
      "\n",
      "bio = doc.find\\_all(\"span\", {\"class\":\"profile-about-content-text linkify\"})  \n",
      "\n",
      "\n",
      "print(bio)\n",
      "\n",
      "'''\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bt14zy/roblox_account_webscraper_trouble_shooting/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do you guys find clients?\n",
      "Text: Hello\n",
      "\n",
      "I am planning to convert the scraping, automation and data extraction skills I've gathered into a real business.\n",
      "\n",
      "To all the professional web scraping freelancers or business owners out there, do you mind sharing:\n",
      "\n",
      "1. Where you find your clients?\n",
      "2. What type of clients, companies do you sell to and what exactly do you sell?\n",
      "3. Do you sell the data or do you sell the services?\n",
      "\n",
      "I have never done outbound prospecting, and I'm looking for some ideas of what exactly to sell and to whom.\n",
      "\n",
      "Any insights will be greatly appreciated. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bsapiu/how_do_you_guys_find_clients/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: The Tiktok API Signing process \n",
      "Text: Any one has any information about it? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bsjm17/the_tiktok_api_signing_process/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Would web scraping be a good way to find the price of a beer at every bar in my city?\n",
      "Text: As the title says. I don’t know much about scraping. I’m trying to index the cheapest bars \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bsvl7u/would_web_scraping_be_a_good_way_to_find_the/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help bypassing cloudflare\n",
      "Text: Hi!,\n",
      "\n",
      "A friend and I are currently working on a web scraping project where we're trying to extract data from a site protected by Cloudflare. We've attempted using selenium\\_stealth and undercover\\_chromedriver hoping to bypass the security measures, but we've only managed to get past the basic checks. Unfortunately, this isn't enough to get access to the site's content.\n",
      "\n",
      "How could we do it ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bsfc6q/need_help_bypassing_cloudflare/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can anyone help me with scraping text in Wiktionary?\n",
      "Text: I am using beautiful soup and when I try to scrape what I want, I get no errors/print statements from my code and no data. An example of a URL is https://en.m.wiktionary.org/wiki/%E6%BC%A2\n",
      "\n",
      "The following text is what I'm interested\n",
      "\n",
      "Phono-semantic compound (形聲／形声, OC *hnaːns): semantic 水 (“water”) + abbreviated phonetic 暵 (OC *hnaːnʔ, *hnaːns) – name of a river\n",
      "\n",
      "And all I want is to  scrape the Chinese characters after the words semantic and phonetic\n",
      "\n",
      "Any help is appreciated \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bspb9i/can_anyone_help_me_with_scraping_text_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can I ever get into Legal trouble for web scraping?\n",
      "Text: Hello, I am a freelance developer with a lot of experience in web scraping. I only did small gigs up until now but I'm trying to expand my skills into a real business now. It begs the question, is there EVER a possibility of getting to legal trouble for scraping data?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bs9a5d/can_i_ever_get_into_legal_trouble_for_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrapping Reddit\n",
      "Text: Hi guys, what is the current state of scrapping reddit? I've heard that API costs are enormous, but when I try to google scrapping reddit I get a lot of posts talking about PRAW, is it still an option to download all posts from given subreddit? Sorry for the naive question, I did not find an answer in google. PRAW github page is leading to old reddit API\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1brtxwj/scrapping_reddit/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Major Hotels Scraping \n",
      "Text: Any advice on the most effective and scaleable way to scrape the prices, points and info from the major hotel chains such as hilton, hyatt, marriott, etc?\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1brssjp/major_hotels_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: My wordpress websites are being massively scraped\n",
      "Text: Hi fellow Scrapers, is there an efficient way to block scraping bots on Woocommerce? My shops are being massively scraped (don't understand what for) \n",
      "\n",
      "I've been recommended recaptcha V3 and Cloudflare Turnstile, but to no avail. These solutions seems to protect forms/comment spam. It doesn't fire up when I try to scrape my own websites. \n",
      "\n",
      "Suggestions welcome. Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1brwxya/my_wordpress_websites_are_being_massively_scraped/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping API vs Proxy \n",
      "Text: Scraping API vs Proxy \n",
      "\n",
      "Hey,\n",
      "\n",
      "I've been doing some web scraping recently and\n",
      "I'm currently using [Vendor A] api for this purpose. It's very reasonably priced compared to other scraping APIs is fast and is very reliable.\n",
      "\n",
      "However I'm currently looking to reduce costs. For this purpose I'm doing some experiments using [Vendor B] rotational residencial proxy and I was very surprised to find out that with more or less the same price that I was paying in [Vendor A] I could only do a tiny fraction of the requests before all my traffic was gone...\n",
      "\n",
      "This was a surprise because implementing scraping with proxy is more complex, with everything related to the headless browser including all the additional code...\n",
      "\n",
      "Some notes regarding my usage with proxy.\n",
      "\n",
      "-Since I'm using a rotational proxy, when I hit a captcha challenge, I just do a new request.\n",
      "On average 1 in 15 request hit captcha.\n",
      "\n",
      "-I've also implemented strategies to save data, like blocking image requests and waiting only for the resources of interest to load up.\n",
      "\n",
      "Does anyone here also had the same experience or you guys thinking I'm doing something wrong?\n",
      "\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1brf3i8/scraping_api_vs_proxy/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Spotify's podcaster's email\n",
      "Text: I need to scrape emails and name podcasters on Spotify to pitch them some services, can somebody help me here?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1brlrdi/spotifys_podcasters_email/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for help on a specific website\n",
      "Text: My kids had some photos taken, we were told the photos were all included as part of our fees.  However in the end their website only lets us download 3 photos, and 2 of the 3 are preselected.  Being the grumpy guy I am I was able to re-enable right click with a chrome extension, and open up a bunch of the photos and download them.  The problem is they are crappy quality.\n",
      "\n",
      "I realized later that the photos ended in \"_s.jpg\" but some of them were \"_m.jpg\".   So I messed around and eventually realized I could get \"_xl.jpg\" which bumped the quality up a lot.\n",
      "\n",
      "I tried a few others, u, xxl, xl2, o... but none of them got me to a higher quality.  I also tried .raw which also didnt help.\n",
      "\n",
      "I figured I would ask if anyone knows this website and if theres any ways to get better quality images:\n",
      "\n",
      "https://internal.getphoto.io/img3/rzwert8r/im/*****_xl.jpg\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1br9khe/looking_for_help_on_a_specific_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Cleaning WikiQuote Dump (willing to pay)\n",
      "Text: I am cleaning up the wikiquote dump. i am getting fairly successful, with the help of chatgpt of course, but the sources still aren't getting the titles on all of the quotes. Sometimes it will say only like 'p.96' in my sources column but i want it to say like 'war and peace p.96'. But the thing is it does this for SOME of the quotes. Probably like 50% of them.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Here's my code  \n",
      "\n",
      "\n",
      "import sqlite3  \n",
      "import re  \n",
      "\n",
      "\n",
      "def clean\\_content(content):  \n",
      " if content is None:  \n",
      " return ''  \n",
      " content = re.sub(r'<\\[\\^>\\]+>', '', content).strip()  \n",
      " return content  \n",
      "\n",
      "\n",
      "def clean\\_wiki\\_markup(text):  \n",
      "text = re.sub(r'\\\\\\[\\\\\\[(?:\\[\\^|\\\\\\]\\]\\*\\\\|)?(\\[\\^\\\\\\]\\]+)\\\\\\]\\\\\\]', r'\\\\1', text)  \n",
      "url\\_pattern = r'(http\\[s\\]?://(?:\\[a-zA-Z\\]|\\[0-9\\]|\\[$-\\_@.&+\\]|\\[!\\*\\\\\\\\(\\\\\\\\),\\]|(?:%\\[0-9a-fA-F\\]\\[0-9a-fA-F\\]))+)'  \n",
      " text = re.sub(url\\_pattern, r'<a href=\"\\\\1\">\\\\1</a>', text)  \n",
      " return text  \n",
      "\n",
      "\n",
      "def parse\\_wikiquote\\_dump(text):  \n",
      "entries = \\[\\]  \n",
      "lines = text.split('\\\\n')  \n",
      " current\\_context = \"\"  \n",
      " i = 0  \n",
      " while i < len(lines):  \n",
      " \\# If the start of one of these sections is encountered, break the loop to stop parsing  \n",
      " if '== Disputed ==' in lines\\[i\\] or '== Misattributed ==' in lines\\[i\\] or '== Quotes about' in lines\\[i\\]:  \n",
      " break  \n",
      " section\\_match = re.match(r'\\^={2,}\\\\s\\*(.\\*?)\\\\s\\*={2,}$', lines\\[i\\])  \n",
      " if section\\_match:  \n",
      " current\\_context = section\\_match.group(1)  # Update current context for new section  \n",
      " if lines\\[i\\].startswith('\\*'):  # Check if the line starts with '\\*' for quotes  \n",
      " quote = lines\\[i\\]\\[1:\\].strip()  # Strip the leading '\\*' and whitespace  \n",
      " source = ''  \n",
      " i += 1  # Move to the next line for potential source details  \n",
      "\\# Collect additional details from lines starting with '\\*\\*'  \n",
      " while i < len(lines) and lines\\[i\\].startswith('\\*\\*'):  \n",
      " if source:  # Add a space before appending if source already has content  \n",
      " source += ' '  \n",
      " source += lines\\[i\\]\\[2:\\].strip()  # Append the source detail, stripping the '\\*\\*'  \n",
      " i += 1  \n",
      " entries.append((quote, clean\\_wiki\\_markup(source)))  \n",
      "i += 1  \n",
      " return entries  \n",
      "\n",
      "\n",
      "def create\\_quotes\\_database():  \n",
      "conn\\_existing = sqlite3.connect('/Users/puter/Desktop/wikiquote.db')  \n",
      "cursor\\_existing = conn\\_existing.cursor()  \n",
      "\n",
      "\n",
      "   conn\\_new = sqlite3.connect('/Users/puter/Desktop/all\\_quotes.db')  \n",
      "cursor\\_new = conn\\_new.cursor()  \n",
      "\n",
      "\n",
      "   cursor\\_new.execute('''  \n",
      "CREATE TABLE IF NOT EXISTS quotes (  \n",
      "id INTEGER PRIMARY KEY,  \n",
      "quote TEXT,  \n",
      "source TEXT,  \n",
      "author TEXT  \n",
      ")  \n",
      "''')  \n",
      "\n",
      "\n",
      "   cursor\\_existing.execute('SELECT \\* FROM pages')  \n",
      " for row in cursor\\_existing.fetchall():  \n",
      "id\\_, title, content = row  \n",
      "content = clean\\_content(content)  \n",
      "quotes\\_sources = parse\\_wikiquote\\_dump(content)  \n",
      " for quote, source in quotes\\_sources:  \n",
      "cleaned\\_quote = clean\\_wiki\\_markup(quote)  \n",
      "cleaned\\_source = clean\\_wiki\\_markup(source)  \n",
      "cursor\\_new.execute('''  \n",
      "INSERT INTO quotes (quote, source, author) VALUES (?, ?, ?)  \n",
      "''', (cleaned\\_quote, cleaned\\_source, title))  \n",
      "\n",
      "\n",
      "   conn\\_new.commit()  \n",
      "conn\\_existing.close()  \n",
      "conn\\_new.close()  \n",
      "\n",
      "\n",
      "create\\_quotes\\_database()  \n",
      "\n",
      "\n",
      "  \n",
      "If someone can help me get this CLEAN I will pay. I need this data.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1br68mr/cleaning_wikiquote_dump_willing_to_pay/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Addresses from Multiple Sites\n",
      "Text: Hello guys, I  hope you have a good one. I am new here so the first thing I did was to search this sub for my problem to not waste anyone's time but I didn't find anything similar, most probably my fault.  \n",
      "\n",
      "\n",
      "So, as the title says, I have received this task in order to be accepted at an internship and basically what I have to do is to extract the addresses of different sites. Now, I have experience with web scraping but on a single site( ex: getting names and prices of products from different categories).  \n",
      "\n",
      "\n",
      "You can probably already tell what my problem is. Different sites store their addresses differently. So, I assume I cannot use something simple like BeautifulSoup. I have heard of autoscraper but I never used it personally.  \n",
      "\n",
      "\n",
      "What do you guys think? Do you have any tips or tricks? Any experience with this stuff? The project is very interesting and I want to learn as much as I can from it.  \n",
      "\n",
      "\n",
      "Have a great day and sorry for the looong message!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bqoxqk/scraping_addresses_from_multiple_sites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Python selenium scrap data from dynamic website table\n",
      "Text:   \n",
      "\n",
      "Hi,\n",
      "\n",
      "I try to use selenium on python to scrap some information from website https://dexscreener.com/. Actually, I just want the name of the current upper one in the list (see screenshot). The filters and order of the list is not relevant as I have already implemented this separately.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "https://preview.redd.it/eq27elmd4brc1.jpg?width=1468&format=pjpg&auto=webp&s=a0e88d6c305333785858ab80fbb79fa4c8511591\n",
      "\n",
      "Using the following structure I tried several methods (By.XPATH, By.Name etc.) but couln’t get the name to print it. I am fairly new to selenium.\n",
      "\n",
      "I use the webdriver for chrome.\n",
      "\n",
      "Could someone help me with that?\n",
      "\n",
      "from selenium import webdriver  \n",
      " from selenium.webdriver.common.by import By  \n",
      " from selenium.webdriver.chrome.options import Options  \n",
      " \n",
      "\n",
      "options = Options()  \n",
      " options.add\\_experimental\\_option(\"debuggerAddress\", \"localhost:8989\")  \n",
      " driver = webdriver.Chrome(options=options)  \n",
      " \n",
      "\n",
      "driver.get(\"https://dexscreener.com/\")  \n",
      " \n",
      "\n",
      "table = driver.find\\_element(By.XPATH,\"...\")  \n",
      " print(..)\n",
      "\n",
      "Thanks a lot!!  \n",
      " Best\n",
      "\n",
      "Maverick\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bquqjd/python_selenium_scrap_data_from_dynamic_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web driver window updates after automated interaction but scraped data does not update\n",
      "Text: Hi.\n",
      "\n",
      "I am scraping a website with Selenium. There is a drop-down menu with a default option selected. Selecting a different option from the menu changes some text on the website and I want to scrape this text.\n",
      "\n",
      "My python script opens an automated Firefox window, selects the option from the drop-down menu, I see that the text is changed but I still have the old text of the default option from the drop-down menu in the rest of my script. I fail to retrieve the new text. What am I doing wrong? I provide more information below but if you need more I can share.\n",
      "\n",
      "There is a button that needs to be clicked that reveals the drop-down menu but the text of the default option from the drop-down menu is already visible. The button's inspected info is\n",
      "\n",
      "    <div id=\"...\" class=\"...\" onclick=\"Frame.onoff(...); Frame.onoff(...);\">\n",
      "        ...\n",
      "    </div>\n",
      "\n",
      "I find the button and click it with something like `button.click()`. Then the drop-down menu is revealed and its inspected info is\n",
      "\n",
      "    <select class=\"...\" name=\"...\" onchange=\"Ajax. ...\">\n",
      "        <option value=\"value1\" selected=\"selected\">option text 1</option>\n",
      "        <option value=\"value2\">option text 2</option>\n",
      "        ...\n",
      "    </select>\n",
      "\n",
      "I locate the drop-down menu and `menu.select_by_value('value2')` on it. The text is changed. I locate it but I get the `value1`'s text. What do I need to do?\n",
      "\n",
      "Edit: I can provide further details if you need. But you need to describe to me what you need, because I am not a web developer and I am not familiar with browser debugging/inspection. I shared everything above because I think those would be enough.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bqsvcy/web_driver_window_updates_after_automated/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What’s the best websites to practice advanced web scraping?\n",
      "Text: Hi guys, I’ve been web scraping for a while now and want to learn some of the deeper knowledge that goes into web scraping. Things like how to replicate cookie requests, dealing with CSRF tokens, and obfuscating JS code. I would also like to learn more about web protocols that would be useful when web scraping. What websites would you recommend? (Yes I know things like this are asked a lot, I looked through and this question hasn’t been asked)\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bqbblj/whats_the_best_websites_to_practice_advanced_web/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I created a Web Scraper that constantly refreshes the page. Are there any repercussions for this?\n",
      "Text: Right now it refreshes every 5 seconds, but I was wondering, could I have it refresh very frequently and not be blocked/banned or anything like that? Goal is to refresh the page like \\~25 times per minute.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bqf83l/i_created_a_web_scraper_that_constantly_refreshes/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What would you do?\n",
      "Text: Trying to knock two birds out with one stone with getting this documentation into txt files via web scraping (for training a ChatGPT model) and also getting better at Python.\n",
      "\n",
      "Requests with Beautiful Soup is pretty easy to understand, and I’ve gotten my head wrapped around selenium and scrapy now (at least a good bit).\n",
      "\n",
      "But pretty sure I did not pick the easiest starting point with trying to learn from this website. The table of contents on the left is not fully accessible without sending expanding with clicks (or using a crawler), and for most pages in the documentation, they have a URL fragment(?) menu on the right hand side.\n",
      "\n",
      "I’ve learned a good bit on what is useful, but since ChatGPT and Claude-3 are deceivingly optimistic about every strategy I propose to them and rarely critical - how would an veteran web-scraper typically tackle a format like this website? Are any of the mentioned methods either insufficient or overkill (scrapy, selenium, beautiful soup/requests)?\n",
      "\n",
      "[https://docs.inductiveautomation.com/docs/8.1/intro](https://docs.inductiveautomation.com/docs/8.1/intro)  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bqb2b5/what_would_you_do/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is BeautifulSoup right tool for the job?\n",
      "Text: Hi.\n",
      "\n",
      "I am scraping some text from a website using BeautifulSoup. In the website, there is a drop-down list with an already selected option. After scraping the first text, I need to select another option from this drop-down list. Selecting the different option replaces the previously scraped text with a new text which I need to scrape as well. I am able to inspect the website in web browser and locate the dropdown list and the texts I need to scrape but they don't seem to co-exist at the same time. Is BeautifulSoup right tool for the job? Should I look into MechanicalSoup or a different tool? Do you have a tool recommendation?\n",
      "\n",
      "Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bpr2gs/is_beautifulsoup_right_tool_for_the_job/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Mobile phone bot farm, how do they do it? I want to adapt it to scraping.\n",
      "Text: TLDR: looking for GitHub repo or resources on how to manage mobile phones for scraping. Something like this https://imgur.com/a/5Qbm3\n",
      "\n",
      "Full disclosure- I have no interest in doing anything sketchy like click fraud or whatever else is out there, i just want to use several mobile phones as a way to bypass some of the automated detection out there. I process a lot of data and this would save me money. \n",
      "\n",
      "I currently scrape several hundred million pages of data per month using proxies, VPS’s and a couple of servers at my house. My code is structured in a way that it tries to grab the data via get request, if that fails or is incomplete then it uses a headless browser, if that fails then it uses a headless browser with stealth plugins. It also ups the proxy with each level, e.g. data center > rotating residential > mobile.\n",
      "\n",
      "One thing I would like to do and have strongly considered is setting up 5-10 mobile phones, each with its own SIM card and unlimited data ($100 a month). Because of the amount of data I consume this would save me money compared to what I pay now per gb.\n",
      "\n",
      "The part I can’t figure out though is how to actually setup any type of automation using a phone. In my mind I imagine some type of app on the phone that either makes an api request every few seconds asking for more URLs to visits and then forwards the payloads back to the api OR it opens up a websocket and gets URLs and sends data that way.\n",
      "\n",
      "I have looked for GitHub repos and looked for information about this online but I haven’t been able to find any resources showing how it was actually implemented.\n",
      "\n",
      "Can anyone point me in the right direction?\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bp90y1/mobile_phone_bot_farm_how_do_they_do_it_i_want_to/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: YouTube scraping question\n",
      "Text: Hey fellas, I want to scrape as many channels as plausible that have videos that title contain the keyword \"crypto\". What would be the best approach to this granular targeting?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bpxaft/youtube_scraping_question/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What are the best tools out there? \n",
      "Text: Are there any actual working webscrapers?\n",
      "\n",
      "I’m looking for a web scraping tool either an api or a bot that has been tested and does what you expect.\n",
      "\n",
      "Is there any of you guys that may have come across and used something similar in the past or currently that will do the job of: \n",
      "\n",
      "- pulling product data such as; \n",
      "\n",
      "• Price \n",
      "• Name \n",
      "• ASIN/SKU/EAN \n",
      "\n",
      "and any other relevant information.\n",
      "\n",
      "Any help would be much appreciated. \n",
      "\n",
      "Thank you. \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bpk4l2/what_are_the_best_tools_out_there/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Daily scrape gets detected every 3/4 days? Fixed by clearing cookies?\n",
      "Text: I'm using Chrome, scarping with selenium. The site used to detect it every time, then I added the arguments:  \n",
      "`$Chrome_Options.AddArguments('user-data-dir=C:\\Users\\username\\AppData\\Local\\Google\\Chrome\\User Data')`  \n",
      "`$Chrome_Options.AddExcludedArgument(\"enable-automation\")`\n",
      "\n",
      "The script moves slower than I can manually do the same actions; just expanding a bunch of dropdowns, grabbing info, and then going to the next page.   \n",
      "How are they detecting me?   \n",
      "Also, less pressing, is there a way to surgically remove whatever flag is in the app data instead of nuking the cookies? I tried changing the cookies file in app data to match the cookies pre-detection but that didn't work. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bp7pm4/daily_scrape_gets_detected_every_34_days_fixed_by/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can anyone explain what's going on with scraping in Indonesia?\n",
      "Text: [https://trends.google.com/trends/explore?date=all&q=scraper&hl=en](https://trends.google.com/trends/explore?date=all&q=scraper&hl=en)  \n",
      "\n",
      "\n",
      "Web scraping is becoming quite popular in Indonesia. Why Indonesia? Can someone explain more about this? Maybe give a link to an article?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bp63pr/can_anyone_explain_whats_going_on_with_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Review Analysis\n",
      "Text: For those of you who use web scrapping on online reviews to look at data for different products, how do you go about analyzing these huge pools of data into something digestible and specific?   \n",
      "\n",
      "\n",
      "I've been trying to use HeyMarvin recently, but their capability isn't really for quantitative data, it doesn't seem to work. Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bp9036/review_analysis/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How analyzing SERP ranking can improve your website's search engine rankings?\n",
      "Text: As a developer, you're likely aware of the importance of Search Engine Results Pages (SERPs) in determining the visibility and success of your website. But have you ever wondered how understanding SERP itself can help you improve your SERP rankings? No? Let's see how. :)\n",
      "\n",
      "* Know Your SERP :  Google SERPs have evolved to include various features like knowledge graphs, image packs, and videos, in addition to the traditional organic results. These features enhance the user experience by providing quick answers or diverse content\n",
      "* Use structured data:  Structured data helps search engines understand and interpret your website's content better. Implementing schema markup, for example, can help Google display your content in a more engaging way on SERPs. This could include star ratings, event dates, or other relevant information.\n",
      "* Optimize for Mobile SERPs : Mobile device searches continue to rise, so optimizing for mobile is crucial. Ensure your website is mobile-friendly and load times are minimal. Google's mobile-first indexing means that the mobile version of your site will be prioritized in SERPs.\n",
      "* Monitor SERP Positions:  Keep a close eye on where your website's pages rank for different keywords. This awareness is crucial because even a slight change in SERP rankings can impact your website's traffic.\n",
      "\n",
      "That's interesting, but how to find SERP Rankings? There are 2 ways:\n",
      "\n",
      "* Follow a manual process - Search the query and check pages one by one. This is an approach that is both labor intensive (and therefore expensive)💰 and can not scale easily. OR\n",
      "* Leverage an #API: This way, they can add/feed all the search terms and get the relevant content/results. Sounds amazing?\n",
      "\n",
      "ApyHub's SERP Rank Checker #API can return a #JSON from any #Google search page.\n",
      "\n",
      "Try the API from here: [https://apyhub.com/utility/serp-rank](https://apyhub.com/utility/serp-rank)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bp7etu/how_analyzing_serp_ranking_can_improve_your/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Seeking Advice on LinkedIn Recruiter Lite Scraping Limits via Selenium\n",
      "Text: Hey everyone,\n",
      "\n",
      "I've been working on a project involving web scraping LinkedIn Recruiter Lite using Selenium, and I'm curious about the scraping limits per day imposed by LinkedIn. I want to ensure that I'm staying within the bounds of their usage policies to avoid any potential issues.\n",
      "\n",
      "Does anyone have experience or insights into the daily scraping limits for LinkedIn Recruiter Lite? Any tips or best practices for managing scraping activities to avoid being flagged or restricted by LinkedIn would be greatly appreciated.\n",
      "\n",
      "Thanks in advance for any help or guidance you can provide!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1botqgi/seeking_advice_on_linkedin_recruiter_lite/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape Walmart Data for Lego Set Prices\n",
      "Text: I am doing some research on Lego prices across different retailers. I have a little basic coding experience and have never done any scraping. Is there a tutorial or easy method to scrape the data on Lego set prices from Walmart (ideally 2 or 3 other retailers as well.)\n",
      "\n",
      "Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1boekos/scrape_walmart_data_for_lego_set_prices/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for the appropriate data scraping tool\n",
      "Text: Hi All,\n",
      "\n",
      "For my PhD, I am to collect news articles from four Surinamese news websites. For example, on [this website](https://www.starnieuws.com/) I would like to collect all articles that mention \"droogte\" (this is Dutch for Drought) in the last ten years. It would be great if I could get a file that includes the title, link, date, and text.\n",
      "\n",
      "Now, I am a human geographer with little knowledge of quantitative data. The actual analysis will be qualitative, but automated collection would save me days of pain.\n",
      "\n",
      "I see that there might be hiccups along the way here. The websites all have different ways of showing their articles and are not very sophisticated. But I would love to hear a (perhaps quite non-technical) tool that could assist me on this.\n",
      "\n",
      "Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bo5ced/looking_for_the_appropriate_data_scraping_tool/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there something like blackhatforum for webscraping?\n",
      "Text: Place where enthusiasts come together and discuss all things related to web-scraping. Technical stuff/Scaling bottlenecks/Business ideas/.... etc.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bnv1ez/is_there_something_like_blackhatforum_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping google maps review\n",
      "Text:  \n",
      "\n",
      "Hi everyone,\n",
      "\n",
      "I'm completely new to web scraping and have a project idea I'm excited about, but it seems a little over my head at the moment.\n",
      "\n",
      "I want to gather review data from Google Maps for a bunch of local businesses to do some analysis. Things like:\n",
      "\n",
      "* Star ratings\n",
      "* The review text itself\n",
      "* Dates of the reviews\n",
      "\n",
      "Here's what I'm confused about:\n",
      "\n",
      "* **Is this even allowed?** I don't want to get in trouble or violate Google's terms.\n",
      "* **Tools:** What kind of tools or programming do I need for this?\n",
      "* **Any tutorials or guides?** If this is okay to do, does anyone know of some good resources to teach me the basics?\n",
      "\n",
      "I'd really appreciate any help or advice you guys can offer!\n",
      "\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bnuume/scraping_google_maps_review/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help a newbie out plz: Scrape job boards such as indeed & Behance etc\n",
      "Text: Hey,\n",
      "\n",
      "Hope everyone is doing good?\n",
      "\n",
      "I'm looking to scrape job boards for information, so that I can reach out about specific roles. I've tried a couple of free tools online and couldn't get the result I wanted. As I'm completely new to this world, I wonder whether there is a tool I've missed, or this something I'm best paying a professional to do?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Appreciate any advice.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bo64lt/help_a_newbie_out_plz_scrape_job_boards_such_as/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: finding out a Facebook-scraper that works ...\n",
      "Text: hi there \n",
      "\n",
      "i am trying to get  data from a facebook group. There are some interesting groups out there. That said: what if there one that has a lot of valuable info, which I'd like to have offline. Is there any (cli) method to download it?\n",
      "\n",
      "i am wanting to download the data myself: Well if so we ought to build a program that gets the data for us through the graph api and from there i think we can do whatever we want with the data that we  get.\n",
      "\n",
      "that said:  Well i think that we can try in python to get the data from a facebook group. Using this SDK\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "    #!/usr/bin/env python3\n",
      "    \n",
      "    import requests\n",
      "    import facebook\n",
      "    from collections import Counter\n",
      "    \n",
      "    graph = facebook.GraphAPI(access_token='fb_access_token', version='2.7', timeout=2.00)\n",
      "    posts  = []\n",
      "    \n",
      "    \n",
      "    post = graph.get_object(id='{group-id}/feed') #graph api endpoint...group-id/feed\n",
      "    group_data = (post['data'])\n",
      "    \n",
      "    all_posts = []\n",
      "    \n",
      "    \"\"\"\n",
      "     Get all posts in the group.\n",
      "    \"\"\"\n",
      "    def get_posts(data=[]):\n",
      "        for obj in data:\n",
      "            if 'message' in obj:\n",
      "                print(obj['message'])\n",
      "                all_posts.append(obj['message'])\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    return the total number of times each word appears in the posts\n",
      "    \"\"\"\n",
      "    def get_word_count(all_posts):\n",
      "        all_posts = ''.join(all_posts)\n",
      "        all_posts = all_posts.split()\n",
      "        for  word in all_posts:\n",
      "            print(Counter(word))\n",
      "    \n",
      "        print(Counter(all_posts).most_common(5)) #5 most common words\n",
      "    \n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    return number of posts made in the group\n",
      "    \"\"\"\n",
      "    def posts_count(data):\n",
      "        return len(data)\n",
      "    \n",
      "\n",
      "get\\_posts(group\\_data) get\\_word\\_count(all\\_posts) Basically using the graph-api we can get all the info we need about the group such as likes on each post, who liked what, number of videos, photos etc and make your deductions from there.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Well besides this i think its worth to try to find a fb-scraper that works\n",
      "\n",
      "i did a quick research and saw on the relevant list of repos on GitHub, one that seems to be popular, up to date, and to work well is [https://github.com/kevinzg/facebook-scraper](https://github.com/kevinzg/facebook-scraper)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "    Example CLI usage:\n",
      "    pip install facebook-scraper\n",
      "    facebook-scraper --filename nintendo_page_posts.csv --pages 10 nintendo\n",
      "    \n",
      "\n",
      "well this fb-scraper was used by many many ppl.  i think its worth a try.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bntnuk/finding_out_a_facebookscraper_that_works/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Instagram?\n",
      "Text: I'm attempting to scrape Instagram for a school project, but it seems impossible. I want to accomplish this using Python, possibly with Scrapy-Splash. Can someone please help me? I'm not making any progress. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bnrt6n/scraping_instagram/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Seleniumbase in Docker\n",
      "Text: Hello, has anyone ever tried using seleniumbase in Docker? It does not seem to be working for me, I get an error (unknown error: cannot connect to chrome at [127.0.0.1:9222](https://127.0.0.1:9222)). Can anyone please share their experience with me when using this (such as dockerfiles as reference). Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bnkjkk/seleniumbase_in_docker/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is taking a screenshot of a Tiff image illegal?\n",
      "Text: If you want to use parts of a large high resolution image, (that a company sells as a tiff image) is it illegal to take jpg images of the large tiff image? Or rather would it be considered modifying their product if you used it in academic research? I feel like it's a totally different thing at this point. You are only taking pictures of a very high resolution tiff image. They sell the whole tiff image file not jpg screenshots of said image. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bnmrcy/is_taking_a_screenshot_of_a_tiff_image_illegal/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Easiest way to scrap any Twitter (X) account tweets without authentication\n",
      "Text: Is such a tool available? I'm not a software developer. I'm looking for a tool something like yt-dlp/youtube-dl. It would be better if there is a GUI for it. Exporting tweets as xlsx file would be good. Thanks.  \n",
      "\n",
      "Came across [something like that](https://github.com/ThallesP/twitter-scraper-openaccount) but it looks complicated to me.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bncrma/easiest_way_to_scrap_any_twitter_x_account_tweets/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Beginners Question (HELP NEEDED)\n",
      "Text:  hi , i just wanted to ask if you can tell me if this site can be scrapped or not. i've tried many ways but no results. so i just wanted to know .   \n",
      "[https://www.enterprise.com/en/car-rental.html?icid=header.reservations.car.rental-\\_-start.a.res-\\_-ENUS.NULL](https://www.enterprise.com/en/car-rental.html?icid=header.reservations.car.rental-_-start.a.res-_-ENUS.NULL) \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bn6sch/beginners_question_help_needed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How many scrape requests do you find you're able to do per day by site size or type (small site, medium site etc.)?\n",
      "Text: Looking to scrape lots of data from sites without overloading them or causing them any issues that will cause conflicts with scraping.\n",
      "\n",
      "If I wanted to scrape a thousand to ten thousand pages, what setup do I need - proxy w/ rotating addresses per every x requests or proxy chain or dynamic proxy, vpn, browser and request header changes, pause between requests especially time.sleep(1) before request time.sleep(3) after request etc.?\n",
      "\n",
      "Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bmrx4c/how_many_scrape_requests_do_you_find_youre_able/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Why web scraping? \n",
      "Text: New to web scraping. \n",
      "Just curious what are all the reasons to scrab webs.\n",
      "Freelance work or selling the data. \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bmz7og/why_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What website have you found to be the most difficult to scrape?\n",
      "Text: Just curious what websites most people have struggled to scrape and what type of techniques they are using to make it hard for scrapers.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bmfdnk/what_website_have_you_found_to_be_the_most/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: PM2 with puppeteer \n",
      "Text: Anyone here tried using PM2 to deal with timeout error with puppeteer? Does running the script on time interval to scrape  one website instead of one big request doable with PM2??\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bmikun/pm2_with_puppeteer/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape websites in google sheets\n",
      "Text: I have a google sheet of 1000 websites that I want to gather their email/contact info. Can someone point me in the right direction?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bm8gk4/scrape_websites_in_google_sheets/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape hacker news\n",
      "Text: \n",
      "URL: https://v.redd.it/6qzxeyb9w5qc1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I'm unsure how a website is seemingly discovering that I'm using a web scraper\n",
      "Text: Okay, so basically I'm using Playwright and Python to automate filling in an online survey to get a voucher code.\n",
      "\n",
      "What's happening, is after running the script successfully once, usually on a second run, the website continually responds with 504 errors. This only seems to happen when using my automated tool, and seemingly the only way to fix this is to wait.\n",
      "\n",
      "I've used playwright-stealth with Python, randomised the user agent, etc. etc. and I'm just not sure why this is happening.\n",
      "\n",
      "This there anything I can do to figure this out? I can provide the website if that's of use but not sure if it is.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bm07v1/im_unsure_how_a_website_is_seemingly_discovering/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: my scrapy-splash is not rendering the section I need\n",
      "Text: I am trying to scrape this website to get the course requirement information for the specific major for my database but when I send a request with scrapy splash, it returns only the elements in the header and footer.\n",
      "\n",
      "[https://catalog.registrar.ucla.edu/major/2022/mathematicsofcomputationbs?siteYear=2022](https://catalog.registrar.ucla.edu/major/2022/mathematicsofcomputationbs?siteYear=2022)\n",
      "\n",
      "    class ScrapingClubSpider(scrapy.Spider):\n",
      "        name = \"courses\"\n",
      "        allowed_domains = [\"catalog.registrar.ucla.edu\"]\n",
      "    \n",
      "        def start_requests(self):\n",
      "            url = \"https://catalog.registrar.ucla.edu/major/2022/mathematicsofcomputationbs?siteYear=2022\"\n",
      "            yield SplashRequest(url, callback=self.parse)\n",
      "    \n",
      "        \n",
      "        def parse(self, response):\n",
      "            # Extract the HTML content from the response\n",
      "            extracted_content = response.css('div').get()\n",
      "            \n",
      "            yield {\n",
      "                \"extracted_content\": extracted_content\n",
      "            }\n",
      "\n",
      "I've tried to go to my splash render page and put the link in but the image that showed what was rendered does not include anything after 'Mathematics of Computations BS'. I was considering switching to selenium to see if that was the issue but from my understanding it shouldn't since it just a dynamic website.\n",
      "\n",
      "I've tried:\n",
      "\n",
      "    import re\n",
      "    import requests\n",
      "    r = requests.get(url = \"https://catalog.registrar.ucla.edu/major/2022/mathematicsofcomputationbs?siteYear=2022\")\n",
      "    body= r.text\n",
      "    filter1 = re.findall('\"pageContent\":.+?description\":\".+?\"', body)\n",
      "    overview = re.findall('\"description\":\".+?\"', filter1[0])\n",
      "    print(overview[0])\n",
      "\n",
      "but realized this method is not feasible if I plan on scraping information from other majors too and their course information. I would be very greatly if someone can help explain why its not rendering somethings properly and how I would go about scraping this website?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bm38p0/my_scrapysplash_is_not_rendering_the_section_i/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Zillow scraper made in Go\n",
      "Text: Hello everyone, I  just created an openn source web scraper for Zillow\n",
      "\n",
      "[https://github.com/johnbalvin/gozillow](https://github.com/johnbalvin/gozillow/)\n",
      "\n",
      "I created a vm on AWS just for testing, I'll delete it in probably next week, you can use it to verify that the project works very well  \n",
      "\n",
      "\n",
      "example for extracting details given ID: [http://3.94.116.108/details?id=44494376](http://3.94.116.108/details?id=44494376)\n",
      "\n",
      "example for searching given coordinates: \n",
      "\n",
      "[http://3.94.116.108/search?neLat=11.626466321336217&neLong=-83.16752421667513&swLat=8.565185490351908&swLong=-85.62044033549569&zomValue=2](http://3.94.116.108/search?neLat=11.626466321336217&neLong=-83.16752421667513&swLat=8.565185490351908&swLong=-85.62044033549569&zomValue=2)  \n",
      "It looks like the some info is been leaked on the server, like the agent's license number, I don't use zillow, so I'm not sure if this info should be public or not, if someonce could confirm if this info will be great\n",
      "\n",
      "[http://3.94.116.108/details?id=44494376](http://3.94.116.108/details?id=44494376) example:   \n",
      "\n",
      "\n",
      "https://preview.redd.it/52sutmaizzpc1.png?width=512&format=png&auto=webp&s=a8cab04e298c88fe7dd6a00f0906ab720a277aa3\n",
      "\n",
      "  \n",
      "If you use often the library, you will get blocked for a few hours, try using a proxy instead\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1blht5o/zillow_scraper_made_in_go/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Problems Making an Image Backup of a Webpage\n",
      "Text: Howdy y'all,\n",
      "\n",
      "The people over at r/DataHoarder recommended me to come ask you guys this question.\n",
      "\n",
      "I'm trying to use WebCopy to make an image of a webpage containing a lot of older instrumentation documentation so that there are other copies out there in case this one eventually goes down, but I am running into some problems. I do have a user account to access the page, but whenever I try and use WebCopy or HttTrack, it doesn't show any of the subdirectories or documents. It instead gives me an authorization required error. I entered my account credentials into WebCopy, so it should be logging in if I am understanding it correctly. Does anyone have any idea what I might be doing wrong here?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bm1get/problems_making_an_image_backup_of_a_webpage/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Why do so many of you scrape e-commerce websites?\n",
      "Text: This seems to be the most common use case. But why?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1blk43p/why_do_so_many_of_you_scrape_ecommerce_websites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Does anyone use GPT Vision for pricing page web scraping?\n",
      "Text: Has anyone succeeded in using GPT Vision to parse e-commerce data that contains numerical information? I have found it to be useful and cost-effective on a small to medium scale. However, the non-deterministic nature of OCR is preventing me from fully committing to it. \n",
      "\n",
      "Currently, my process consists of the following steps:\n",
      "\n",
      "1. I use Selenium to take screenshots and manipulate them as needed.\n",
      "\n",
      "2. I pass the screenshot, along with metadata about the page, to a GPT-4 model. The model is given a detailed prompt that addresses the key features that need to be extracted.\n",
      "\n",
      "3. I perform manual verification of the results and make any necessary improvements to the prompt.\n",
      "\n",
      "Want to try passing the HTML doc along with an image as well. \n",
      "\n",
      "Can anyone share their experience with using GPT Vision for this purpose?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bl29hw/does_anyone_use_gpt_vision_for_pricing_page_web/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to click on cloudflare's human verification checkbox programatically?\n",
      "Text: I'm using selenium to access a url that has cloudflare protection. I'm neither able to find the checkbox programmatically, nor able to avoid being detected in the first place. I'm currently exploring the possibility of replacing all selenium code with `requests`-based approach, but there's many additional complexities associated with the conversion, therefore I need the selenium approach to work for the time being. I tried other approaches with `seleniumbase` and `undetected-chromedriver`, both trigger the human check, so neither is an option as well.\n",
      "\n",
      "After some experimentation with selenium flags, I managed to make the manual clicking work. Now I want to make the clicking happen programmatically but whenever I lookup the checkbox, it's not there.\n",
      "This is the html contained in the iframe which contains a #document thing:\n",
      "```\n",
      "<html class=\"lang-en-us\" lang=\"en-US\">  \n",
      "<head>  \n",
      "<meta content=\"IE=Edge,chrome=1\" http-equiv=\"X-UA-Compatible\">  \n",
      "<meta content=\"noindex, nofollow\" name=\"robots\">  \n",
      "<meta content=\"width=device-width, initial-scale=1, maximum-scale=1\" name=\"viewport\">  \n",
      "<link as=\"image\" href=\"/cdn-cgi/challenge-platform/h/g/cmg/1/5C%2BC%2ByWwxMhwHSFzD6IXss7k3Ce7cQUSnvOIGdr3VHo%3D\"  \n",
      " rel=\"preload\">  \n",
      "<title>Just a moment...</title>  \n",
      "<script>  \n",
      " (function () {  \n",
      " ***window***.\\_cf\\_chl\\_opt = {  \n",
      " cvId: '3',  \n",
      " cZone: 'challenges.cloudflare.com',  \n",
      " cTplV: 5,  \n",
      " chlApivId: '0',  \n",
      " chlApiWidgetId: 'bgr5h',  \n",
      " chlApiSitekey: '0x4AAAAAAAAjq6WYeRDKmebM',  \n",
      " chlApiMode: 'managed',  \n",
      " chlApiSize: 'normal',  \n",
      " chlApiRcV: '1/LgjrycEEA8PT3fG',  \n",
      " chlApiTimeoutEncountered: 0,  \n",
      " chlTimeoutMs: 120000,  \n",
      " cK: \"visitor-time\",  \n",
      " cType: 'chl\\_api\\_m',  \n",
      " cNounce: '66121',  \n",
      " cRay: '868999239a3db4a8',  \n",
      " cHash: '3d4832a363db77b',  \n",
      " cFPWv: 'g',  \n",
      " cLt: 'n',  \n",
      " cRq: {  \n",
      " ru: 'aHR0cDovL2NoYWxsZW5nZXMuY2xvdWRmbGFyZS5jb20vY2RuLWNnaS9jaGFsbGVuZ2UtcGxhdGZvcm0vaC9nL3R1cm5zdGlsZS9pZi9vdjIvYXYwL3JjdjAvMC9iZ3I1aC8weDRBQUFBQUFBQWpxNldZZVJES21lYk0vZGFyay9ub3JtYWw=',  \n",
      " ra: 'TW96aWxsYS81LjAgKE1hY2ludG9zaDsgSW50ZWwgTWFjIE9TIFggMTBfMTVfNykgQXBwbGVXZWJLaXQvNTM3LjM2IChLSFRNTCwgbGlrZSBHZWNrbykgQ2hyb21lLzEyMy4wLjAuMCBTYWZhcmkvNTM3LjM2',  \n",
      " rm: 'R0VU',  \n",
      " d: 'gFqOGXWJCaCU2MiUcThgN9QMrx1oCYZHgH1aJBYvFaLAxORt4pafmlShGcWQhD5leDt0MhXtI8I3NaSwLCy6U5FPHlNDzdu2AZ474WUjfTFm0lyD/pqRaihuZsI+xU90Hkg9GhjNGTv9mbGcxlSqCCNGbG4WN7wvB5vcv1guXcHhy4rLVzt1hg+Khkcka9uW/SG4VmD6T7b0k3EnP/zeh8OH5C6B3n53CUmKauOFn1GZ7x7WU3Ed+n+TUNmAKRr/pWmLXiMM6warmVhyXjuqgN/DUuzyTf7WXcbZYcOBauwEZFBkahnnsmKFXjUZ+BkjL1D5juBv3wPBCC692nrNhaKQqWQbVqXP7cJymC7b1pcifUlKYP2toaGjwGvRrrCTES72KAeQujflrOzUp/GwnIAXynIYstsRpVuI2TElCupnIjzuEGrzRikjuDfTzX9PHj8ESFA7WDNztSvXwJbtMe+S09EdH0lkW6nXqvDoeSIhyHhaazNWkRz7cjVKhHpU0vFYhP5SDwzzrhB0aO0QA//mEln+y9lQwUC9Nv76UWDU571H2x3m/XZWSVAFfDmvUuye8hxaJnYwYroNLqAuA7VBkFxfO25kweHo/HSewnnuOGTacQSiyV1tninrm3Tb',  \n",
      " t: 'MTcxMTE0NjMzMi43NTE=',  \n",
      " m: 'G2Y52PPhILQtNvat1sLD0EM3phzlsvaqdseov2TuWb0=',  \n",
      " i1: 'jKby7Qnj3yNSSq5GiUjFIw==',  \n",
      " i2: 'zRkddtlZhK9bVGKp5KJstg==',  \n",
      " uh: '5C+C+yWwxMhwHSFzD6IXss7k3Ce7cQUSnvOIGdr3VHo=',  \n",
      " hh: 'WCiLdNo2uN2aXsfJJhG2HFkP3bOo0fw8tsFAppLisvs=',  \n",
      " zh: '9D+zbxCfwBPyr1pF5Wb5E9kRItcGU2xCgzO1zGTKToQ=',  \n",
      " }  \n",
      "};  \n",
      "var handler = function (event) {  \n",
      " var e = event.data;  \n",
      "if (e.source && e.source === 'cloudflare-challenge' && e.event === 'meow' && e.widgetId === ***window***.\\_cf\\_chl\\_opt.chlApiWidgetId) {  \n",
      " if (***window***\\['parent'\\]) {  \n",
      " ***window***\\['parent'\\].postMessage({  \n",
      " source: 'cloudflare-challenge',  \n",
      " widgetId: ***window***.\\_cf\\_chl\\_opt.chlApiWidgetId,  \n",
      " event: 'food',  \n",
      " seq: e.seq,  \n",
      " }, \"\\*\");  \n",
      " }  \n",
      "}  \n",
      "}  \n",
      " if (***window***.addEventListener) {  \n",
      " ***window***.addEventListener('message', handler);  \n",
      " } else {  \n",
      " ***window***.attachEvent('onmessage', handler);  \n",
      " }  \n",
      "}());  \n",
      " </script>  \n",
      "<script src=\"/cdn-cgi/challenge-platform/h/g/orchestrate/chl\\_api/v1?ray=868999239a3db4a8\"></script>  \n",
      "</head>  \n",
      "<body class=\"theme-dark size-normal\">  \n",
      "<div class=\"main-wrapper\">  \n",
      "<noscript>  \n",
      "<h1 style=\"color:#bd2426;\">Please turn JavaScript on and reload the page.</h1>  \n",
      "</noscript>  \n",
      "<div aria-atomic=\"true\" aria-live=\"polite\" id=\"content\" style=\"display: flex;\">  \n",
      "<div id=\"challenge-stage\" style=\"display: flex;\">  \n",
      "<div class=\"ctp-checkbox-container\" role=\"alert\" style=\"display: flex;\"><label  \n",
      " class=\"ctp-checkbox-label\"><input type=\"checkbox\"><span class=\"mark\"></span><span class=\"ctp-label\">Verify you are human</span></label>  \n",
      "</div>  \n",
      "</div>  \n",
      "<div class=\"cb-container\" id=\"verifying\" style=\"display: none; visibility: hidden;\">  \n",
      "<svg aria-hidden=\"true\" class=\"unspun\" fill=\"none\" id=\"spinner-icon\" viewBox=\"0 0 30 30\"  \n",
      " xmlns=\"http://www.w3.org/2000/svg\">  \n",
      "<line class=\"circle\" x1=\"15\" x2=\"15\" y1=\"1.5\" y2=\"5.5\"></line>  \n",
      "<line class=\"circle\" transform=\"rotate(45 24.5459 5.45405)\" x1=\"24.5459\" x2=\"24.5459\" y1=\"5.45405\"  \n",
      " y2=\"10.45405\"></line>  \n",
      "<line class=\"circle\" transform=\"rotate(90 28.5 15)\" x1=\"28.5\" x2=\"28.5\" y1=\"15\" y2=\"20\"></line>  \n",
      "<line class=\"circle\" transform=\"rotate(135 24.5459 24.546)\" x1=\"24.5459\" x2=\"24.5459\" y1=\"24.546\"  \n",
      " y2=\"29.546\"></line>  \n",
      "<line class=\"circle\" transform=\"rotate(180 15 28.5)\" x1=\"15\" x2=\"15\" y1=\"28.5\" y2=\"33.5\"></line>  \n",
      "<line class=\"circle\" transform=\"rotate(-135 5.4541 24.5459)\" x1=\"5.4541\" x2=\"5.4541\" y1=\"24.5459\"  \n",
      " y2=\"29.5459\"></line>  \n",
      "<line class=\"circle\" transform=\"rotate(-90 1.5 15)\" x1=\"1.5\" x2=\"1.5\" y1=\"15\" y2=\"20\"></line>  \n",
      "<line class=\"circle\" transform=\"rotate(-45 5.45408 5.45404)\" x1=\"5.45408\" x2=\"5.45408\" y1=\"5.45404\"  \n",
      " y2=\"10.45404\"></line>  \n",
      "</svg>  \n",
      "<div id=\"verifying-msg\">  \n",
      "<span id=\"verifying-text\">Verifying...</span>  \n",
      "</div>  \n",
      "</div>  \n",
      "<div class=\"cb-container\" id=\"success\" role=\"alert\" style=\"display:none\">  \n",
      "<svg aria-hidden=\"true\" id=\"success-icon\" viewBox=\"0 0 52 52\" xmlns=\"http://www.w3.org/2000/svg\">  \n",
      "<circle class=\"success-circle\" cx=\"26\" cy=\"26\" r=\"25\"></circle>  \n",
      "<path class=\"p1\" d=\"m13,26l9.37,9l17.63,-18\"></path>  \n",
      "</svg>  \n",
      "<span id=\"success-text\">Success!</span>  \n",
      "</div>  \n",
      "<div class=\"cb-container\" id=\"fail\" role=\"alert\" style=\"display:none\">  \n",
      "<svg aria-hidden=\"true\" id=\"fail-icon\" viewBox=\"0 0 52 52\" xmlns=\"http://www.w3.org/2000/svg\">  \n",
      "<circle class=\"failure-circle\" cx=\"26\" cy=\"26\" fill=\"none\" r=\"25\"></circle>  \n",
      "<path class=\"failure-cross\" d=\"M14.1 27.2 l24.124.2\" fill=\"none\"></path>  \n",
      "</svg>  \n",
      "<div id=\"failure-msg\">  \n",
      "<span id=\"fail-text\">Failure!</span>  \n",
      "<br><span id=\"fr-helper\"><a href=\"#refresh\" id=\"fr-helper-link\">Having trouble?</a></span>  \n",
      "</div>  \n",
      "</div>  \n",
      "<div class=\"cb-container\" id=\"expired\" role=\"alert\" style=\"display:none\">  \n",
      "<svg aria-hidden=\"true\" id=\"expired-icon\" viewBox=\"0 0 52 52\" xmlns=\"http://www.w3.org/2000/svg\">  \n",
      "<circle class=\"expired-circle\" cx=\"26\" cy=\"26\" r=\"25\"></circle>  \n",
      "<path class=\"expired-p1\" d=\"m13,32l15,0l0,-23\"></path>  \n",
      "</svg>  \n",
      "<div id=\"expiry-msg\">  \n",
      "<span id=\"expired-text\">Expired.</span><a href=\"#refresh\" id=\"expired-refresh-link\">Refresh</a>  \n",
      "</div>  \n",
      "</div>  \n",
      "<div class=\"cb-container\" id=\"challenge-error\" role=\"alert\" style=\"display:none\">  \n",
      "<span id=\"challenge-error-text\"></span>  \n",
      "</div>  \n",
      "<div id=\"branding\">  \n",
      "<a class=\"cf-link\" href=\"https://www.cloudflare.com/products/turnstile/?utm\\_source=turnstile\\&amp;utm\\_campaign=widget\"  \n",
      " rel=\"noopener noreferrer\"  \n",
      " target=\"\\_blank\">  \n",
      "<svg aria-label=\"Cloudflare\" fill=\"none\" id=\"logo\" role=\"img\" viewBox=\"0 0 74 25\"  \n",
      " xmlns=\"http://www.w3.org/2000/svg\">  \n",
      "<path d=\"M61.8848 15.7841L62.0632 15.1578C62.2758 14.4126 62.1967 13.7239 61.8401 13.2178C61.5118 12.7517 60.9649 12.4773 60.3007 12.4453L47.7201 12.2836C47.6811 12.2829 47.6428 12.2728 47.6083 12.2542C47.5738 12.2356 47.5442 12.209 47.5217 12.1766C47.4996 12.1431 47.4856 12.1049 47.4807 12.0649C47.4758 12.025 47.4801 11.9844 47.4933 11.9465C47.5149 11.8839 47.5541 11.8291 47.6061 11.7888C47.658 11.7486 47.7204 11.7247 47.7856 11.72L60.4827 11.5566C61.9889 11.4864 63.6196 10.2462 64.1905 8.73372L64.9146 6.81361C64.9443 6.73242 64.951 6.64444 64.9341 6.55957C64.112 2.80652 60.8115 0 56.8652 0C53.2293 0 50.1421 2.38158 49.0347 5.69186C48.2864 5.12186 47.3535 4.85982 46.4228 4.95823C44.6785 5.13401 43.276 6.55928 43.1034 8.32979C43.059 8.77189 43.0915 9.21845 43.1992 9.64918C40.3497 9.73347 38.0645 12.1027 38.0645 15.0151C38.0649 15.2751 38.0838 15.5347 38.1212 15.7919C38.1294 15.8513 38.1584 15.9057 38.2029 15.9452C38.2474 15.9847 38.3044 16.0067 38.3635 16.0071L61.5894 16.0099C61.5916 16.0101 61.5938 16.0101 61.596 16.0099C61.6616 16.0088 61.7252 15.9862 61.7772 15.9455C61.8293 15.9049 61.867 15.8483 61.8848 15.7841Z\"  \n",
      " fill=\"#F6821F\"></path>  \n",
      "<path d=\"M66.0758 6.95285C65.9592 6.95285 65.843 6.95582 65.7274 6.96177C65.7087 6.96312 65.6904 6.96719 65.6729 6.97385C65.6426 6.98437 65.6152 7.00219 65.5931 7.02579C65.5711 7.04939 65.555 7.07806 65.5462 7.10936L65.0515 8.84333C64.8389 9.58847 64.918 10.2766 65.2749 10.7827C65.6029 11.2494 66.1498 11.5233 66.814 11.5552L69.4959 11.7186C69.5336 11.7199 69.5705 11.73 69.6037 11.7483C69.6369 11.7666 69.6654 11.7925 69.687 11.8239C69.7092 11.8576 69.7234 11.896 69.7283 11.9363C69.7332 11.9765 69.7288 12.0173 69.7153 12.0555C69.6937 12.118 69.6546 12.1727 69.6028 12.2129C69.5509 12.2531 69.4887 12.2771 69.4236 12.2819L66.6371 12.4453C65.1241 12.5161 63.4937 13.7558 62.9233 15.2682L62.722 15.8022C62.7136 15.8245 62.7105 15.8486 62.713 15.8724C62.7155 15.8961 62.7236 15.9189 62.7365 15.9389C62.7495 15.9589 62.7669 15.9755 62.7874 15.9873C62.8079 15.9991 62.8309 16.0058 62.8544 16.0068C62.8569 16.0068 62.8592 16.0068 62.8618 16.0068H72.4502C72.506 16.0073 72.5604 15.9893 72.6051 15.9554C72.6498 15.9216 72.6823 15.8739 72.6977 15.8195C72.8677 15.2043 72.9535 14.5684 72.9529 13.9296C72.9517 10.0767 69.8732 6.95285 66.0758 6.95285Z\"  \n",
      " fill=\"#FBAD41\"></path>  \n",
      "<path class=\"logo-text\"  \n",
      " d=\"M8.11963 18.8904H9.75541V23.4254H12.6139V24.8798H8.11963V18.8904Z\"></path>  \n",
      "<path class=\"logo-text\"  \n",
      " d=\"M14.3081 21.9023V21.8853C14.3081 20.1655 15.674 18.7704 17.4952 18.7704C19.3164 18.7704 20.6653 20.1482 20.6653 21.8681V21.8853C20.6653 23.6052 19.2991 24.9994 17.4785 24.9994C15.6578 24.9994 14.3081 23.6222 14.3081 21.9023ZM18.9958 21.9023V21.8853C18.9958 21.0222 18.3806 20.2679 17.4785 20.2679C16.5846 20.2679 15.9858 21.0038 15.9858 21.8681V21.8853C15.9858 22.7484 16.6013 23.5025 17.4952 23.5025C18.3973 23.5025 18.9958 22.7666 18.9958 21.9023Z\"></path>  \n",
      "<path class=\"logo-text\"  \n",
      " d=\"M22.6674 22.253V18.8901H24.3284V22.2191C24.3284 23.0822 24.7584 23.4939 25.4159 23.4939C26.0733 23.4939 26.5034 23.1003 26.5034 22.2617V18.8901H28.1647V22.2093C28.1647 24.1432 27.0772 24.9899 25.3991 24.9899C23.7211 24.9899 22.6674 24.1268 22.6674 22.2522\"></path>  \n",
      "<path class=\"logo-text\"  \n",
      " d=\"M30.668 18.8907H32.9445C35.0526 18.8907 36.275 20.1226 36.275 21.8508V21.8684C36.275 23.5963 35.0355 24.88 32.911 24.88H30.668V18.8907ZM32.97 23.4076C33.9483 23.4076 34.597 22.8609 34.597 21.8928V21.8759C34.597 20.9178 33.9483 20.3614 32.97 20.3614H32.3038V23.4082L32.97 23.4076Z\"></path>  \n",
      "<path class=\"logo-text\"  \n",
      " d=\"M38.6525 18.8904H43.3738V20.3453H40.2883V21.3632H43.079V22.7407H40.2883V24.8798H38.6525V18.8904Z\"></path>  \n",
      "<path class=\"logo-text\" d=\"M45.65 18.8904H47.2858V23.4254H50.1443V24.8798H45.65V18.8904Z\"></path>  \n",
      "<path class=\"logo-text\"  \n",
      " d=\"M54.4187 18.8475H55.9949L58.5079 24.8797H56.7541L56.3238 23.8101H54.047L53.6257 24.8797H51.9058L54.4187 18.8475ZM55.8518 22.5183L55.1941 20.8154L54.5278 22.5183H55.8518Z\"></path>  \n",
      "<path class=\"logo-text\"  \n",
      " d=\"M60.6149 18.8901H63.4056C64.3083 18.8901 64.9317 19.13 65.328 19.5406C65.6742 19.883 65.8511 20.3462 65.8511 20.9357V20.9526C65.8511 21.8678 65.3691 22.4754 64.6369 22.7919L66.045 24.88H64.1558L62.9671 23.0658H62.2507V24.88H60.6149V18.8901ZM63.3299 21.7654C63.8864 21.7654 64.2071 21.4915 64.2071 21.0551V21.0381C64.2071 20.5674 63.8697 20.328 63.3211 20.328H62.2507V21.7665L63.3299 21.7654Z\"></path>  \n",
      "<path class=\"logo-text\"  \n",
      " d=\"M68.2112 18.8904H72.9578V20.3024H69.8302V21.209H72.6632V22.5183H69.8302V23.4683H73V24.8798H68.2112V18.8904Z\"></path>  \n",
      "<path class=\"logo-text\"  \n",
      " d=\"M4.53824 22.6043C4.30918 23.13 3.82723 23.5022 3.18681 23.5022C2.29265 23.5022 1.67746 22.7493 1.67746 21.8851V21.8678C1.67746 21.0047 2.27593 20.2676 3.1698 20.2676C3.84367 20.2676 4.35681 20.6882 4.5734 21.2605H6.29764C6.02151 19.8349 4.78716 18.7707 3.18681 18.7707C1.36533 18.7707 0 20.1666 0 21.8851V21.9021C0 23.6219 1.3486 25 3.1698 25C4.72762 25 5.94525 23.9764 6.26645 22.6046L4.53824 22.6043Z\"></path>  \n",
      "</svg>  \n",
      "</a>  \n",
      "<div id=\"terms\">  \n",
      "<a href=\"https://www.cloudflare.com/privacypolicy/\" id=\"privacy-link\" rel=\"noopener noreferrer\"  \n",
      " target=\"\\_blank\">Privacy</a><span class=\"link-spacer\"> • </span><a  \n",
      " href=\"https://www.cloudflare.com/website-terms/\" id=\"terms-link\" rel=\"noopener noreferrer\"  \n",
      " target=\"\\_blank\">Terms</a>  \n",
      "</div>  \n",
      "</div>  \n",
      "</div>  \n",
      "</div>  \n",
      "</body>  \n",
      "</html>\n",
      "\n",
      "```\n",
      "I want to access the `<input type=\"checkbox\">` so I do:\n",
      "```\n",
      ">>> driver.switch_to.frame(driver.find_element(By.XPATH, '//iframe'))\n",
      ">>> driver.find_elements(By.CSS_SELECTOR, 'input[type=checkbox]')\n",
      "[]\n",
      "```\n",
      "And I also tried accessing the checkbox using JS:\n",
      "```\n",
      "> document.querySelector('#challenge-stage > div > label > input[type=checkbox]')\n",
      "null\n",
      "```\n",
      "Any idea how this could be achieved?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1blcq3z/how_to_click_on_cloudflares_human_verification/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Frames and Selenium \n",
      "Text: Current issue. I have come across multiple websites I need to scrape that seem straight forward however my driver is unable to find the content via xpath even when I am able to find the content in driver.page_source. I have a feeling this has to do with how the frames work on the sites but in some cases I am not able to find the frame itself. Had anyone experienced this? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bl69u4/frames_and_selenium/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Quickly Access Your Automate WorkFlow From Context Menu | Automa\n",
      "Text:  You can quickly access your workflow of routine task by contextmenu \n",
      "\n",
      "[https://youtu.be/bB29hKZqpgI](https://youtu.be/bB29hKZqpgI)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bl984k/quickly_access_your_automate_workflow_from/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: (Doubt , Help Needed)How to perform WebScarping in python to move to next page without automation framework ?\n",
      "Text: So this is the website   \n",
      "[https://www.enterprise.com/en/car-rental.html?icid=header.reservations.car.rental-\\_-start.a.res-\\_-ENUS.NULL](https://www.enterprise.com/en/car-rental.html?icid=header.reservations.car.rental-_-start.a.res-_-ENUS.NULL)  \n",
      "in the home page it will ask you to fill the location detail and after clicking on submit it'll navigate to the next page .  \n",
      "\n",
      "\n",
      "and this is the page i wanna scrape details from.  \n",
      "i can't think of any ways to do it without automation frameworks.\n",
      "\n",
      "     from bs4 import BeautifulSoup\n",
      "    import requests\n",
      "    \n",
      "    url = \"https://www.enterprise.com/en/car-rental.html?icid=header.reservations.car.rental-_-start.a.res-_-ENUS.NULL\"\n",
      "    \n",
      "    headers = {\n",
      "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
      "    }\n",
      "    \n",
      "    page = requests.get(url, headers=headers)\n",
      "    soup = BeautifulSoup(page.text, 'html.parser') \n",
      "\n",
      "what should i do ?  \n",
      "or is it possible atleast to do this without automation frameworks ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bl2aoi/doubt_help_neededhow_to_perform_webscarping_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Fun scrape for the weekend?\n",
      "Text: How about Hacker News at [https://news.ycombinator.com/news](https://news.ycombinator.com/news)?  It is rather dense with fields of interest.\n",
      "\n",
      "Or suggest a site here and we could pick the one with the most mentions,\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bklc1s/fun_scrape_for_the_weekend/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Reddit Promoted Posts\n",
      "Text: New to webscraping. I am trying to work on a research project where I need data (posts, comments , upvotes) on Reddit promoted posts. Can I use the reddit API to do that? Is it possible to distinguish between reddit text ads and the freeform ads that have been launched recently? Any help appreciated. I am trying to understand how the user engagement has changed ( for better or for worse) after freeform ads have been introduced.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bkrulx/scraping_reddit_promoted_posts/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Issue with Request Cost, Brightdata example\n",
      "Text: So to send like 5 Million requests in bright data it costs 15k if you are using web unblocker, even more with Resedential Proxy, if 8.5$/Gb and request is 1.2mb.\n",
      "\n",
      "This looks so expensive to me, guys is there a way around this other than doing scraping without using these proxies? I was thinking if there is a way for some Batch Request, where in one request you can send multiple urls, but that implementation has to be at the proxy level. I'm in a scenario where I need to send 5M requests a day.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bk9hrc/issue_with_request_cost_brightdata_example/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: web scrapping with python , how i can extract the info?\n",
      "Text: Hello mate, I'm using python to get some useful info from below website , using the Library beautifulSoup , I want the date (2024-04-07 and 2024-05-16) , it is showing in the html code , but not when i use below code. Can give me some suggestion , how I can extract these two info from the website?   \n",
      "\n",
      "\n",
      "page = requests.get(url)\n",
      "\n",
      "doc = BeautifulSoup(page.text,\"html.parser\")\n",
      "\n",
      "print(doc.prettify())  \n",
      "\n",
      "\n",
      "[https://www.hapag-lloyd.com/en/online-business/schedule/interactive-schedule/interactive-schedule-solution.html?sn=SHANGHAI&sl=CNSHA&sp=200000&en=ROTTERDAM&el=NLRTM&ep=3011&exportHaulage=MH&importHaulage=MH&departureDate=2024-03-21&weeksAfterStart=4&reefer=N](https://www.hapag-lloyd.com/en/online-business/schedule/interactive-schedule/interactive-schedule-solution.html?sn=SHANGHAI&sl=CNSHA&sp=200000&en=ROTTERDAM&el=NLRTM&ep=3011&exportHaulage=MH&importHaulage=MH&departureDate=2024-03-21&weeksAfterStart=4&reefer=N)\n",
      "\n",
      "https://preview.redd.it/nxevzytztrpc1.png?width=1794&format=png&auto=webp&s=f11f524f987baf39f6e4c5f7776e23c06ee0177f\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bkkwwm/web_scrapping_with_python_how_i_can_extract_the/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Simplifying Product Descriptions with OpenAI API \n",
      "Text: \n",
      "\n",
      "\n",
      "I’ll start this by saying I am very much a beginner when it comes to using OpenAi’s API but am very willing to learn as it’s something I enjoy. \n",
      "\n",
      "I’m trying to simplify a large number of product descriptions that can be used to search on eBay. I have toyed with the idea of using OpenAI’s API to do this and applying some fine tuning to really get the results I want. I want to separate all the the description into things like simplified description, brand, colour…etc. The fine tuned API is able to handle most of this in terms of separating the descriptions but cannot simplify the main product description very well. For example:\n",
      "\n",
      "I provide: \n",
      "\n",
      "ASUS VIVOBOOK 15 X1500 256GB SSD LAPTOP IN INDIE BLACK: MODEL NO X1500EA-EJ2670W (WITH BOX & ALL ACCESSORIES). INTEL PENTIUM GOLD 7505 @ 2.00GHZ, 8GB RAM, 15.6\" SCREEN, INTEL UHD GRAPHICS\n",
      "\n",
      "I want to receive something like:\n",
      "  \n",
      "Simplified Product Description: ASUS Vivobook 15 X1500, Colour: Black, Model No: X1500EA-EJ2670W, Accessories: All …etc \n",
      "\n",
      "I believe the API finds it difficult to understand what is needed in the “Simplified Product Description” and what can be used to return a useful search on eBay. In this case, “ASUS Vivobook 15 X1500” is returns useful results as includes necessary details yet not too specific, but how does it know whether it should include other data so it produces E.g: “ASUS Vivobook 15 X1500EA-EJ2670W”. Unless I train the whole API on very similar, specific examples to the one being searched at the time, I believe it will often fail. Really it needs some sort of external, live validation (although preferably not human input as I want to automate this process). \n",
      "\n",
      "I know this could very easily not be possible but I am curious to know other people’s thoughts. \n",
      "\n",
      "Also if this is the wrong sub please let me know, thanks in advance to anyone’s advice! \n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bkjeph/simplifying_product_descriptions_with_openai_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to bypass/solve datadome  cloudfare captcha\n",
      "Text: I'm using puppeteer And selenium Still Some sites are detecting and blocking it with captcha\n",
      "Is there any other approach that I was use? \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bkjedm/how_to_bypasssolve_datadome_cloudfare_captcha/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for Indeed Scraper - easy & free\n",
      "Text: Hi I'm new to web scrapping and so does getting jobs from indeed. Wondering if there's a plugin or features I could use to scrape specific jobs from indeed? Import it to the WordPress website  \n",
      "\n",
      "\n",
      "*Tried / Test:*  \n",
      "Feedzy Rss (but indeed does not support it)  \n",
      "Apify Indeed Scraper (It works via XML but I'm not sure about the duplication + storage since I'm using the free one)   \n",
      "Jobspikr (works but expensive my boss does not want to subscribe)   \n",
      "\n",
      "\n",
      "like auto scrape jobs from indeed + no need to do it manually T\\_T + free as much    \n",
      "\\~ thanks I'm dumb and not sure what other option and resources\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bkjb1q/looking_for_indeed_scraper_easy_free/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: New to \"Webscraping\" - looking for recommendations\n",
      "Text: Hi all! I am wondering if you can all help with this little issue I have. I am trying to find a \"web scraping\" tool that works for finding smaller news report companies' stories. I also need to find them within a date range (IE, stories of a specific topic published within 72 hours of performing the search). Whenever I try to use a scraping tool online, it doesn't let me specify the date range. As a result, I am getting news stories that are over a year old in some cases. I have tried Scrape-IT, SERPAPI, and a few others, but I am not having any luck specifying the time period.\n",
      "\n",
      "Do you have any recommendations? I am trying to find news stories that are not easily found with a simple Google search and sort by date. I just feel like I am hitting a wall with it. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bkbtjm/new_to_webscraping_looking_for_recommendations/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: [Discussion] ISP Proxies vs Residential. Help me understand what to choose?\n",
      "Text: Trying to learn the ropes and understand some of the nuances of proxy products for large scraping projects and enterprise deployments. For adversarially scraping hundreds of thousands of website pages, are there any major differences if one uses ISP proxies vs residential? Also who's hands-down the best solution for serious scraping projects? Thinking about using bright data -- any thoughts on this one?\n",
      "\n",
      "TY so much.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bj2qr9/discussion_isp_proxies_vs_residential_help_me/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: X/Twitter scraping\n",
      "Text: Hey everyone!  So, I've been diving into scraping X lately, and it's been a blast!  However, I've hit a bit of a roadblock when it comes to fetching video URLs.  I've managed to scrape text, images, GIFs, etc, but for some reason, those video URLs are playing hard to get! It seems like X might be using a third-party service, and even digging into the source code didn't reveal much. 😅 So, if anyone has any bright ideas on how I can find those video URLs, I'd be incredibly grateful! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bjbyye/xtwitter_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How are rotating proxies not suspicious to websites?\n",
      "Text: Suppose I am trying to scrape a popular, large social media website, website A, without a logged in account. \n",
      "\n",
      "If I use a single IP, after say 100 pages, I will get blocked or backed off.\n",
      "\n",
      "If I use a rotating proxy, it appears I am scraping the 100 pages with 100 different IPs...however, aren't there like 100 other people also using those same IPs (which is why it is called rotating...you use it, then you hand it off to the next user)? In which case, from the website's perspective, the rotating IP you are using in any given instance has already tried to browse 100 other pages (due to other users using that IP for the same website).\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Sure, my numbers of IPs and other users of the IPs may be a bit off, but in theory isn't it still possible that the rotating IP service has many other users trying to use their rotating IPs to scrape the same popular, large social media sites?\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bivjja/how_are_rotating_proxies_not_suspicious_to/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: CPU/Threads during the scraping process.\n",
      "Text: Hello,  \n",
      "I am a junior developer and have a question about performance in scraping. I noticed that optimizing the script for software, for example, scraping Google and inserting data into PostgreSQL, is not very effective. Regardless of what I use for process management, such as pm2 or systemd, and how many processes I run, the best results come when I set up a similar number of instances of the script as threads on the server processor, correct? I have conducted tests using various configurations, including PostgreSQL with pgBouncer, and the main factor seems to be CPU threads, correct? One approach to optimization is to use a more powerful server or multiple servers, correct?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1biq504/cputhreads_during_the_scraping_process/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trying to find certain API endpoint\n",
      "Text: Hello! I'm currently working on extracting real estate information from the specific website provided: [https://www.njuskalo.hr/prodaja-stanova](https://www.njuskalo.hr/prodaja-stanova). This website contains numerous listings. Upon selecting a listing (e.g., [https://www.njuskalo.hr/nekretnine/srima-novogradnja-luksuzni-3s-db-stan-oglas-41986145](https://www.njuskalo.hr/nekretnine/srima-novogradnja-luksuzni-3s-db-stan-oglas-41986145)), it directs us to details about the property.\n",
      "\n",
      "My objective is to retrieve basic details such as price, location, and living area size through API requests. I am curious if there are other viable approaches to obtain this information or if discovering this specific API endpoint is feasible.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bih0z9/trying_to_find_certain_api_endpoint/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How would I go about scraping a Bluestacks chat App?\n",
      "Text: I have no experience in scraping or coding but would like to figure out a way to scrape a chat app for a certain phrase, and then get the tool to notify me. It's a simple chat app so I thought there would be pretty easy software that you could run natively on your PC, there is no website attached so it has to scrape the screen in some way or another. Point us in the right direction and ill figure it out from there cheers.   \n",
      "\n",
      "\n",
      "If not would a tool that takes a screenshot every 10 seconds and reads text be a viable option?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bie3w4/how_would_i_go_about_scraping_a_bluestacks_chat/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ebay just changed their markup in a drastic way\n",
      "Text: the shipping details table is gone, and now they are hiding the additional information in a script tag that only gets added to the dom if you click 'more details'\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Has anyone already seen this?\n",
      "\n",
      "Does anyone have any information  about it?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bi198z/ebay_just_changed_their_markup_in_a_drastic_way/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Election result in Russia, bypass scraping protections\n",
      "Text: Im new in webscraping and have experience with simple protections only (eg request timings) so I need help in solving some more advance protections.\n",
      "\n",
      "I wanted to scrap data from election department site and faced next problems:\n",
      "\n",
      "1. The obvious one is captcha protection. I heard about services that change ip address on every failed request but didnt managed to fined a free one.\n",
      "2. All numeric values are presented in page code as a set of chars (I saw letters and numbers but probably it can use symbols as well) that are replaced by specific font to display numbers (eg \"eA9\" is visually presented as \"125\", check Image 1 for real example). I tried to make a decoding table but it helped only for a few sections since different sections use different replacement fonts.\n",
      "3. The site has regional restrictions.  It's not a problem for me rn since I am in Russia but I am moving to other country in a few days. Probably russian vpn could help, so I dont think it's a big problem.\n",
      "4. You need to click item to get sub-items in side menu (check Image 2) and the number of sub-levels is inconsistent and varies between 1 and 3. I need the deepest level to get result table for every election point  (location? i dunno how to name it).\n",
      "\n",
      "https://preview.redd.it/4ax2xby1i8pc1.png?width=1204&format=png&auto=webp&s=2a856c75c8ed27f44d8a474d719a4f7052b38234\n",
      "\n",
      "[Navigation structure](https://preview.redd.it/9a8hzs0jh8pc1.png?width=1100&format=png&auto=webp&s=6351e49863d47db897c66b7ae0a9c887f83a0dc4)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bidlsp/election_result_in_russia_bypass_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraper job management / alert tool for novice\n",
      "Text: Hi all\n",
      "\n",
      "I'm grabbing about 500k rows per day of ecomm product data for a project. Scraper runs every night. \n",
      "\n",
      "**Issue:** Sometimes encountering issues where the target sites are under maintenance etc. and then I end up with missing data which I don't realize until I query the db.\n",
      "\n",
      "**Question:** What would be the simplest job management / alerts tool for a novice? Would like to auto-retry failed requests. Send me alerts on issues. etc. \n",
      "\n",
      "Currently this runs as basic system: python requests, cron jobs, writing data to postgres table on Linode.\n",
      "\n",
      "Appreciate any suggestions. I don't mind paying a few dollars of saas fees for something if it would save me a lot of code. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bhzggg/scraper_job_management_alert_tool_for_novice/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Protected pages?\n",
      "Text: Hello,\n",
      "\n",
      "I wonder if most of you scrape public web pages only.  Is it OK if a page is behind a user ID?  Does that mean that the content is more protected or something?  I dunno if the owner of thet user ID will get in hot water.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1biafh0/protected_pages/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Easy way of getting request headers?\n",
      "Text: So i thought this would be easy or easier than what i am encountering. I was trying to use SeleniumBase in UC mode and i thought i would be able to get the request headers ether by just asking the driver object for them, that didnt work or by running javascript, that also didnt work. Well i thought i would then use Selenium Wire, well its not compatible with SeleniumBase in UC mode.   \n",
      "\n",
      "\n",
      "i Just need to get the headers for one request from the browser so that i could copy them and then use requests to mimic the browser for the rest of my scraping. Maybe there is some other way i am unaware of but i am between using MITM proxy to get them or writing a custom chrome extension. I feel like both of those options are a little overkill.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bhzknt/easy_way_of_getting_request_headers/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: News scraping\n",
      "Text: Hello, I want to scrape news from other news websites that I would later post on my website. What tool would help me do that?   \n",
      "\n",
      "\n",
      "Thank you\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bhmcfg/news_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Getting tickers from Interactive Brokers using post-requests\n",
      "Text: Hi All for some reason interactive brokers don't make it easily accessible to get tickers from their site. I currently get them via their exchange pages using a normal request query. However, this is becoming a bit less reliable. I am trying to implement a imitation of their product search [https://www.interactivebrokers.co.uk/en/trading/products-exchanges.php#/](https://www.interactivebrokers.co.uk/en/trading/products-exchanges.php#/) However, i am having some problems getting it to work as i am newish to this sort of web scrapping. \n",
      "\n",
      "This is my current code \n",
      "\n",
      "url = \"[https://www.interactivebrokers.co.uk/IBSales/servlet/exchange?apiPath=getProductsByFilters](https://www.interactivebrokers.co.uk/IBSales/servlet/exchange?apiPath=getProductsByFilters)\"\n",
      "\n",
      "payload = {\"pageNumber\":1,\"pageSize\":\"100\",\"sortField\":\"symbol\",\"sortDirection\":\"ASC\",\"product\\_country\":\\[\"GB\"\\],\"product\\_symbol\":\"\",\"new\\_product\":\"all\",\"product\\_type\":\\[\"STK\"\\],\"domain\":\"uk\"}\n",
      "\n",
      "headers ={'user-agent': 'Mozilla/5.0 (X11; Linux x86\\_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.28 Safari/537.36'}\n",
      "\n",
      "response = [requests.post](https://requests.post)(url, data=payload, headers=headers)\n",
      "\n",
      "print(response.text)\n",
      "\n",
      "However, it returns the following \n",
      "\n",
      "<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n",
      "\n",
      "<html><head>\n",
      "\n",
      "<title>400 400</title>\n",
      "\n",
      "</head><body>\n",
      "\n",
      "<h1>400</h1>\n",
      "\n",
      "<p>Your browser sent a request that this server could not understand.<br />\n",
      "\n",
      "</p>\n",
      "\n",
      "</body></html>\n",
      "\n",
      "So clearly I am not doing it correctly. I was wondering if anyone could help me make this work.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Cheers.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bhtyf3/getting_tickers_from_interactive_brokers_using/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What's the most efficient way, cost and speed wise, you use to change your IP during extensive web automation?\n",
      "Text: Happy weekend everybody.\n",
      "\n",
      "I currently use rotating 4G proxies from private vendors who actually setup each proxy by hand, works okay for the most part; but pretty damn expensive. I purchase a 4G proxy for about $40/month to $50/month, therefore not a good solution for scalability. 10 proxies is easily \\~$500/month...\n",
      "\n",
      "What do you guys scale with? Let it be a paid or free solution. Anything that scales like charm.\n",
      "\n",
      "I had a conversation with a friend the other day and he told me that he met someone who uses IP spoofing, I have a very broad idea about that. Anyone knows how could that possibly work?\n",
      "\n",
      "Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bh8moe/whats_the_most_efficient_way_cost_and_speed_wise/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Using multiple webcams with selenium.\n",
      "Text: Hello everyone,\n",
      "\n",
      "I'm currently working on automating interactions with a specific website, but I've encountered a challenge due to its requirement for a webcam. Initially, I attempted to bypass this by utilizing the flag `chrome_options.add_argument(f\"--use-file-for-fake-video-capture={path}\")`, which, while effective in general, unfortunately, does not work with this particular site.\n",
      "\n",
      "Subsequently, I explored using pyvirtualcam alongside Unity Capture. However, this approach also proved to be less than ideal. Although it's possible to install multiple devices, streaming to one seems to simultaneously stream to all, a limitation discussed in [this GitHub issue](https://github.com/letmaik/pyvirtualcam/issues/121).\n",
      "\n",
      "Has anyone managed to overcome similar challenges or could offer any advice or insights on this matter? Any suggestions or thoughts would be greatly appreciated.\n",
      "\n",
      "Thank you in advance for your time and assistance!  \n",
      "\n",
      "\n",
      "(I need multiple webcams for multithreading...)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bhpwbv/using_multiple_webcams_with_selenium/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: BrowserForge: Intelligent browser header and fingerprint generator\n",
      "Text: BrowserForge is a smart browser header and fingerprint generator that mimics the frequency if different browsers, operating systems, and devices found in the wild.\n",
      "\n",
      "### Features\n",
      "\n",
      "- Uses a Bayesian generative network to mimic actual web traffic\n",
      "\n",
      "- Extremely fast runtime (0.1-0.2 miliseconds)\n",
      "\n",
      "- Easy and simple for humans to use\n",
      "\n",
      "- Extensive customization options for browsers, operating systems, devices, locales, and HTTP version\n",
      "\n",
      "- Injectors for Playwright and Pyppeteer\n",
      "\n",
      "- Written with type safety\n",
      "\n",
      "**Comparison**: Other popular libraries such as fake-headers do not consider the frequencies of header values in the real world, and are often flagged by bot detectors for unusual traffic.\n",
      "\n",
      "See it here: https://github.com/daijro/browserforge\n",
      "\n",
      "Credit to Apify's nodejs fingerprint-suite for the original logic!\n",
      "\n",
      "Hope you guys find it useful!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bh8bzn/browserforge_intelligent_browser_header_and/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape the scanned PDF data\n",
      "Text: Problem I am facing:  1) how to take text data from the scanned pdf  I have searched libraries like pdf2json, pdf-parse, and pdf-lib but I haven't found any such solution. I can use tesseract ocr here. But node-tesseract requires the image format as input. And I don't find any such package which can convert the pdf to image.  another thing is, I have a PDF URL, not the actual PDFs. So I am just sending get request and getting the buffer data and passing this into pdf parse and I can retrieve the data from the pdf. But the problem happens when it scanned pdf\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bhjvxe/scrape_the_scanned_pdf_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Cloudflare captcha perma loop\n",
      "Text: This simple scraping script worked perfectly a few years ago. Today im trying it again and im getting cloudflare captcha perma loop. Any methods of fixing? Some forums have suggested an undetected version of chromedriver.\n",
      "\n",
      " CODE: [https://pastebin.com/BuSHxGGj](https://pastebin.com/BuSHxGGj) \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bhfl91/cloudflare_captcha_perma_loop/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Facebook scraping. Impossible?\n",
      "Text: Hi,\n",
      "\n",
      "Been trying to scrape facebook events. However it is as if facebook changes where the elements on the page is located. Sometimes I manage to go further in my program, sometimes it crashes right at the beginning since it can't find the element I'm looking for.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "For example, this is the xpath's for the same button on an event page (see more button for the description): \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "//\\*\\[@id=\"mount\\_0\\_0\\_kc\"\\]/div/div\\[1\\]/div/div\\[3\\]/div/div/div\\[1\\]/div\\[1\\]/div\\[2\\]/div/div/div/div/div\\[1\\]/div/div/div/div\\[7\\]/div/span/div\\[2\\]/div\n",
      "\n",
      "//\\*\\[@id=\"mount\\_0\\_0\\_qp\"\\]/div/div\\[1\\]/div/div\\[3\\]/div/div/div\\[1\\]/div\\[1\\]/div\\[2\\]/div/div/div/div/div\\[1\\]/div/div/div/div\\[7\\]/div/span/div\\[3\\]/div\n",
      "\n",
      "//\\*\\[@id=\"mount\\_0\\_0\\_ax\"\\]/div/div\\[1\\]/div/div\\[3\\]/div/div/div\\[1\\]/div\\[1\\]/div\\[2\\]/div/div/div/div/div\\[1\\]/div/div/div/div\\[7\\]/div/span/div/div\n",
      "\n",
      "//\\*\\[@id=\"mount\\_0\\_0\\_fJ\"\\]/div/div\\[1\\]/div/div\\[3\\]/div/div/div\\[1\\]/div\\[1\\]/div\\[2\\]/div/div/div/div/div\\[1\\]/div/div/div/div\\[6\\]/div/span/div\\[2\\]/div\n",
      "\n",
      "//\\*\\[@id=\"mount\\_0\\_0\\_nU\"\\]/div/div\\[1\\]/div/div\\[3\\]/div/div/div\\[1\\]/div\\[1\\]/div\\[2\\]/div/div/div/div/div\\[1\\]/div/div/div/div\\[6\\]/div/span/div\\[3\\]/div\n",
      "\n",
      "//\\*\\[@id=\"mount\\_0\\_0\\_/i\"\\]/div/div\\[1\\]/div/div\\[3\\]/div/div/div\\[1\\]/div\\[1\\]/div\\[2\\]/div/div/div/div/div\\[1\\]/div/div/div/div\\[6\\]/div/span/div\\[3\\]/div\n",
      "\n",
      "//\\*\\[@id=\"mount\\_0\\_0\\_Sp\"\\]/div/div\\[1\\]/div/div\\[3\\]/div/div/div\\[1\\]/div\\[1\\]/div\\[2\\]/div/div/div/div/div\\[1\\]/div/div/div/div\\[6\\]/div/span/div\\[3\\]/div\n",
      "\n",
      "//\\*\\[@id=\"mount\\_0\\_0\\_BX\"\\]/div/div\\[1\\]/div/div\\[3\\]/div/div/div\\[1\\]/div\\[1\\]/div\\[2\\]/div/div/div/div/div\\[1\\]/div/div/div/div\\[7\\]/div/span/div\\[3\\]/div\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "As you can see there is some differance.\n",
      "\n",
      "The xpaths were extracted from by just checking manually. I'm using java selenium and chromedriver.\n",
      "\n",
      "Is there anyway I can get around this? I have not tried using proxies or going headless and stuff like that, but I doubt that would change anything? What are my alternatives?\n",
      "\n",
      "FYI: I'm trying to scrape events data, like location, description, host, time, title. I get the links for each event by going to a [https://www.facebook.com/\"hostname\"/upcoming\\_hosted\\_events](https://www.facebook.com/\"hostname\"/upcoming_hosted_events). and then getting all the links from there, which always work. However it is when I then go to the seperate links that I run into issues.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Extremely thankful for any help!!  \n",
      "\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bgsyhp/facebook_scraping_impossible/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help me build a Viber scrape/bot\n",
      "Text: Would anyone be able to advise me what would be the simplest way to build a bit of automation on top of \"Viber\" OS level app?   \n",
      "\n",
      "\n",
      "Basically, i am attempting to build a \"bot / script\" that would automatically respond in a Viber group chat (as me) if someone posts a \"key phrase\" as part of their message.   \n",
      "\n",
      "\n",
      "Viber's REST API seems to be out of the question for this. No Web UI either which means WEB automation tools like Puppeteer etc. go out the window.  \n",
      "\n",
      "\n",
      "I am taking a look at Wireshark now but I suspect network packet \"sniffing\" is also not gonna work since the packets will be encrypted...? I was hopping i would trigger a script based on parsed packet data.  \n",
      "\n",
      "\n",
      "Looks like i need something like a  GUI automation tool?  \n",
      "\n",
      "\n",
      "Any pointers in the right direction are welcomed! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bh53vm/help_me_build_a_viber_scrapebot/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: “Intelligent automated clicking” exist?\n",
      "Text: Have any tools begun bringing AI into the world of web scraping and automaton yet?  \n",
      "\n",
      "\n",
      "So I can say something like “look for all accounts that responded to this post and click on their profile. If the age is less than 6 months old, scrape all the posts they made” Without worrying about the page structure and how it can change over time. \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bgzzdk/intelligent_automated_clicking_exist/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Adding a pause time in puppeteer-cluster\n",
      "Text: I have been running a scraper to save a webpage of a product information as a pdf locally, there were a total of 2597 pages and each page has a total of 20 links to be converted into pdf. So currently my script is able to get all 20 web pages converted into pdfs, but it takes time so I used puppeteer-cluster. The problem now is that if I increased the number of concurrencies to more than 3 there are a lot of timeouts and missing results. Any idea on how can you insert a pause time after each cluster run to not overload the server??\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bgt1wb/adding_a_pause_time_in_puppeteercluster/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Fastest web scraping technique?\n",
      "Text: I am trying to build an open-source alternative to Perplexity but that needs me to scrape a lot of websites. Sometimes it’s slow and other times my IP gets blocked. I tried puppeteer and running it on Vercel serverless functions but it’s slow depending on the website.\n",
      "\n",
      "For my IP blocking I am trying Brighton data to not only scrape but allow proxies. Unfortunately it’s even slower. I mean double the time. I really need help please.\n",
      "\n",
      "\n",
      "What should I do? I am trying to build most of it myself so what am I missing? Should I deploy a server only for scraping all the time?\n",
      "\n",
      "HELP!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bg68ds/fastest_web_scraping_technique/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape for fun\n",
      "Text: Hi,\n",
      "\n",
      "I am going to scrape the book War and Peace and do a word frequency analysis as a fun weekend project.\n",
      "\n",
      "I will post an update when I finish.  The book is available here where chapters are split into separate url's.\n",
      "\n",
      "[https://americanliterature.com/author/leo-tolstoy/book/war-and-peace/summary](https://americanliterature.com/author/leo-tolstoy/book/war-and-peace/summary)\n",
      "\n",
      "EDIT:\n",
      "\n",
      "Here is a quick run-down of the steps and nodes used to accomplish this task:\n",
      "\n",
      "[https://www.reddit.com/r/saitology/comments/1bgojn7/weekend\\_fun\\_project](https://www.reddit.com/r/saitology/comments/1bgojn7/weekend_fun_project)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bga01v/scrape_for_fun/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping in no GUI env using selenium headless browser\n",
      "Text:  Currently testing my project in an environment without GUI,it is written in python in order to scrap data from facebook marketplace using selenium package and headless browser, link to the project: [https://github.com/lokman-sassi/FMP-Scraper-with-Selenium](https://github.com/lokman-sassi/FMP-Scraper-with-Selenium) , for that I'm using ubuntu 22.04 as subsystem in windows (only terminal).\n",
      "\n",
      " The problem is, when i read a documentation about selenium, it says that i don't need the browser installed at all on my computer to work with, he will use only the driver of the browser, but i was surprised that while executing my file in ubuntu, he returned to me an error saying that i don't have chrome installed ! which is contrary to the documentation, how can i fix that issue, cause i want to scrap without the need to the browser installed on my computer\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bgjb2z/scraping_in_no_gui_env_using_selenium_headless/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: FlowQ: Your Friendly Neighbourhood Distributed Computing System!\n",
      "Text:  Data Extraction can be a headache, when we are dealing with large amounts of data sources. I have created FlowQ to tackle that problem, providing an Simple and completely Free API to accelerate Data Extraction Tasks!\n",
      "\n",
      "FlowQ is a **Distributed Computing system** API, Which aims to make Distributed Computing **Free and Simple to use!**\n",
      "\n",
      "# Features of FlowQ:\n",
      "\n",
      "* **Effortless Setup**: Ditch the complicated configurations! FlowQ runs right out of the box, no ssh headaches or pre-installation required.\n",
      "* **Simple and Secure Connection**: Leverages the Hack.Chat and FileBin platforms to establish secure, base-64 encrypted and anonymous connections with your computing cluster.\n",
      "* **Parallel task execution**: FlowQ maximizes your network by executing tasks in parallel across your machines(with multi-threading), significantly boosting your processing speed.\n",
      "\n",
      "## You can add computers to your cluster just running by:\n",
      "\n",
      "    pip install FlowQ\n",
      "    python -m FlowQ.cluster -c <your-channel-name>\n",
      "\n",
      "* Do not worry, if you dont have any extra computers!, you can just run the above code in a **Google Colab Instances**, to add computers in your cluster!\n",
      "\n",
      "# What my project does:\n",
      "\n",
      "Helps to complete tasks which are time and memory intensive, faster by splitting the work between computers. It can speedup your program task by X times( X is number of computers in cluster)\n",
      "\n",
      "# Target Audience:\n",
      "\n",
      "Anyone, who wants to speedup thier program tasks. Eg: Data Extraction, Data Transformation, or any kind of repetative task(Especially for Data Extraction tasks from Web) :)\n",
      "\n",
      "Github: [FlowQ GitHub Repo](https://github.com/StoneSteel27/FlowQ) (Has more informations!)\n",
      "\n",
      "PyPi page: [https://pypi.org/project/FlowQ/](https://pypi.org/project/FlowQ/)\n",
      "\n",
      "# Some Points I wanted to say:\n",
      "\n",
      "* The project is in **very early development stage**. So, if you get any **issues**, Please report it. It would be a great help!\n",
      "* Your Contributions are **Happily Welcomed!** I would like to see new ideas from people!\n",
      "* Leave feedbacks on anything you would wanna see it Fixed or Improved! **I would love to read your opinions!**\n",
      "\n",
      "Thanks for Reading :)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bg28r2/flowq_your_friendly_neighbourhood_distributed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do I web scrape and download a file from a website with CloudFlare?\n",
      "Text: Hey guys,\n",
      "\n",
      "So from the research I've done, it should be pretty simple to web scrape and download a file from a website. I'm using FlareSolverr to bypass CloudFlare, and hence have the following script to web scrape and download a file:\n",
      "\n",
      "    def request_page(download_link, stream):\n",
      "        url = \"http://localhost:8191/v1\"\n",
      "    \n",
      "        print(download_link)\n",
      "    \n",
      "        headers = {\n",
      "            \"Content-Type\": \"application/json\",\n",
      "            \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
      "        }\n",
      "    \n",
      "        if stream:\n",
      "            data = {\n",
      "                \"cmd\": \"request.get\",\n",
      "                \"url\": download_link,\n",
      "                \"stream\": True,\n",
      "                \"maxTimeout\": 60000\n",
      "            }\n",
      "        else:\n",
      "            data = {\n",
      "                \"cmd\": \"request.get\",\n",
      "                \"url\": download_link,\n",
      "                \"maxTimeout\": 60000\n",
      "            }\n",
      "    \n",
      "        response = requests.post(url, headers=headers, json=data)\n",
      "    \n",
      "        return response\n",
      "    \n",
      "    r = request_page(link, False)\n",
      "    \n",
      "    print(r.text)\n",
      "    \n",
      "    #response = requests.get(download_link, headers=headers)\n",
      "    if r.status_code == 200:\n",
      "        print('Success')\n",
      "    else:\n",
      "        print(\"Something went wrong\")\n",
      "        print(r.status_code)\n",
      "    \n",
      "    final_url = r.url\n",
      "    \n",
      "    print(final_url)\n",
      "    \n",
      "    # Gets the filename (everything after the last trailing /)\n",
      "    filename = final_url.rsplit('/', 1)[-1]\n",
      "    \n",
      "    # Sends a HTTP request to the specified URL and save the response from server in a response object called r\n",
      "    r = request_page(final_url, True)\n",
      "    \n",
      "    # Checks if the request is successful\n",
      "    if r.status_code == 200:\n",
      "        # Gets the Content-Length from the metadata from final_url\n",
      "        filesize = int(r.headers['Content-Length']) / 1024 / 1024\n",
      "    \n",
      "        # Tell user we are downloading filesize\n",
      "        print(f\"Starting {filename}: {filesize} MB.\")\n",
      "    \n",
      "        # Downloads the file to the directory the user enters\n",
      "        with open(filename, 'wb') as f:\n",
      "            for chunk in r.iter_content(chunk_size=1024): \n",
      "                if chunk:\n",
      "                    f.write(chunk)\n",
      "    \n",
      "        # Tell user the current status and file information\n",
      "        print(f\"Completed {filename}: {filesize} MB.\")\n",
      "    else:\n",
      "        print(\"Failed to get the file.\")\n",
      "\n",
      "The variable \\`final\\_url\\` gives back \"[http://localhost:8191/](http://localhost:8191/)\", so it's not working properly.\n",
      "\n",
      "I'm new to web scraping and am not really familiar with the theory behind it, so could someone tell me why this script doesn't work and \\`response = requests.get(download\\_link, headers=headers)\\` does (supposedly)?\n",
      "\n",
      "Many thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bgc69h/how_do_i_web_scrape_and_download_a_file_from_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: [Newbie question] Sticky vs rotating proxies. What's best for web scraping?\n",
      "Text: I've just starting playing around with scraping for a side project and and I'm currently wrapping my feeble mind around best practices.  \n",
      "\n",
      "For someone who needs to scrape the same pool of websites on a daily basis over a long period of time, are there any benefits of having a ton of high quality residential sticky proxies vs run-of-the mill rotating ones?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bfo1qi/newbie_question_sticky_vs_rotating_proxies_whats/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Status 202 while scrapping Redfin html\n",
      "Text: Using requests, and bs4  in python, I iterate through a list of redfin urls of listing to gather some text information from their html. However, after the 6th url, I start getting html status 202, and despite having relatively long timeouts (>30s) between iterations and exponential timeouts between re-attempts, I cannot get pass the status 202. Any ideas how to overcome that?  \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bg4yrk/status_202_while_scrapping_redfin_html/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping only some iterations in aws ec2 instance\n",
      "Text: I need assistance with deploying my Python web scraping code on an AWS EC2 Linux instance. Despite running it in headless mode on the server (headless=new), it's not consistently scraping data for some iterations.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bg2k9z/scraping_only_some_iterations_in_aws_ec2_instance/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I need to find a way to scrape plain text from over 1,000 URLs (mix of PDF and standard web pages) for work, and am feeling completely in over my head\n",
      "Text: I made the mistake of giving the people I work with the impression that this is something I'm capable of, and I'm kicking myself for it. I have a database of over 1,000 URLs that consist of standard web pages and PDF files hosted on the web. I need to find a way to scrape the plain text from these URLs, so I can analyze the data using one of the NLP libraries available in Python (like NLTK). \n",
      "\n",
      "I've been using GPT 4 to generate scripts for me, with only marginal success. GPT generates a script for me, I test it out, I report back to GPT with the results as well as any error messages I received while running it, I ask GPT to refine/modify/fix the script, I run it again, and then rinse and repeat. I've started from scratch three times now, because I keep running into dead ends. I've used scripts that are supposed to process URL lists stored in a .txt file, scripts for processing URLs in a .csv file, and scripts for processing URLS in an .xlsx file.\n",
      "\n",
      "I haven't been able to successfully scrape text from a single PDF. I've been able to scrape text from some of the web pages, but not the majority of them, and only with a bunch of superfluous text included (headers, footers, nav bar, sidebar, menus, etc.). \n",
      "\n",
      "Instead of going back to the drawing board again, I figured I'd ask around here, first. Is what I'm looking to do even feasible? I have no programming experience, hence why I'm using GPT to generate scripts for me. Are there any pre-built tools available that would offer a creative or roundabout way of extracting text from a large collection of URLs?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bfstno/i_need_to_find_a_way_to_scrape_plain_text_from/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: [Newbie question] I have 20,000+ URLs. What is the best approach to get website content dump of all these urls and their key navigation pages? Thanks in advance\n",
      "Text: Normal scraping as far as I understand does not work in this case. Because I can't create site map for each - I am not looking for to as well. I just want the full website dump with all the key internal navigation links. Any help appreciated.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bfyegp/newbie_question_i_have_20000_urls_what_is_the/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Project for resume \n",
      "Text: Hi guys, i’vs been learning some web scraping recently and finish a project. My question is, should I put the project in my resume since I feel like web scraping could be idk frown upon by big companies since they try so hard to prevent it. It’s probably a stupid question but please be kind if you can. Also, i probably won’t upload the dataset I did on the github that show my code cuz you never know. Is that ok?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bfwy0x/project_for_resume/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: yfinance is not working? Or what am I doing wrong? Can't get the desired output\n",
      "Text: \n",
      "URL: https://i.redd.it/5do7t4499ioc1.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Playwright : How can I select and click inside an iframe without timeout exceeding?\n",
      "Text: Im scraping a page that shows a captcha which I want to select and click. The first problem is that i always get errors as page.waitForSelector: Timeout 30000ms exceeded  \n",
      "How can I make it wait longer? or I get error Target page, context or browser has been closed  \n",
      "before the click event hapens.\n",
      "\n",
      "Also if that captcha is not shown (lets say only half the time) but the actual webpage, because Im waiting for the captcha, it will exceed the time and the rest of the code wont be run. How can I continue with the code if there is no captcha shown?\n",
      "\n",
      "`const getHtmlPlaywright = () => {`\n",
      "\n",
      "`const browser = await {chromium}[\"chromium\"].launch({chromiumSandbox: false})`  \n",
      "`const context = await browser.newContext();   const page = await`  \n",
      "`context.newPage()   await page.goto(\"www.example.com\");                const`  \n",
      "`captchaFrame = await page.waitForSelector(\"iframe[title='reCAPTCHA']\");`  \n",
      "`const captchaFrameContent = await captchaFrame.contentFrame();   const`  \n",
      "`captchaCheckbox = await captchaFrameContent.waitForSelector(\"#g-`  \n",
      "`recaptcha-response\");   const formSubmit = await`  \n",
      "`page.waitForSelector(\"#recaptcha-form\");   await captchaCheckbox.click();`  \n",
      "`await formSubmit.click();    const html = await page.content();   await`  \n",
      "`context.close();`\n",
      "\n",
      "`await browser.close();`\n",
      "\n",
      "`return html;`\n",
      "\n",
      "`}`\n",
      "\n",
      "Im trying to access an iframe inside a form and click it.\n",
      "\n",
      "\n",
      "\n",
      "    <form id=\"recaptcha-form\" action=\"...\" method=\"post\" onsubmit=\"validateForm(event)\">      \n",
      "       <div class=\"g-recaptcha\" data-callback=\"onCaptchaSubmit\"> \n",
      "              <div> \n",
      "                 <div> \n",
      "                    <iframe title=\"reCAPTCHA\" ... src=\"...\"></iframe> \n",
      "                 </div> \n",
      "                 <textarea id=\"g-recaptcha-response\" class=\"g-recaptcha-response\" \n",
      "                       style=\"..., display: none;\">\n",
      "                 </textarea> \n",
      "               </div> \n",
      "               <iframe style=\"display: none;\"></iframe> \n",
      "      </div> \n",
      "      <input type=\"submit\" class=\"chip\" value=\"Continue\" disabled=\"disabled\" />  \n",
      "    </form>\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bfe8um/playwright_how_can_i_select_and_click_inside_an/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Another win for web scraping: Court rules against Air Canada's baseless claims\n",
      "Text: Ian from [seats.aero](https://seats.aero) shared an update on Discord today:\n",
      "\n",
      ">Today the court ruled strongly in our favor against Air Canada's motion for a preliminary injunction. Air Canada attempted to prevent us from supporting Aeroplan and retrieving availability from Aeroplan while the case is ongoing. This attempt was denied in full by the judge, allowing us to continue to support Aeroplan while the case is ongoing.  \n",
      ">  \n",
      ">[Seats.aero](https://Seats.aero) only retrieves public mileage fares from public websites, and we are confident in our legal position. This is a big victory for us and we will continue to try and resolve this case any way we can. Thanks for your support!\n",
      "\n",
      "This is a win for web scraping, and a win for all the people whose lives are improved when they are free to access public data in a manner of their choosing.\n",
      "\n",
      "The web has become increasingly hostile towards users. Companies vie to consume more of your time with 'sticky' apps and engagement traps, meticulously tracked via analytics tools. They employ numerous deceptive dark patterns to extract your money and encourage more spending. Platforms leveraging web scraping in innovative ways to address genuine issues will always remain central in combating these contentious practices.\n",
      "\n",
      "Detailed documents are available here [https://www.courtlistener.com/docket/67896748/air-canada-v-localhost-llc/?filed\\_after=&filed\\_before=&entry\\_gte=&entry\\_lte=&order\\_by=desc](https://www.courtlistener.com/docket/67896748/air-canada-v-localhost-llc/?filed_after=&filed_before=&entry_gte=&entry_lte=&order_by=desc)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bez33q/another_win_for_web_scraping_court_rules_against/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: why do you need proxies while scraping ?\n",
      "Text: I am new to web scraping and I cam across HTTP proxies and I can't get my  head around why do we need to use it \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bf71jv/why_do_you_need_proxies_while_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I forked pyppeteer and revamped it into a new browser automation package: mokr.\n",
      "Text: \n",
      "URL: /r/Python/comments/1beun0u/i_forked_pyppeteer_and_revamped_it_into_a_new/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Using Playwright chromium to get the tweet content via URL. Any idea what is the work around ?\n",
      "Text: &#x200B;\n",
      "\n",
      "https://preview.redd.it/tpklv69zjcoc1.png?width=2048&format=png&auto=webp&s=b1cd8f88ba5eb64549b3a215749b7ac7df39b28b\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1besx8y/using_playwright_chromium_to_get_the_tweet/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Find all posts from one specific word in Facebook\n",
      "Text: Hello, I am student from university and I need to create a program to find all recent post in Facebook about my school. I tried with two programs than I found in GitHub and one program than i made with ChatGPT but didnt worked.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Can anyone help me find a tutorial that is trustworthy and helps me solve my problem.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bf065b/find_all_posts_from_one_specific_word_in_facebook/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Receiving 406 from Website via Fastly CDN\n",
      "Text: Hi,\n",
      "\n",
      "im scraping a website that is served via Fastly CDN. Before I could easily scrape, but now they added some measures.\n",
      "\n",
      "I use the headers from my safari browser via the aiohttp library in python. Unfortunately I permanently receive a 406 error without any content.\n",
      "\n",
      "    request_headers = {\n",
      "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
      "                    \"Accept-Encoding\": \"gzip, deflate\",\n",
      "                    \"Accept-Language\": \"en-GB,en;q=0.9\",\n",
      "                    \"Cache-Control\": \"no-cache\",\n",
      "                    \"Connection\": \"keep-alive\",\n",
      "                    \"Host\": \"{uri.netloc}\".format(uri=urlparse(scrapeable_url.url)),\n",
      "                    \"Pragma\": \"no-cache\",\n",
      "                    \"Referer\": \"http://www.google.de/\",\n",
      "                    \"Sec-Fetch-Dest\": \"document\",\n",
      "                    \"Sec-Fetch-Mode\": \"navigate\",\n",
      "                    \"Sec-Fetch-Site\": \"same-origin\",\n",
      "                    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15\",\n",
      "                }\n",
      "\n",
      "Any idea, why I can easily scrape via playwright (expensive and slow), but not with aiohttp using the same headers.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bemccq/receiving_406_from_website_via_fastly_cdn/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Struggling to map out the architecture of my webscraping app (Python multithreading/multiprocessing questions)\n",
      "Text: Hi, I'm new to webscraping. I want to scrape about 10,000 site pages/API endpoints about once every 5 seconds 24/7. I then want to do some processing based on the results and update values in DB. I plan on hosting this on EC2.\n",
      "\n",
      "I could conceivably do this using two queues. The web scraping part will take urls from a URL queue and add the results to the URL data queue. Then workers consume from the URL data queue to process the data and write to DB, then re-add the URL back to the URL queue. Basically a circle of never ending data processing. Can also implement limiting the flow as it will probably be less than 5 seconds for a URL to complete the loop.\n",
      "\n",
      "I want to avoid as much latency as possible between fetching the data and processing the data. I was thinking of doing all of this in a single python program to avoid overhead like serializing/deserializing when passing data around. I'm fairly confident this will fit in memory on a small/medium sized EC2 instance but maybe I'm wrong. Does this approach seem reasonable? Or is the latency really not a big deal from using stuff like RabbitMQ or SQS? \n",
      "\n",
      "I'm not familiar with python's multithreading/multiprocessing. The approach in my head is use one process to fetch web scrape and one to many processes to process the data. The webscraping process asynchronously requests the data (can also add multithreading to this process for speedup). Then use multiprocessing.Queue for my queues to pass data between the web scraping process and the data processing processes. It might even be overkill to do all this, maybe a single process can do everything, but in my head decoupling the webscraping and data processing makes the most sense architecture wise.\n",
      "\n",
      "Another complication is that I need a cache of separate data to reference for some decision making when processing the web scraped data. I probably need to pause all the data processing code while I'm updating the cache separately.\n",
      "\n",
      "Any advice?\n",
      "\n",
      "edit: Unrelated, but my brother who works on ML stuff told me to use an ML library called Ray which also provides general stuff for scaling distributed python apps. They even have an example about [web crawling](https://docs.ray.io/en/latest/ray-core/examples/web-crawler.html). Could be interesting!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1be1psf/struggling_to_map_out_the_architecture_of_my/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape only new data \n",
      "Text: I'm working on extracting data from a website that updates twice daily. Currently, my method captures all the information each time. Is there a method to automatically collect only the newly added data?\n",
      "P.S. I have no coding experience.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1beesyl/scrape_only_new_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I'm trying to scrape this website www.leroymerlin.fr which is heavily protected. Anyone can tell me how can I bypass this? I'm using python and undetected chrome.\n",
      "Text: Thank you all for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1be17vf/im_trying_to_scrape_this_website_wwwleroymerlinfr/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Longshot but...scraping new TikTok videos?\n",
      "Text: Has anyone been able to scrape/list the new videos posted on Tiktok in the past X hours/days?\n",
      "\n",
      "I'm able to scrape the videos of users timelines. Im able to monitor accounts on a consistent basis, but I still miss a ton of videos that are posted and are not popular enough/are not posted by accounts I am monitoring\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1be0g09/longshot_butscraping_new_tiktok_videos/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: let's talk about web scraper salaries\n",
      "Text: </>\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bd36vc/lets_talk_about_web_scraper_salaries/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Has anyone seen this?\n",
      "Text: what does this mean for freelancers and those learning to code?\n",
      "\n",
      "https://x.com/cognition_labs/status/1767548768734294113?s=46\n",
      "URL: https://i.redd.it/h02ss11cxxnc1.jpeg\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trouble downloading MP4s with requests\n",
      "Text: I'm trying to download MP4s off a site that's behind a paywall and login (I've paid). I used python selenium to login and find the the MP4 link, then python requests to write the bytes content to file. My code:\n",
      "```\n",
      "link_href = # Found the mp4 link on the website\n",
      "with open('D:/filename','wb') as f:\n",
      "    response = requests.get(link_href)\n",
      "    f.write(response.content)\n",
      "```\n",
      "Instead of video content, I get the html to their login page. I've used this technique successfully with a different site. I'm guessing it worked because on the other site, the mp4 files were stored somewhere on a public cloud, whereas with this site, the mp4 files are stored on the site itself behind their paywall.  When comparing the URLs, it seems my theory is correct, because the URLs that work look like 'https://somelongrandomstring.mp4' whereas the URLs that don't work look like 'https://sitename/videoname.mp4'. How can I use the requests module to get the video content?\n",
      "\n",
      "As a side question, what would be the legality of me selling access to the public URLs for the other site?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bdcnhj/trouble_downloading_mp4s_with_requests/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Python Web Scraping does not load every link from Google Search\n",
      "Text: Hi,\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I wrote a program which sends a query to google which only filters out linkedin profiles (on that part everything is working) ... now the problem that I have is that I can't get it to work to give me all the links from a certain search request. It only gives me 9 Linkedin links into my Excel file and then moves onto the next search query. So that's that main thing.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Beyond that my code is also missing a time delay to load the new sections ... but when I entered some sort of time.sleep() it crushed the entire thing... \n",
      "\n",
      "My Chrome has autoscrolling enabled. After a certain amount of links (manually scrolling down) however, it gives me some sort of message that only the first 226 results were shown to only \n",
      "\n",
      "Then I had problems with Recaptcha obviously so hope this can be bypassed with some sort of delay functionality.. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "So I can't solve these things. The rest is working, however. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Here is the relevant section of the code:\n",
      "\n",
      " \n",
      "\n",
      "`collected_links = []`  \n",
      "`for index, row in niches_df.iterrows():`  \n",
      " `mutterindustrie = row.iloc[0]`  \n",
      " `for sub_industrie_keywords in row.iloc[1:]:`  \n",
      " `if pd.notna(sub_industrie_keywords):  # Stellt sicher, dass die Zelle nicht leer ist`  \n",
      " `search_query = create_search_query(sub_industrie_keywords, jobtitel, laender[0])`  \n",
      " `encoded_search_query = quote_plus(search_query)`  \n",
      " `final_search_url = basis_url + encoded_search_query`  \n",
      " `driver.get(final_search_url)`  \n",
      " `try:`  \n",
      " `WebDriverWait(driver, 30).until(`  \n",
      " `EC.presence_of_all_elements_located((By.XPATH, \"//a[contains(@href, 'linkedin.com/in/')]\"))`  \n",
      "`)`  \n",
      " `links = driver.find_elements(By.XPATH, \"//a[contains(@href, 'linkedin.com/in/')]\")`  \n",
      " `for link in links:`  \n",
      " `href = link.get_attribute('href')`  \n",
      " `collected_links.append([href, mutterindustrie, sub_industrie_keywords, ', '.join(laender), ', '.join(jobtitel)])`  \n",
      " `except TimeoutException:`  \n",
      " `print(\"Keine weiteren Ergebnisse gefunden oder Timeout erreicht.\")`  \n",
      " `save_to_excel(collected_links, \"LinkedInResults.xlsx\")`  \n",
      "`driver.quit()`\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bd8az6/python_web_scraping_does_not_load_every_link_from/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Assistance with scraping - T&C website concerns\n",
      "Text: I’m looking at scraping a few sites that publicly displays pricing, availability, sqft. of apartments. My question is if I were to do this and essentially create a business out of it, what are the concerns with trying to scrap the information, create a client portal and charge a minimal fee to have all the information in an easy to see dashboard for a client. \n",
      "\n",
      "I’ve read about making sure the robot.txt is good and doesn’t prohibit scraping but I’ve seen dozens of stuff where Zillow, Homes.com and other websites like this are scraped. \n",
      "\n",
      "Any help would be super appreciative. I’m new to this and want to ensure I’m doing things correctly before I dive into creating something. \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bddijt/assistance_with_scraping_tc_website_concerns/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Automated UPS claim submission\n",
      "Text: Whats the best way to go about automating filing claims on ups website?  There's so much pop-ups and I have to submit an invoice and enter a tracking number and fill a lot of fields. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bd40aw/automated_ups_claim_submission/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Legalities of web scraping a site\n",
      "Text: I am very new to web scraping.  I'm curious, how legal is it to scrape a website?\n",
      "\n",
      "There is an ecommerce site I wanted to scrape - the image, the description and prices of a product - and I'm wondering how legal is this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bd6g0a/legalities_of_web_scraping_a_site/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Google Search results (on a small scale)\n",
      "Text: I'm working on a Question Answering system that scrapes the internet for information in real time. It seemed like a pretty simple task, but I'm having trouble with the web-scraping aspect. I'm pretty new to any sort of scraping so I need to get an idea of this - is it a pretty easy task to scrape Google search - like scraping the top 5 links of 10 different search queries? I feel like that's not a huge number, but I'm already having issues and I think they're related to Google blocking bots (basically the scraped links were good at first, and accurate to what was coming up when I searched manually, but then it started returning links only from sites like quora or youtube, which I couldn't get the info I wanted from). Is this something that you need to use an API for or is it pretty easy to work around with some proxy changes and stuff? Is it just Google that's difficult to work around (if it even is, maybe small-scale google search scraping is really basic?) and are other browsers search results easier to scrape?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bcs1pz/scraping_google_search_results_on_a_small_scale/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Wholesale + Products for eCommerce\n",
      "Text: Hey everyone!\n",
      "\n",
      "I have a wholesaler that I need to scrape because they don't have price lists or marketing materials easily laid out in .csv or any usable way for eCommerce.\n",
      "\n",
      "I\"m putting all the brand names behind spoilers cuz of the subreddit rules around linking to paid services. I just need whatever information is relevant I'm not trying to sell anyhtig, **but I am willing to pay for a service that does what I need because** ***I'm not a developer of any kind*** **and just need this to keep going on my business.** \n",
      "\n",
      "**Website to scrape:** >!Magenta !<Wholesaler Website\n",
      "\n",
      "**Info to get:** descriptions, pricing, ***photos (multi photo products are key!),*** *hopefully minimum quantity if it can figure out from order page.*\n",
      "\n",
      "**Tools using:** I'm looking at >!Shopify & Ecwid!< for my site, Scraping w/ >!Apify !<potentially? I need it to login, get the information, not DDOS or otherwise freak out the site owner with requests, and get the information I need. I am open to more laborious ways of doing this. The website itself is set up really poorly from the perspective of monitoring in any way, which is particularly difficult for an ecommerce site using them as a wholeasler triyng to get the product lists, the different categories and subcategories, the minimum quantities, and all the photos to at least get started (Eventually we'll replace with our own original assets but you need to have the default manufacturer ones to look legitimate and to compare across vendors). \n",
      "\n",
      "Anyways, all help is appreciated! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bcqo85/scraping_wholesale_products_for_ecommerce/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping a table, why does it say \"cannot set a row with mismatched columns\", when clearly my headers match with the row's like headers are 4 and rows are also 4\n",
      "Text: &#x200B;\n",
      "\n",
      "https://preview.redd.it/961ykc48nunc1.png?width=1385&format=png&auto=webp&s=17021658a1bf5773165bdfeac469930ba2873b0c\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bcqovq/scraping_a_table_why_does_it_say_cannot_set_a_row/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Instagram Profile Scraper \n",
      "Text: The \"Instagram Profile Scraper\" is a Python script designed to extract multimedia content from Instagram profiles using Selenium and BeautifulSoup libraries. The script provides a user-friendly interface leveraging Tkinter, enabling users to input the target Instagram username, password, profile URL, and desired download directory. It automates the process of logging into Instagram, navigating to the specified profile, and collecting images and videos.\n",
      "\n",
      "Key Features:\n",
      "\n",
      "User-friendly Interface: Implemented with Tkinter, facilitating ease of use and interaction for users.\n",
      "Automated Login: Utilizes Selenium to automate the login process, enhancing efficiency and convenience.\n",
      "Content Extraction: Extracts images and videos from Instagram profiles, ensuring comprehensive content collection.\n",
      "Dynamic Scrolling: Utilizes dynamic scrolling to load additional content, ensuring comprehensive data retrieval.\n",
      "Download Management: Organizes downloaded content into respective image and video directories for easy access and management.\n",
      "Error Handling: Implements robust error handling to gracefully manage exceptions during the scraping process.\n",
      "Password Security: Incorporates password hiding functionality to enhance user privacy and security.\n",
      "Technologies Used:\n",
      "\n",
      "Python: Core programming language for script development.\n",
      "Selenium: Web automation library for browser interaction and navigation.\n",
      "BeautifulSoup: HTML parsing library for extracting data from web pages.\n",
      "Tkinter: GUI toolkit for creating the user interface.\n",
      "Requests: HTTP library for making web requests.\n",
      "JSON: Data interchange format for parsing Instagram API responses.\n",
      "Chrome WebDriver: Web browser automation tool for Selenium.\n",
      "URL: https://youtu.be/IdfFj56P5y4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: 🏝️ We are looking for the perfect scraping tool for travel sites. Help requested!\n",
      "Text: Hi!\n",
      "\n",
      "We are currently developing a portal on the subject of \"Travel and Sport\". For this we need data from various tour operators. The tour operators have a certain pool of trips that is updated once or twice a year. We have to scrape this basic data once and keep it up to date. In total, there are certainly 20 smaller websites that we have to scrape and keep up to date, although - as described - there are not changes every week. In addition, every trip has certain availabilities. These can change daily.\n",
      "\n",
      "To summarize again:\n",
      "\n",
      "We have around 20 websites, each with around 300 trips. We have to scrape this basic data once and then check it for updates once every two weeks. The availability of the trips is only a very small part and must be checked daily for updates.\n",
      "\n",
      "Do you have any recommendations on the best way to do this? We have a developer with Python experience. However, it would be nice if non-coders could also use the tool.\n",
      "\n",
      "A small budget is available for this. If it costs money, it should also be easy to use and not require an insane amount of training.\n",
      "\n",
      "So far, my colleague has only tested Selenium very briefly. As he is the developer, he would certainly be able to cope with it. As a non-developer, I would of course prefer a web tool. We are simply looking for the best compromise.\n",
      "\n",
      "Many thanks for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bbywdi/we_are_looking_for_the_perfect_scraping_tool_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Info From Individual Listing Profiles After Scraping the Listing Site? Help Needed!\n",
      "Text: Hey all, I'm trying to build a workflow and tech stack for list building and need some advice. I'm trying to scrape businesses from Houzz, but am struggling with how to get the right data.\n",
      "\n",
      "Here's the problem, when you scrape Houzz with a tool like Instant Data Scraper or Easy Scraper, you just get the company name + Houzz profile URL. The website URL, company phone number, and address is tucked within each of the Houzz profile URLs, so I need a tool that can visit each scraped Houzz profile URL and extract the data.   \n",
      "\n",
      "\n",
      "So the process would essentially be:\n",
      "\n",
      "1. Scrape listing page for company name + Houzz profile URLs\n",
      "2. Add Houzz profile URLs to workflow to scrape website, phone number, and address\n",
      "\n",
      "Anyone know of a tool that can do this? Or an alternative solution?   \n",
      "\n",
      "\n",
      "I was also planning on using Clay to enrich the data further, but it would be pretty cost using it for this entire process, so I'm hoping to solve at least the basic data needs in a more cost-effective way.   \n",
      "\n",
      "\n",
      "Thanks!   \n",
      " \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bcbsmy/scraping_info_from_individual_listing_profiles/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How did OpenAI scrap the entire Internet for training Chat GPT?\n",
      "Text: Out of curiosity, how did OpenAI *scrape the entire Internet for training ChatGPT?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need Help Bypassing Nike's Akamai\n",
      "Text: Hi! I know basic Selenium. When it gets too complicated then I get lost. I want to build a Bot that can automatically purchase a product. I am using undetected Chromedriver, changed headers, and don´t know what else to do. Is there maybe a standard Selenium script to bypass Akamai specifically? Can anyone help me?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "    options = uc.ChromeOptions()\n",
      "            options.add_argument('--start-maximized')\n",
      "            options.add_argument(\"--no-sandbox\")\n",
      "            options.add_argument(\"--disable-dev-shm-usage\")\n",
      "            options.add_argument('--disable-popup-blocking')\n",
      "            options.add_argument('--disable-extensions')\n",
      "            options.add_argument('--ignore-ssl-errors')\n",
      "            options.add_argument('--ignore-certificate-errors')\n",
      "            options.add_argument('--disable-blink-features=AutomationControlled')\n",
      "            options.add_argument('log-level=3')\n",
      "            options.add_experimental_option('prefs', {'intl.accept_languages': 'en,en_US'})\n",
      "            # options.add_argument(f\"--user-data-dir={user_dir}\")\n",
      "            prefs = {f'profile.default_content_settings.popups': 0,\n",
      "                     \"credentials_enable_service\": False,\n",
      "                     \"profile.password_manager_enabled\": False,\n",
      "                     'useAutomationExtension':False,\n",
      "                     \"excludeSwitches\": [\"enable-automation\"]}\n",
      "            options.add_experimental_option(\"prefs\", prefs)\n",
      "            #options.add_experimental_option('useAutomationExtension', False)\n",
      "            #options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
      "            #options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
      "            #options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
      "            headers = {\n",
      "                    'Referer': 'hhttps://www.google.com/',\n",
      "                    'sec-fetch-site': 'cross-site',\n",
      "                    # Add any other headers as needed\n",
      "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',\n",
      "                    }\n",
      "            options.add_argument(f\"--user-agent={headers['User-Agent']}\")\n",
      "            # if proxy != \"No\":\n",
      "            #     options.add_argument(f\"--proxy-server={proxy}\")\n",
      "            if headless:\n",
      "                options.add_argument('--headless')\n",
      "                options.add_argument(\"--window-size=1920,1080\")\n",
      "                options.add_argument(\"--disable-gpu\")\n",
      "                options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
      "            driver = uc.Chrome(options=options)\n",
      "            driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
      "            return driver\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bbdfil/need_help_bypassing_nikes_akamai/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Shein Products based on sku list\n",
      "Text: Hi guys, anyone knows how can I scrap shein products based on a sku list I have?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bb7vsi/shein_products_based_on_sku_list/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How scrape with the selenium group post?\n",
      "Text: hi,\n",
      "\n",
      "I tried scrape with selenium group post and I have problem with python code.\n",
      "\n",
      ">`posts = self.browser.find_elements(By.XPATH, \"//div[@class='xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs x126k92a')]\")`  \n",
      "`print(posts.count())`\n",
      "\n",
      "This code not [](http://working.Do) you have any idea?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bb6yec/how_scrape_with_the_selenium_group_post/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrap all the messages from a really big whatsapp chat?\n",
      "Text: Hey guys!I need to get all the messages(including audio, images and videos. Stickers if possible) from a really old chat. I´ve searched methods to make this, but only found two ways:  \n",
      "\n",
      "\n",
      "* Suspicious Software.\n",
      "* Extract data from whatsapp backups.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I don't want to download any suspicious software, and extracting data from WA backups requires downgrading to older versions of Whatsapp (I'm afraid of losing data) and I don't think it saves media either. And the last solution for me was to scrap the WA web. But I have some questions:\n",
      "\n",
      "1. Is this possible to make? (pls say yes 🥹)\n",
      "2. Do you guys know any better solution for this?\n",
      "3. Wich is the best lib/software/api for this? (I just used Selenium a couple times)\n",
      "4. By the amount of messages and media, will my web driver crash?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1bb5v6j/how_to_scrap_all_the_messages_from_a_really_big/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I need to scrap 1M+ pages heavily protected (cloudflare, anti bots etc.) with python. Any advice?\n",
      "Text: Hi all,\n",
      "Thank you for your help.\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1babrzc/i_need_to_scrap_1m_pages_heavily_protected/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Hawaii DCCA Disciplinary Actions Scraping Help\n",
      "Text: I have some code here to scape hawaii dcca disciplinary actions. The structure of the html tags are not ideal and about half of the reports are pdf docs, so the approach I thought best was to extract the text from the links and use a regex search to get matches and put them in a data frame. I am able to get respondent, case number, sanction, and effective date, but I am having some trouble collecting the case descriptions. Additionally, I only want records under \"Medical Board\". The current code collects all of the cases on the page/pdf. \n",
      "\n",
      "Any ideas on how to approach this? Relatively new to we scraping so open to any and all tips as well. \n",
      "\n",
      "links = \\[\\]\n",
      "\n",
      "\\#loop though site pages\n",
      "\n",
      "for i in range(0, 26):\n",
      "\n",
      "base\\_url = f'[https://cca.hawaii.gov/oah/category/news-releases/page/{i}/](https://cca.hawaii.gov/oah/category/news-releases/page/{i}/)'\n",
      "\n",
      "page = urlopen(base\\_url)\n",
      "\n",
      "soup = BeautifulSoup(page, 'html.parser')\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\# Find all <h3> \n",
      "\n",
      "h3\\_tags = [soup.select](https://soup.select)('h3 a\\[href\\]')\n",
      "\n",
      "\n",
      "\n",
      "\\# Extract links from <h3><a> tags\n",
      "\n",
      "for a\\_tag in h3\\_tags:\n",
      "\n",
      "link = a\\_tag\\['href'\\]\n",
      "\n",
      "links.append(link)\n",
      "\n",
      "\\# initialize regex pattern \n",
      "\n",
      "pattern = re.compile(r\"Respondent:\\\\s\\*(.\\*?)\\\\s\\*Case Number:\\\\s\\*(.\\*?)\\\\s\\*Sanction:\\\\s\\*(.\\*?)\\\\s\\*Effective Date:\\\\s\\*(\\[\\\\d\\\\s-\\]+)\\\\s\\*\", re.DOTALL)\n",
      "\n",
      "\\#description\\_pattern = re.compile(r'Effective Date:(.\\*?)(?:Respondent:|$)', re.DOTALL)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\#create empy data frame\n",
      "\n",
      "columns = \\['Respondent', 'Case Number', 'Sanction', 'Effective Date'\\]\n",
      "\n",
      "df = pd.DataFrame()\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\#loop through links\n",
      "\n",
      "for link in links: \n",
      "\n",
      "\\#check if pdf\n",
      "\n",
      "if '.pdf' in link: \n",
      "\n",
      "\\#create temperary pdf\n",
      "\n",
      "read = requests.get(link)\n",
      "\n",
      "with open('temp.pdf', 'wb') as pdf\\_file:\n",
      "\n",
      "pdf\\_file.write(read.content)\n",
      "\n",
      "pdf\\_document = [fitz.open](https://fitz.open)('temp.pdf')\n",
      "\n",
      "\\#extract text from document\n",
      "\n",
      "text = \"\"\n",
      "\n",
      "for page\\_num in range(pdf\\_document.page\\_count):\n",
      "\n",
      "page = pdf\\_document\\[page\\_num\\]\n",
      "\n",
      "text += page.get\\_text()\n",
      "\n",
      "text = text.replace('\\\\n', '')\n",
      "\n",
      "\\#find matches and append results to dataframe\n",
      "\n",
      "matches = pattern.findall(text)\n",
      "\n",
      "\\#description\\_matches = description\\_pattern.findall(text)\n",
      "\n",
      "\\#description\\_matches = description\\_matches if description\\_matches else \\[''\\]\n",
      "\n",
      "\\#comment\\_df = pd.DataFrame(description\\_matches)\n",
      "\n",
      "temp\\_df = pd.DataFrame(matches, columns=columns)\n",
      "\n",
      "\\#temp\\_df\\['Description'\\] = comment\\_df\n",
      "\n",
      "df = df.append(temp\\_df)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "else: \n",
      "\n",
      "\\#open html and extract text\n",
      "\n",
      "page = urlopen(link)\n",
      "\n",
      "html\\_bytes = [page.read](https://page.read)()\n",
      "\n",
      "html = html\\_bytes.decode(\"utf-8\")\n",
      "\n",
      "soup = BeautifulSoup(html, 'html.parser')\n",
      "\n",
      "text = soup.get\\_text().replace('\\\\n', '').replace('\\\\xa0', '')\n",
      "\n",
      "\\#find matches and append results to dataframe\n",
      "\n",
      "matches = pattern.findall(text)\n",
      "\n",
      "\\#description\\_matches = description\\_pattern.findall(text)\n",
      "\n",
      "\\#description\\_matches = description\\_matches if description\\_matches else \\[''\\]\n",
      "\n",
      "\\#comment\\_df = pd.DataFrame(description\\_matches)\n",
      "\n",
      "temp\\_df = pd.DataFrame(matches, columns=columns)\n",
      "\n",
      "\\#temp\\_df\\['Description'\\] = comment\\_df\n",
      "\n",
      "df = df.append(temp\\_df)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1baq7p3/hawaii_dcca_disciplinary_actions_scraping_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Passing value with Puppeteer to field not working\n",
      "Text:  \n",
      "\n",
      "I'm trying to pass data to the field with Puppeteer (website: [https://przegladarka-ekw.ms.gov.pl/eukw\\_prz/KsiegiWieczyn/znajnieKW](https://przegladarka-ekw.ms.gov.pl/eukw_prz/KsiegiWieczyn/znajnieKW)) using a scrapper made in Puppeteer, but I failed when entering the value into the first field. I am talking about this:\n",
      "\n",
      "    <input id=\"kodWydzialuInput\" class=\"required\" type=\"text\" maxlength=\"4\" aria-label=\"Lista rozwijana wybierz kod wydziału\" aria-required=\"true\" autocomplete=\"off\">\n",
      "\n",
      "I cant find element with getElement(), cant type with focus()+keybord.type() or page.type().\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1baeczc/passing_value_with_puppeteer_to_field_not_working/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Could you give me some resources?\n",
      "Text: Can you recommend any resources or channels for people new to web scraping? I just wanted some things to consume, like: communities, videos with some tutorials, some good sites for scraping, etc.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b9tc4o/could_you_give_me_some_resources/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is scrapping the latest tweet from multiple Twitter account is possible?\n",
      "Text: I have scrapped twitter few years back. But i heard that since August scrapping X is nearly impossible.\n",
      "\n",
      "My use case is that i have bunch if twitter handles and i want to get thier latest tweet. \n",
      "If i can scrape the last tweet asap thats awesome but i don't mind a bit of delay.\n",
      "\n",
      "So coming to my actual query that which resources should i look to handle this use case. \n",
      "\n",
      "I know python and bit of selenium but i read somewhere that you cant get last post from X. \n",
      "\n",
      "Any lead or direction will be super helpful. Thanks .\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b9yjw3/is_scrapping_the_latest_tweet_from_multiple/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I need help with web drivers.\n",
      "Text: I have recently got into web scraping and need some advice. I have installed selenium and now need to install a web driver (chrome). Problem is I don't know which one to install. Chrome version is 122.0.6261.95 (official build) 64 bit. But I can't find any web driver for it on the chrome web driver site. Would appreciate any advice.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b9u7n1/i_need_help_with_web_drivers/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Which undetected/stealth headless scraper for multi-processing/threading?\n",
      "Text: I am working on a project where I need to scrape product prices in online shops. Still trying to figure out the tech-stack for the project. \n",
      "\n",
      "Requirements:\n",
      "\n",
      "* bypass Cloudflare\n",
      "* render Javascript\n",
      "* run in parallel\n",
      "* run in Docker\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Not really 100% happy with any of the tools I tried so far:\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "undetected-chromedriver:\n",
      "\n",
      "* can run with [multi-processing](https://github.com/ultrafunkamsterdam/undetected-chromedriver/discussions/1063#discussioncomment-5853479), however, doesn't support proxies with authentification... selenium-wire does, but it doesn't work that well with multi-processing\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "hrequests:\n",
      "\n",
      "* doesn't render Javascript correctly in some cases for me\n",
      "* also the library isn't very popular\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "puppeteer-extra / puppeteer-extra-plugin-stealth:\n",
      "\n",
      "* hasn't been updated for over a year\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "ulixee hero:\n",
      "\n",
      "* very early stage of development and not very popular\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Happy for any advice. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b8xsi3/which_undetectedstealth_headless_scraper_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: A Service/Proxy for Bypassing Cloudflare and Captcha stuff\n",
      "Text: Do you know any good proxy that works well in bypassing cloudflare and captcha, a paid service, with IP rotation or Residential Proxy or anything that works?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b908vq/a_serviceproxy_for_bypassing_cloudflare_and/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to search for a term within a Twitter list?\n",
      "Text: \n",
      "\n",
      "I was referring to twitter documentation and it says we need to search on that way to find all tweets from that list:\n",
      "\n",
      "\n",
      "*list:NASA/astronauts-in-space-now* \n",
      "\n",
      " \n",
      "I suppose to find a term within that list we just need to add the term in the command, so I tried that for the term 'earth':  \n",
      "\n",
      "\n",
      "*list:NASA/astronauts-in-space-now earth*  \n",
      "\n",
      "\n",
      "but both search commands didn't work, it keeps showing other contents even from accounts outside the list. For the first command it even showed some porn result.  \n",
      "\n",
      "\n",
      "Did it change? Does anyone know how to do that?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b8xm39/how_to_search_for_a_term_within_a_twitter_list/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Paid project put on back burner..\n",
      "Text: So admittedly I  have only web scraped a handful of times, I normally pull my data from databases and load them in their desired location. In this situation I had a client reach out that wanted data (current and potential customer) pulled from one CRM and placed somewhere else. This would run nightly updating the newest customers, the best way for me to do this was web scrape via python script. I was able to extract the exact data I needed and was working on the second portion of my process, after reaching out they informed me that ownership had gotten an alert on back end work and they were not happy, and there would be extra charges if this continued. This is now on the back burner till further notice. My question, I read term and services and no where did it mention anything about data scraping or requests...Is it best practice to reach out to the company with your intentions? Is this something that the owners should do or myself? I am just trying to see if this is something I can avoid in the future.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b92iys/paid_project_put_on_back_burner/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to hack websites behind WAF, cloudflare, akamai, imperva\n",
      "Text: Report\n",
      "\n",
      "[https://drive.google.com/file/d/1RdssR9XpbQGVSaWtmyvZP\\_jeN7T0CQjN/view](https://drive.google.com/file/d/1RdssR9XpbQGVSaWtmyvZP_jeN7T0CQjN/view)\n",
      "\n",
      "Hello Everyone, I found a way to bypass these WAF systems, they way to bypass them is to get the real IP from the server\n",
      "\n",
      "So this is before:\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "https://preview.redd.it/m12zugrx1qmc1.png?width=1750&format=png&auto=webp&s=805d1b49f7cf6f98f8f9c909a215104d30d21730\n",
      "\n",
      "This is after:  \n",
      "\n",
      "\n",
      "https://preview.redd.it/4umg8vzy1qmc1.png?width=1506&format=png&auto=webp&s=fff446748d0b8dd84c9150db1130847d21a8caf5\n",
      "\n",
      "The fundamentals  to get the real IP is to send HTTP request to every possible IP until the real server responses back.  \n",
      "The full report is here:\n",
      "\n",
      "you will need to have Go installed on your systems, here its is the code:  \n",
      "[https://github.com/johnbalvin/marcopolo/](https://github.com/johnbalvin/marcopolo/)\n",
      "\n",
      "Btw, this is my first time making reports like this , so be kind.  \n",
      "I'm probably not following any good design pattern, also I don't have enogh experience teaching, so probably the videos won't have a good audio, or good teaching practices.  \n",
      "\n",
      "\n",
      "This is not just for \"hacking\" but it's also to create web scrappers using the real IP from the host\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b80ooc/how_to_hack_websites_behind_waf_cloudflare_akamai/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best way to scale up my Playwright-Python webscraper?\n",
      "Text: Hey all\n",
      "\n",
      "I have a Playwright-python script set up that asynchronously runs several headless browsers in parallel on one machine. The input is a csv file containing a list of URLs to crawl on. The URLs need javascript to display properly so scraping a single page takes around 5 seconds each in an ideal situation.\n",
      "\n",
      "The issue is, that I want to scale this approach up to get through more URLs faster. My understanding is that the best approach is to parallelize the work across several instances, but I have never attempted this before and am a bit lost on how to proceed.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I have access to AWS and all its features for reference. Any starting points would be helpful.\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b81r1f/best_way_to_scale_up_my_playwrightpython/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I created an open source tool for extracting data from websites\n",
      "Text: \n",
      "URL: https://v.redd.it/9csshdm98jmc1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape the videos of a private online course?\n",
      "Text: Hello recently, i've bought a 2600$ online course, with different modules and videos with my friend, i paid for about 1000 among it and my friend payed the money left. My friend is the only one with the logins.\n",
      "And he dont want the staff to know that 2 people bought it, and potentially ban us  for it. \n",
      "\n",
      "We would like to scrape the videos in the course and download it then transfer it to my pc in different folders so i can follow it peacefully, without having to lose a grand invested.\n",
      "I have no idea how to do that so far, i belive it's doable. \n",
      "Could a kind soul help me out, or redirect me\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b8e1w2/how_to_scrape_the_videos_of_a_private_online/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping job boards (failed miserably)\n",
      "Text: I'm currently working on a website to aggregate jobs from various job boards such as indeed, wellfound, ycombinator etc. \n",
      "\n",
      "I'm using beautiful soup to do the scraping, using css selectors to get the elements.\n",
      "\n",
      "But if I try going to the actual sites to scrape, it either blocks me or gives a 404 error. How should I change my approach to this (currently a beginner), should I use selenium and automation tools, is the lazy loading affecting my approach.\n",
      "\n",
      "I'll be grateful for any tips you have.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b8704i/scraping_job_boards_failed_miserably/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking to quickly archive some lost websites from the wayback machine however when I try to pull via wget I don't seem to be getting a full backup, some links are missing and not all css is coming with, it's more complete on IA's side. any help is greatly appreciated.\n",
      "Text: Command being used,\n",
      "\n",
      "    wget --recursive --no-clobber --page-requisites --convert-links --domains web.archive.org\n",
      "\n",
      "I've also tried using this Ruby Project on Linux, every attempt it's pulling the html data back corrupted (possibly encoded?), I'm using a DNS service that isn't rate limiting me, and have even modified the code to use \"sleep3\" per download initialized to prevent Wayback machine from rate limiting me.\n",
      "\n",
      "[https://github.com/hartator/wayback-machine-downloader](https://github.com/hartator/wayback-machine-downloader)\n",
      "\n",
      "[https://github.com/hartator/wayback-machine-downloader/issues/273](https://github.com/hartator/wayback-machine-downloader/issues/273)\n",
      "\n",
      "only was able to recover the web images, surely there's a better way to pull entire websites down locally?\n",
      "\n",
      "I see web services that claim to do this perfectly for a fee, but their software must be similar?\n",
      "\n",
      "[https://archivarix.com/](https://archivarix.com/)\n",
      "\n",
      "what's the magic I'm missing here fellas, I'm new to Web Scrapping.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b7sszo/looking_to_quickly_archive_some_lost_websites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can websites spot crawlers by no-mouse-movement?\n",
      "Text: Hey folks, I've been wondering if websites like LinkedIn can spot crawlers due to entirely \"keyboard-like\" behavior. As a web developer myself, I understand that websites can detect mouse movement on their pages but do you think LinkedIn would have something crazy that can detect the \"type\" of mouse movement (including acceleration and movement paths) to distinguish between a crawler and human?\n",
      "\n",
      "I tried my hand at Selenium but see that some websites throttle me even when I'm at a tad slower pace than a human. Perhaps their bot detection system is able to spot tha difference as bot interactions = no mouse movement?\n",
      "\n",
      "All of that said, my idea is to perform some healthy crawling for self-use without abusing web servers and also nothing commercial.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b7myab/can_websites_spot_crawlers_by_nomousemovement/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: WebScraping Spotify\n",
      "Text: Can someone help me do web scraping of artists on Spotify safely? I specifically need to get the monthly listeners\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b7qkrb/webscraping_spotify/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping 100,000 product pages from a single website: what's conceptually the strategy?\n",
      "Text: I'm using Python and have a bunch of Premium proxies. I want to scrape product details, prices from a dozen or so websites. Each website has about 100,000 products in my niche.\n",
      "\n",
      "But how do I structure my workflow?\n",
      "\n",
      "- I do not want to hammer their site with requests (play nice)\n",
      "- I do not want to crash my own system because I am making too many requests at once (if that's possible)\n",
      "\n",
      "Conceptually, in my head, without any experience, I would think that:\n",
      "\n",
      "1. Segment the 100,000 URLS in batches per CPU core\n",
      "2. I though that parallel woud be best, but the actions are not heavy in computing, so maybe async/await is fine?\n",
      "3. Add some delay (for the not hammering part)\n",
      "\n",
      "Can you share some conceptual examples of this workflow?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b72rw1/scraping_100000_product_pages_from_a_single/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to Not get DETECTED by social media? I use 4g proxy and stealth browser! Rate my setup \n",
      "Text: I use gologin as stealth browser\n",
      "\n",
      "I use 4g mobile proxy \n",
      "\n",
      "Usually Not do more than 200 ip call for account\n",
      "\n",
      "Usually scarpe 500 users a Day using different social media and different account and ip....\n",
      "\n",
      "I use Nodejs, Playwright and Firebase\n",
      "\n",
      "Should i change tò puppeteer stealth ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b71ch2/how_to_not_get_detected_by_social_media_i_use_4g/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I'm looking to scrape google maps for new service businesses\n",
      "Text: I want to scrap google maps for new service based businesses such as tow trucking, landscaping, plumbing, hvac etc. Can anyone lead me in the right direction towards a scraper that does this, so the businesses it scrapes do have to be new. Maybe there is something on github or someone here might have an answer, thank you! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b7ggnq/im_looking_to_scrape_google_maps_for_new_service/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: WebScraping in no GUI environnement \n",
      "Text: I’m working on a project for real estate, i started scraping data, started from facebook marketplace, and it was successful.\n",
      "\n",
      "I used browser driver, and worked with chrome ( because it is installed on my laptop ).\n",
      "But I’m wondering if someone else wanted to use my script and he doesn’t have any type of browsers , for example someone using Arch, Debian, CentOs, would it be feasible ?\n",
      "\n",
      "If it is, how i can do that !\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b7fmp5/webscraping_in_no_gui_environnement/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What are the best selectors to use when scraping Google search results?\n",
      "Text: I am building an SEO Chrome extension for a client that will scrape search results and display them in a formatted window. I just want to know which selectors Google doesn't change often, such as classes or data IDs, what I should use. If anyone have experience of building such thing would love to know about their experience\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b78cl6/what_are_the_best_selectors_to_use_when_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Fetching addresses from geospatial view\n",
      "Text: I am planning on fetching all addresses from below URL for training a downstream recommendation system.   \n",
      "If you open below link, zoom in and them click on any property you can see a small popup with address. When we click it sends a get request to API with x & y points and brings the address. Problem is I cannot find all the addresses without brute forcing X & Y.  \n",
      "\n",
      "\n",
      "[https://www.planningportal.nsw.gov.au/spatialviewer/#/find-a-property/address](https://www.planningportal.nsw.gov.au/spatialviewer/#/find-a-property/address)  \n",
      "\n",
      "\n",
      "Is there any other way to bring in addresses? Any service or API. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b7d150/fetching_addresses_from_geospatial_view/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: google ReCaptcha - no Api\n",
      "Text:  \n",
      "\n",
      "I am trying to scrape data from this website. [https://servicesenligne2.ville.montreal.qc.ca/sel/evalweb/index](https://servicesenligne2.ville.montreal.qc.ca/sel/evalweb/index)\n",
      "\n",
      "where I enter the lot number through \"Lot rénové,\" for example: 1381520. \n",
      "\n",
      "It will take you to the property details webpage. \n",
      "\n",
      "The issue is that the site is using Google ReCaptcha to check every request; there is no API. \n",
      "\n",
      "Is there a way to scrape data without using Selenium?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b73cho/google_recaptcha_no_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I was using scrapy to web scrap a page but the css selector always gives empty output\n",
      "Text: IDK if it's a security  thing or me making a mistake \n",
      "\n",
      "I do Css selectors in the terminal and it yields  nothing \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b72xrk/i_was_using_scrapy_to_web_scrap_a_page_but_the/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping products from shopee using python requests returns 90309999\n",
      "Text: Since 2022, I was able to scrap [shopee listings](https://shopee.com.br/search?filters=9&keyword=funko&locations=Nacional&noCorrection=true&page=0&sortBy=relevancy) using [python requests](https://requests.readthedocs.io/en/latest/) with headers `User-Agent`and `From` only. However, since last week, I've been receiving error code number ***90309999*** from {\"is\\_customized\":false,\"is\\_login\":false,\"action\\_type\":2,\"error\":90309999,\"tracking\\_id\":\"583f2b10-6b13-4402-8b3b-a6b8f0fead62\"}\n",
      "\n",
      "Also, I've checked this [stackoverflow thread](https://stackoverflow.com/a/74333263/5465165) and added `headers x-api-source` and `af-ac-enc-dat`, but isn't working. Any suggestions?\n",
      "\n",
      "    from requests import Session\n",
      "    \n",
      "    base_url = 'https://shopee.com.br/search?filters=9&keyword=funko&locations=Nacional&noCorrection=true&page=0&sortBy=relevancy'\n",
      "    \n",
      "    headers = {\n",
      "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0',\n",
      "        'From': '',\n",
      "        'af-ac-enc-dat': 'null',\n",
      "        'x-api-source': 'pc',\n",
      "    }\n",
      "    \n",
      "    with Session() as s:\n",
      "        s.get(base_url, headers=headers)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b6vhq2/scraping_products_from_shopee_using_python/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping customer logos / testimonials from company websites, is this legal?\n",
      "Text: Is it legal to scrape company logos from websites? Similarly, company names from client / customer testimonials, case studies, etc. I know a lot of the time companies don't consent to their logos being used on websites but case studies and testimonials obviously require their consent/participation.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b6sdf3/scraping_customer_logos_testimonials_from_company/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title:  Best lenguage to create web scraper  100 instagram users Daily \n",
      "Text: \n",
      "There are some cheap scrap services in the marketplace\n",
      "no point on pay for  software when I can do that by myself, only users no point on adding other targets when there's still room to improvement.\n",
      "\n",
      "What i want acchive Is :\n",
      "\n",
      "the flow would be the next Scrap x amount users from your targets -> Follow only from that list and move them to another list once you've followed\n",
      "\n",
      "Once you've followed everyone in the list scrap new users and delete the duplicates from both lists, with that you're always following recent followers from your targets and removing duplicates/followed people\n",
      "\n",
      "Best lenguage to create web scraper  100 instagram users Daily?\n",
      "\n",
      "Puppeteer stealth extra or playwright?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b6jbin/best_lenguage_to_create_web_scraper_100_instagram/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any ideas for overcoming disappearing ad overlays like this with Selenium? Disappears with dev tools so can't inspect the close button!\n",
      "Text: \n",
      "URL: https://i.redd.it/wp3wfb81lbmc1.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Linkedin Automation Bot with every possible scraping! (2024)\n",
      "Text: Built a LinkedIn scraper for automating connection requests, follow messages, profile visits, post likes, profile endorsements, and more. Find on [GitHub](https://github.com/linkoutapp/linkout-scraper)\n",
      "\n",
      "If you want it hosted on Railway, here's a [docker image](https://github.com/linkoutapp/docker-image/)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b66qs2/linkedin_automation_bot_with_every_possible/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: List of links, a pdf within each link\n",
      "Text: Hi i have a list of links\n",
      "\n",
      "and in each link there's a pdf in there that needs to be download  (not a direct link)\n",
      "\n",
      "what should i use to do this in batch\n",
      "\n",
      "thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b6c62f/list_of_links_a_pdf_within_each_link/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Test exams with the correct answer from a website\n",
      "Text: I am wondering if its possible to get all the questions and correct answer of this website and the easiest way of doing it: \n",
      "\n",
      "https://lifeintheuktestweb.co.uk/test-2/ \n",
      "\n",
      "There are 24 exams, I need them just to study, copy paste will take me ages. \n",
      "\n",
      "Thanks for your help. \n",
      "\n",
      "\n",
      "URL: https://lifeintheuktestweb.co.uk/test-2/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help with building webscraper for job board site\n",
      "Text: I hope I can post this. but i need some help to build a webscraper for a job board I have. I used to use automated point and click tools, but the job board I get jobs from obviously has done some retooling and either I get a 404 page when i load the saved job search page in my automated program, or all of the pages are just repeats of the same 100-200 jobs. If i go to the same page on another computer then I see the jobs as they should be listed with the dates going down as the jobs get older. I'm looking for a pretty automated setup if anyone can point me in the right direction. I'm trying to give as much info as I can without getting my post removed. Thanks. BTW the job board is flexjobs\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b5ssnz/need_help_with_building_webscraper_for_job_board/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: New to webscraping, would appriciate Rstudio help.\n",
      "Text: Sorry if this is not the correct place to post but I was l wondering if anyone had any knowledge on how to use a proxy service with RStudio?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b5y3s6/new_to_webscraping_would_appriciate_rstudio_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Sport Betting API\n",
      "Text: \n",
      "Hello, I’ve built an api with websocket to get real time odds & events has also an endpoint for pregame.\n",
      "\n",
      "I do the major of sport nba, nhl, etc.\n",
      "\n",
      "I was wondering how much would this be worth per month?\n",
      "\n",
      "Should I also make the api send the a telegram bot and sell this as another service too?\n",
      "\n",
      "Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b5nkjk/sport_betting_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What to do when View Page Source does not show the data I want to scrape?\n",
      "Text: Hello - I am fairly new to web scraping, but thought it would be fun to try to figure out how to scrape some data from a trading site in a game I'm playing (Elder Scrolls Online).\n",
      "\n",
      "For reference I am using the requests module from python to try to scrape search result data from here:[https://us.tamrieltradecentre.com/pc/Trade/SearchResult](https://us.tamrieltradecentre.com/pc/Trade/SearchResult)\n",
      "\n",
      "My goal is to be able to look up a specific item to see if there are any new results since the last search.\n",
      "\n",
      "The problem I'm having is that the requests module does not return the data from the search result which seems to be injected into the page in a way that is not 'scrapeable'.\n",
      "\n",
      "When I click 'view page source' on a page with results (for example this one: [https://us.tamrieltradecentre.com/pc/Trade/SearchResult?IconName=staff.png&ItemID=25784&ItemNamePattern=Phoenix+Moth+Restoration+Staff](https://us.tamrieltradecentre.com/pc/Trade/SearchResult?IconName=staff.png&ItemID=25784&ItemNamePattern=Phoenix+Moth+Restoration+Staff)), the ' search-result-view ' shows as empty even though there are clearly results on the page and if I use inspect I can see them.\n",
      "\n",
      "I'm trying to understand why this occurs - if perhaps this is intentionally designed to prevent people from scraping site data? I know I could use selenium to visit the site manually and get the data but it's a lot slower and more overhead so just want to see if anyone could help educate me a bit on why this approach doesn't work.\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b5omfc/what_to_do_when_view_page_source_does_not_show/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Private FB Group Post Comment Scraping HELP!\n",
      "Text: I have a large Facebook group, we produce merchandise.   \n",
      "\n",
      "\n",
      "We wanted to produce an item that included as many members as possible so we asked members to drop a comment on a post to be featured on the tee design - 3.6k people commented   \n",
      "\n",
      "\n",
      "I need to now somehow scrape all those names from the post and export them into a list so they can be used in the design.   \n",
      "\n",
      "\n",
      "Can anyone help with this? Thanks in advance \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b5vvbn/private_fb_group_post_comment_scraping_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trying to use selenium and chrome driver to take screen shot of googles search result. \n",
      "Text: Hi. I’m not a coder but had ChatGPT made me a python script to do a search on Google, then take a screen shot of the results and save it to a file. After a little while it blocks me with a captcha. Is there a way around this? If I used googles api would it allow me to look at search results and take a screenshot. I’m very new and trying to learn. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b5jioa/trying_to_use_selenium_and_chrome_driver_to_take/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Creating a job board for specific job titles\n",
      "Text: I'm quite new to this area but wondering if some of you could guide me.\n",
      "\n",
      "The idea I have is to create a job board for myself which searches the web for a specific job title every 15 minutes > puts the result on a sheet or some location that I can find\n",
      "\n",
      "I'm also based in Europe so want to sort by location if I can. My initial thought was to run advanced searches on Google like the ones below and automatically add them to Google sheets:\n",
      "\n",
      "\"sales manager\" site:jobs.ashbyhq.com  \n",
      "\"sales director\" site:jobs.ashbyhq.com  \n",
      "\"sales manager\" site:apply.workable.com  \n",
      "\"sales director\" site:apply.workable.com\n",
      "\n",
      "For a bigger version of this I was thinking to collect career page URLs on Linkedin for the companies that hire in Europe. \n",
      "\n",
      "Can this be done? If yes, can you tell me where to start? The goal is to find jobs as soon they are published and apply to them on companies' career pages. Willing to spend some money on 3rd party tools like Zapier and Make. I have access to Microsoft Power Automate, Copilot, ChatGPT if that helps.\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b4obiy/creating_a_job_board_for_specific_job_titles/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Which VPN service allows users to connect programmatically by python?\n",
      "Text:  Most of popular VPN services come with GUI software. This isn't what I'm  looking for. I need to be able to connect to their VPN service by  python programming language. I need to connect and disconnect several  times to multiple different VPN servers around the world. Are there any  services that come with an API for python? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b4tbl8/which_vpn_service_allows_users_to_connect/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to use playright py with vercel\n",
      "Text: Where or how, \"playright  install\" should be writen so that I can install it in vercel, and I am using flask \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b4r9g6/how_to_use_playright_py_with_vercel/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Open street map\n",
      "Text: Hi, I need help with scraping data from the map after pressing the button, request is sent and now I would like to know how to get the BBOX value to make 1 request for all possible points.  \n",
      "https://internet.gov.pl/geoserver/public/wms?SERVICE=WMS&VERSION=1.3.0&REQUEST=GetFeatureInfo&FORMAT=image%2Fpng&TRANSPARENT=true&QUERY\\_LAYERS=address\\_points&LAYERS=layers&INFO\\_FORMAT=application%2Fjson&FEATURE\\_COUNT=5&I=229&J=30&WIDTH=256&HEIGHT=256&CRS=EPSG%3A3857&STYLES=&BBOX=2659855.7102675624%2C6585814.357050786%2C2660008.5843241327%2C6585967.231107356   \n",
      "\n",
      "\n",
      "it returns \n",
      "\n",
      "{\n",
      "\n",
      "\"type\": \"featureCollection\",\n",
      "\n",
      "\"features\": \\[\n",
      "\n",
      "{\n",
      "\n",
      "\"type\": \"Feature\",\n",
      "\n",
      "\"id\": \"s\\_lubelskie\\_address\\_points.3856808\",\n",
      "\n",
      "\"geometry\": {\n",
      "\n",
      "\"type\": \"Point\",\n",
      "\n",
      "\"coordinates\": \\[\n",
      "\n",
      "2659993.05772531,\n",
      "\n",
      "6585948.52715873\n",
      "\n",
      "\\]\n",
      "\n",
      "},\n",
      "\n",
      "\"geometry\\_name\": \"geometry\",\n",
      "\n",
      "\"properties\": {\n",
      "\n",
      "\"terc\": 604011,\n",
      "\n",
      "\"simc\": 987800,\n",
      "\n",
      "\"city\\_name\": \"Hrubieszów\",\n",
      "\n",
      "\"streets\": 17011,\n",
      "\n",
      "\"street\\_name\": \"field\\_street\",\n",
      "\n",
      "\"house\\_number\": \"28A\",\n",
      "\n",
      "\"summary\": 4\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      "\\],\n",
      "\n",
      "\"totalFeatures\": \"unknown\",\n",
      "\n",
      "\"numberReturned\": 1,\n",
      "\n",
      "\"timeStamp\": \"2024-03-02T13:46:48.149Z\",\n",
      "\n",
      "\"crs\": {\n",
      "\n",
      "\"type\": \"name\",\n",
      "\n",
      "\"properties\": {\n",
      "\n",
      "\"name\": \"urn:ogc:def:crs:EPSG::3857\"\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      "and I only need the id from this\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b4oofw/open_street_map/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Evaluate the maintenance cost of a scrapper in terms of changes in the website source code\n",
      "Text: What is the way to go if you want to evaluate how often things change on a website you are interested in crawling? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b4k4hw/evaluate_the_maintenance_cost_of_a_scrapper_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How much do you make from web scraping?\n",
      "Text: I'm pretty skilled at web scraping. Can you make living from web scraping?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b3xekj/how_much_do_you_make_from_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Getting detected using all measures\n",
      "Text: Hi! So I'm completely lost. I am trying to scrape leboncoin.fr, and I'm using playwright (python) and 200 rotating proxies. I make a new chromium browser for EVERY request, have the correct headers, I'm using playwright_stealth and checked with several bot detection tests. However, I still seem to get detected 98% of the time. Any suggestions?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b47qd5/getting_detected_using_all_measures/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Question on too many requests error: 429\n",
      "Text: Hi guys, \n",
      "\n",
      "I'm scraping a website that requires login, and it will return 429 errors (too many requests error) when I scrape for 60 pages consecutively.\n",
      "\n",
      "For your info, I am using python library requests to run this scraper synchronously.\n",
      "\n",
      "An interesting finding is that every time I faced a 429 error, I could reset this via clicking the \"Disconnect and delete runtime\" and I could scrape the website again for another 60 pages. \n",
      "\n",
      "Not sure if I could automate this \"Disconnect and delete runtime\" for my web scraper. I don't mind to pay a fee for Google Cloud or proxy. Just that my budget is around $3 monthly because it's just personal hobby project.\n",
      "\n",
      "Thanks in advance.\n",
      "\n",
      "https://preview.redd.it/mmlkmz7lrtlc1.png?width=1450&format=png&auto=webp&s=ba87fec95a8cf6112af091591ff871460b34550b\n",
      "\n",
      "Found this reference but not sure which scraping techniques is suitable for my case: [https://stackoverflow.com/questions/22786068/how-to-avoid-http-error-429-too-many-requests-python](https://stackoverflow.com/questions/22786068/how-to-avoid-http-error-429-too-many-requests-python)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b4c56x/question_on_too_many_requests_error_429/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Monthly Self-Promotion Thread - March 2024\n",
      "Text: Hello and howdy, digital miners of /r/webscraping!\n",
      "\n",
      "The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n",
      "\n",
      "* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n",
      "* Maybe you've got a ground-breaking product in need of some intrepid testers?\n",
      "* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n",
      "* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n",
      "\n",
      "Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n",
      "\n",
      "Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b3rn0a/monthly_selfpromotion_thread_march_2024/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to properly use multi-threading for web-scraping?\n",
      "Text: How do you approach this?\n",
      "\n",
      "In Python, it is easy to use multiprocessing-package. In Node things get a little more complicated.\n",
      "\n",
      "Is it the recommended way to spawn processes from your code or would you rather build a scraper that runs only one process and instead run this scraper multiple times on the same machine?\n",
      "\n",
      "How to make sure that the CPU power is actually put to good use on Linux? Or inside Docker?\n",
      "\n",
      "So many questions! I hope some of you already went on this journey and care to help out. Cheers :)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b3vinp/how_to_properly_use_multithreading_for_webscraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Use-cases of web scraping\n",
      "Text: What is your primary use-case for building web scrapers? Most of my scrapers have been for tracking odds on sports betting sites, but I imagine the most common use-case is probably e-commerce price scraping since there's no other real option for this. However, the articles I've read put lead generation at the top, which seems silly to me since there are plenty of lead gen services that aggregate data from thousands of places, and would probably be more economical and trustworthy than building/maintaining a ton of scrapers.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b3lvhu/usecases_of_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Puppeteer on AWS - 15 seconds to execute a simple scrape of 1 URL. How can I speed it up?\n",
      "Text: Hey guys. My code simply visits a website, scrapes the first 200 words of text, then it’s done. \n",
      "\n",
      "This is currently taking in excess of 15 seconds. And if I add another URL in (E.g the same website, but navigating to the /blog) - it takes up to 30 seconds. \n",
      "\n",
      "I can share my code/answer any questions. I’m using @Sparticuz to deploy it to Lambda from an S3 bucket. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b3scae/puppeteer_on_aws_15_seconds_to_execute_a_simple/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Mass web scrapping for a company, am I doing it wrong ?\n",
      "Text: I did some web scraping freelance job for a company few years ago.\n",
      "\n",
      "The job was to scrap 10-20 public websites to get public tenders information. It was very much straight forward, I used selenium, xpath, click on some buttons, done.\n",
      "\n",
      "They contacted me again, and this time, it's \\~1000. I have some budget, so I can hire people on upwork to do the scraping scripts and focus on a software architecture to handle all the scrapers (which is my main skill, not scraping).\n",
      "\n",
      "But, I realized that I didn't knew about scraping that much. I know selenium, but that's about it. I didn't used things like beautiful soup, etc...  \n",
      "I always assumed that selenium was the only good solution, because it's running in a web browser, loading the js, and I can click on buttons.\n",
      "\n",
      "I tried beautifulsoup and scrapy, and it seems to be the case. I can't click on buttons and interact with the webpage.\n",
      "\n",
      "But I am feeling that I might missing something. Am I ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b3df3e/mass_web_scrapping_for_a_company_am_i_doing_it/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping reactive content (button click)\n",
      "Text: Hello,\n",
      "\n",
      "I’m attempting to harvest pricing from petco, using python and hrequests.\n",
      "\n",
      "I’m running into a problem where the website loads the pricing data via html and a reactive component button alters the price when you select 1 month, 6 month or 12 months of product.\n",
      "\n",
      "Is there away to toggle this using requests? Or am I better off using a browser emulator?\n",
      "\n",
      "https://www.petco.com/shop/en/petcostore/product/heartgard-plus-chewables-for-dogs-26-to-50-lbs\n",
      "\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b3ohhp/scraping_reactive_content_button_click/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraper, Bot, or Automation?\n",
      "Text: I'm looking to figure out what the hell to call my thing lol. The context is this:\n",
      "\n",
      "I've been building basically an automation that does in fact collect data using Python playwright, and I had a long conversation with Google's Gemini about what to call it.\n",
      "\n",
      "It called it a bot, but it's not the full automation. \n",
      "\n",
      "Basically, I'm trying to find a clean separation between the part that collects data and take actions on the website and the part that'll actually do automations.\n",
      "\n",
      "What term would you use for something that doesn't ONLY scrape data, but also can take actions on a website? Like \"MySite.apply_to_job(job)\"?\n",
      "\n",
      "Gemini calls it a bot, but to me a bot could have more things like a database and what not built in.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b3o8yk/webscraper_bot_or_automation/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Just joined, still a noobie.\n",
      "Text: Im just an enthusiast, learning all of this in my spare time. Ive probably spent about 300 hours or so now studying c, c++, html, python and such as well as writing my own code.\n",
      "\n",
      "My main project is to build an ai capable of scraping charts, and the internet to find patterns and trends. Then having it choose stocks based off of those predictions and executing those trades through my preffered trading platforms api, then selling once they either succeed or fail.\n",
      "\n",
      "Scripy and beautiful soup are my preferred tools for making spiders. \n",
      "\n",
      "My ideal plan  is to use them with a set of functions defining the strategy i used myself to make money, as well as a set of deep learning to improve upon that strategy.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b37qus/just_joined_still_a_noobie/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web scraping in excel/google sheets - as a total beginner\n",
      "Text: Hello guys, I'm in the e-comm business and recently i'm thinking of more and more automation.\n",
      "\n",
      "I generate 50-60 tracking numbers daily and they contain name, phone number, tracking number and product. I would like all this data to go into an excel/google sheets automatically.\n",
      "\n",
      "I have no experience in coding ( besides a little bit of HTML, CSS and JS ) and it's my second day of thinking about this ideea.\n",
      "\n",
      "Would this be hard to do ? What tools should i use ? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b3124f/web_scraping_in_excelgoogle_sheets_as_a_total/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: best way to go around nginx rate limits?\n",
      "Text: I am scraping a website from which I know is using nginx and rate limits each IP addresses to 2MB, at the moment proxies are a little scarce, what I love about this site is it does not block your IP, there's just a speed limit, how can I go around this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b349vv/best_way_to_go_around_nginx_rate_limits/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Built A LinkedIn Scraper\n",
      "Text: LinkedIn scraper that scrapes specific profiles or scrapes multiple profiles based on a keyword.\n",
      "\n",
      "Check it out [scraper](https://github.com/danited1234/linkedin_scraper)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b2rg6i/built_a_linkedin_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Facebook Post liker list scraper\n",
      "Text: Hi guys, I'm on a project crawling a Facebook post's liker list data.\n",
      "\n",
      "I've been using the mobile web version and it was fine the whole last year. But recently, Facebook is getting tough and blocks my account. I don't crawl like crazy, maybe around 1k liker per 30 minutes with a residential proxy. \n",
      "\n",
      "Anyone knows a more solid solution to this? Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b2qdr9/facebook_post_liker_list_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Tweety project - 24/7 Twitter Scraper\n",
      "Text: Hi all, I run an OnlyFans Agency, and have an ongoing Twitter Scraping project. Aim is to scrape a subset of profiles as close to 24/7 as possible and action each one.\n",
      "\n",
      "Recently I built a pretty manual Twitter scraper which opens chrome browsers to get elements. Realising how labour-some that is, I got a dev from Fiverr to build an API scraper for me.\n",
      "\n",
      "He built a code using Tweety, which supposedly scraped twitter 24/7 and notifies me when one of the target users posts a tweet.\n",
      "\n",
      "I'm hoping to find someone who has experience with Tweety who can help me understand how to use this code - happy to share\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b2d6r6/tweety_project_247_twitter_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any demo website that blocks scraping bot by IP\n",
      "Text: Hi guys,\n",
      "\n",
      "I am learning scraping, and I did some scraping previously mainly via python library requests. What I usually did is to find the AJAX API from the website and scrape the data from it.\n",
      "\n",
      "Personally, I haven't had encountered IP blocking unless that site requires login access to get the website.\n",
      "\n",
      "Any demo website which I could learn scraping and bypass IP block?\n",
      "\n",
      "Thanks in advance.\n",
      "\n",
      "(Notes: I know some website like quotes.toscrape.com and httpbin but they're not blocking me if keep scraping with same IP or account login)\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b2n7dm/any_demo_website_that_blocks_scraping_bot_by_ip/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Advanced Web Scraping With Python: Extract Data From Any Site\n",
      "Text: Learn how to manage cookies and custom headers, avoid TLS fingerprinting, recognize important HTTP headers, and implement exponential-backoff request retrying in Python!\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "[https://jacobpadilla.com/articles/advanced-web-scraping-techniques](https://jacobpadilla.com/articles/advanced-web-scraping-techniques)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b1s4rr/advanced_web_scraping_with_python_extract_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Manual email alias list\n",
      "Text: I've started webscraping with one specific site, for which I needed a ton of mail accounts. Since this was one of the hardest parts and I didn't and still don't have much experience I opted to create mails manually using different services. Here are my experiences.\n",
      "\n",
      "|Service|Free Mails (Amount)|Paid Mails|Notes|\n",
      "|:-|:-|:-|:-|\n",
      "|Apple Icloud+|0|Unlimited (0.99$)|Quickly detects fraudulent use and blocks incoming mails by specific addresses.|\n",
      "|Mozilla Relay|5|Unlimited (0.99$)|Free tier does not allow answering mails. Might be considered low quality as it's an obvious spam address|\n",
      "|Duck Mail|50 - 500|No paid tier|The privacy G.O.A.T comes to our rescue once again. By far the best service, but I feel a bit guilty using too many of this because of this exact reason, as they will slowly get blacklisted on popular sites (Twitch already did so). Also don't create too many aliases at once.|\n",
      "|Proton Pass|10|Unlimited (2.39$)  Around 5 aliases / min|You can't verify your forwarding address and relaying from your Proton Mail to another provider is Pro \"Mail\" Tier only (3.99$)|\n",
      "|Yahoo|0|500 (5$)|Working with Yahoo always feels a bit sketchy. Forwarding is a bit harder, since you need to use IMAP, but it should do the trick with Pro tier.|\n",
      "|Outlook|10|10|High quality mail addresses, very hard to create. You get blocked very fast, but it works.|\n",
      "|Gmail|\\~1000|\\~1000|You can create Gmail aliases by adding random dots in your address. What you might consider is *GOOGLE* and how easy it is to detect. [Gmail Alias Generator](https://github.com/brentspine/gmail-alias-generator)|\n",
      "|33Mail|Around 10 mails / hour; Limited bandwidth|Higher rate limits (1$ - 50$)|You will get your own subdomain, for example mail(@)brentspine.33mail.com. Very limited bandwidth and no replies included.|\n",
      "|Own Domain|0|Unlimited (3$ / month)|I host my own domain, which costs me 2.99$ a month and allows unlimited mail addresses for that domain. You can also set up a catch all service.|\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b256en/manual_email_alias_list/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best tools for stealth/undetected web scraping in 2024?\n",
      "Text: I'd like to develop a tool for price tracking of certain products. This will be a big project, so my choice for the tools I will use is important. I am happy about any hints, thank you!\n",
      "\n",
      "Here is what I gathered:\n",
      "\n",
      "**Javascript:**\n",
      "\n",
      "[puppeteer-extra-plugin-stealth](https://github.com/berstend/puppeteer-extra/tree/master/packages/puppeteer-extra-plugin-stealth)\n",
      "\n",
      "* 🟡 last updated 1 year ago\n",
      "* 🟢 high popularity\n",
      "* \\+ [ghost-cursor](https://github.com/Xetera/ghost-cursor)\n",
      "   * 🟢 last updated 1 week ago\n",
      "\n",
      "[puppeteer-real-browser](https://github.com/zfcsoftware/puppeteer-real-browser)\n",
      "\n",
      "* 🔴 low popularity\n",
      "\n",
      "**Python:**\n",
      "\n",
      "[undetected-chromedriver](https://github.com/ultrafunkamsterdam/undetected-chromedriver)\n",
      "\n",
      "* 🟢 last updated 1 week ago\n",
      "* 🟢 high popularity\n",
      "\n",
      "[selenium-stealth](https://github.com/diprajpatra/selenium-stealth/)\n",
      "\n",
      "* 🔴 last updated 4 years ago\n",
      "* Python\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Which tool do you think is best for scraping websites that are protected by Cloudflare or similiar tools? Which of them is better regarding configurability?\n",
      "\n",
      "If you were to start all over now with your project - which route would you take?\n",
      "\n",
      "THANKS!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b1e78j/best_tools_for_stealthundetected_web_scraping_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there a way to bypass the \"Unauthorised Client\" error while webscraping?\n",
      "Text: \"https://www.99acres.com/api-aggregator/discovery/srp/search?area\\_unit=1&platform=DESKTOP&moduleName=GRAILS\\_SRP&workflow=GRAILS\\_SRP&page\\_size=25&page=2&city=15&preference=S&res\\_com=R&seoUrlType=DEFAULT&recomGroupType=VSP&pageName=SRP&groupByConfigurations=true&lazy=true\"\n",
      "\n",
      "I could previously scrape this website, however I'm getting this error all of a sudden.\n",
      "\n",
      "{\n",
      "\n",
      "\"message\": \"Unauthorized Client\",\n",
      "\n",
      "\"responseStatus\": \"UNAUTHORIZED\"\n",
      "\n",
      "}\n",
      "\n",
      "Is there a way to bypass it?\n",
      "\n",
      "The code that I'm using -\n",
      "\n",
      "    #Scrape the Data\n",
      "    \n",
      "    chromedriver_path = 'E:\\\\chromedriver\\\\chromedriver.exe'\n",
      "    chrome_options = webdriver.ChromeOptions()\n",
      "    chrome_options.add_argument(f\"webdriver.chrome.driver={chromedriver_path}\")\n",
      "    driver = webdriver.Chrome(options=chrome_options)\n",
      "    \n",
      "    \n",
      "    #Load the URL\n",
      "    \n",
      "    scrape_prompt = input(\"Should you scrape the data? \\n1. Yes \\n2. No\")\n",
      "    if scrape_prompt == \"1\":\n",
      "        print(\"The scraping process has begun\")\n",
      "        url1 = \"https://www.99acres.com/api-aggregator/discovery/srp/search?area_unit=1&platform=DESKTOP&moduleName=GRAILS_SRP&workflow=GRAILS_SRP&page_size=25&page=\"\n",
      "        url2 = \"&city=15&preference=S&res_com=R&seoUrlType=DEFAULT&recomGroupType=VSP&pageName=SRP&groupByConfigurations=true&lazy=true\"\"\n",
      "        headers = {\n",
      "                \"Accept-Language\": \"en-US,en;q=0.9\",\n",
      "                \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
      "        }\n",
      "        \n",
      "        page_number=0\n",
      "        all_data = []\n",
      "        data_list = []\n",
      "        previous_data = None\n",
      "    \n",
      "        max_pages = 3000\n",
      "        for page_number in range(0,max_pages+1):\n",
      "            base_url = url1 + str(page_number) + url2    \n",
      "            response = requests.get(base_url, headers=headers)\n",
      "            \n",
      "            if response.status_code == 200:\n",
      "                data = response.json()\n",
      "                all_data.append(data)\n",
      "                print(f\"Page {page_number} data retrieved.\")\n",
      "                # Check if the data of the current page is the same as the previous page\n",
      "                if previous_data and all(prev_item == curr_item for prev_item, curr_item in zip(previous_data, data)):\n",
      "                    print(f\"Data retrieval is complete.\")\n",
      "                    break\n",
      "                else:\n",
      "                    previous_data = data\n",
      "            else:\n",
      "                print(f\"Failed to retrieve data for page {page_number}. Status code: {response.status_code}\")\n",
      "                break\n",
      "            time.sleep(3)\n",
      "            page_number +=1\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b23y3a/is_there_a_way_to_bypass_the_unauthorised_client/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Reliable non dedicated phone service\n",
      "Text: I need to create many PVA (phone verified account) bots for my scraper. Is there a service that allows me to verify a lot of accounts for a specific site? I will not have to use the number outside of this one time use, so buying dedicated numbers would get very expensive very fast. Is there any website I can try out that you might be able to vouch for? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b227nj/reliable_non_dedicated_phone_service/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is it illegal to write code that just replaces me clicking like a monkey every day?\n",
      "Text: &#x200B;\n",
      "\n",
      "I've written a couple of very simple node js / playwright scripts to get interesting car deals and one for searching scientific papers.\n",
      "\n",
      "They aren't used in any commercial way.\n",
      "\n",
      "I know about the \"robots\" field in the websites' manifest, but... is this automation (i.e web scraping) *merely for personal purposes* illegal?\n",
      "\n",
      "I am in the UK (but can easily use a VPN, although I doubt this changes anything ?) \n",
      "\n",
      "I unfair for this to be illegal, since it's just ones' automation of typing. \n",
      "\n",
      "What is the reality?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b0w3i0/is_it_illegal_to_write_code_that_just_replaces_me/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Deleted Reddit User Page\n",
      "Text: Hello!\n",
      "\n",
      "Forgive my lack of knowledge. I majored in computer science but have very little web dev knowledge. \n",
      "\n",
      "Anyways, I woke up today and found myself in an interesting position. Last night, I was looking through the comments of a certain reddit user, who has suddenly become a person of interest. Maybe you can figure out who it is, but it's not that important. Today, all of the comments are deleted, but I can still access them because I had the page open in Chrome. I don’t know how this stuff works; I can duplicate the tab and still see everything. I’m worried I'll suddenly lose access to it all.\n",
      "\n",
      "I'd like to scrape the page as it is in my browser, without a refresh. Is there a way to do this? I've done some googling, but haven't found anything promising yet. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b1gs0g/scraping_deleted_reddit_user_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ikea kitchen planner - adjust kitchen and gather new url + price + image using python\n",
      "Text: Greetings all,\n",
      "\n",
      "**Background:**\n",
      "\n",
      "Ikea offers a kitchen planner where potential customers can design their own kitchen based on for example their floorplan and personal wishes.\n",
      "\n",
      "Ikea also offers some pre-signed kitchens that are public. See for example:  \n",
      "[](https://kitchen.planner.ikea.com/nl/nl/planner/?projectId=46C94AB4-7FA9-4722-8D89-FD079CED790D)  \n",
      "I'm from the Netherlands, hence the 'nl/nl' in the weburl, but I bet similar example urls are available for the US website ('us/en').\n",
      "\n",
      "What is nice about this feature is that it automatically generates 8 example kitchens based on the floorplan that is provided. Hence, when you adjust the floorplan manually (under 'Bepaal je ruimte'), 8 new kitchens will be generated (under 'Kies een favoriet'), each of which has a price and 'standard' image coupled to it (under 'Realiseren'). Creating a floor plan is manual by default and the url within the browser is not updated by adjusting the floorplan.\n",
      "\n",
      "**Question:**\n",
      "\n",
      "How could we use the Ikea kitchen planner to automatically generate 8 kitchens, along with their url's, prices and images after inputing the length and width of the floorplan using python?\n",
      "\n",
      "Kind regards,\n",
      "\n",
      "Victor\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b1bzl8/ikea_kitchen_planner_adjust_kitchen_and_gather/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need Help with Scraping Detailed Financial Reports from ekrs.ms.gov.pl\n",
      "Text: Hello r/webscraping!\n",
      "\n",
      "I'm struggling with a project where I need to scrape detailed financial reports of Polish companies from a specific website. I'm looking for advice on how to do this more efficiently using Python.\n",
      "\n",
      "**Website URL**: I'm aiming to scrape data from the search form results page at https://ekrs.ms.gov.pl/rdf/pd/search_df. Specifically, I'm interested in the results page that comes up after entering a KRS number, like KRS 0000402267 for example.\n",
      "\n",
      "**Data Points**: My goal is to extract specific information from the financial reports, such as the company name, KRS number, fiscal year, and detailed financial data presented in the reports, like revenue, profits, assets, and liabilities.\n",
      "\n",
      "**Project Description**: I'm working on a financial analysis of Polish companies and need an automated way to gather their financial reports for further analysis. I started experimenting with Selenium in Python, but I'm encountering difficulties related to the dynamic nature of the site, such as AJAX and dynamically loaded data, making my current script inefficient and unreliable.\n",
      "\n",
      "So far, I haven't been able to find a hidden API that might simplify the process, so I'm open to any suggestions on techniques and tools that could help in this situation.\n",
      "\n",
      "Could anyone share tips or similar experiences related to scraping sites with dynamically loaded data? What tools or libraries could help with this? What strategies can I apply to deal with the dynamic nature of this site?\n",
      "\n",
      "I would be grateful for any advice and suggestions.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b1afxl/need_help_with_scraping_detailed_financial/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webinar Invite: Overcome Web Scraping Barriers\n",
      "Text:  \n",
      "\n",
      "**Unlock the Secrets to Effortless Web Scraping - Join Our Webinar!**\n",
      "\n",
      "Hey everyone!\n",
      "\n",
      "Frustrated with web scraping limitations? Join NetNut's exclusive webinar and discover how to bypass anti-bot systems like a pro. Our experts, Eitan Bremler and Pierluigi Vinciguerra, will guide you through advanced strategies to mimic real users, navigate CAPTCHAs, and rotate IPs seamlessly.\n",
      "\n",
      "📅 **Date:** March 28, 2024  \n",
      "⏰ **Time:** 9:00 AM EST  \n",
      "🔒 **Limited spots available** \\- Secure yours now!\n",
      "\n",
      "Learn to collect data efficiently without getting blocked. Perfect for data scientists, SEO professionals, and anyone eager to refine their web scraping skills.\n",
      "\n",
      "**Reserve your spot here:** [NetNut Webinar Registration](https://netnut.io/lp/website-unblocker-webinar/?utm_source=reddit&utm_medium=organic)\n",
      "\n",
      "See you at the webinar!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b18dix/webinar_invite_overcome_web_scraping_barriers/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping food publications\n",
      "Text: Hey everyone,\n",
      "\n",
      "I'm working on a project where we're trying to scrape restaurant information from hundreds of food publications like Bon Appetit, Spoon University, Eater, The Infatuation, etc...\n",
      "\n",
      "Right now, we're currently using Python and Selenium to manually build scrapers for each publication. While this works for a few publications, it's not ideal when you want to scrape food publications across hundreds of cities in the US. Do you guys know an ideal solution, service, or software that can help us with this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b0xbq7/scraping_food_publications/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Decode / Decrypt request response\n",
      "Text: Hello,  I'm diving into a project where i need to decrypt and understand the data returned by the Australian Open's CourtVision API: https://itp-ao-sls.infosys-platforms.com/prod/api/court-vision/belowCourt/year/2024/eventId/580/matchId/MS701/pointId/0\\_0\\_0.\n",
      "\n",
      "API requests like this are make by pages like the [AO Open WebSite](https://ausopen.com/match/2024-jannik-sinner-vs-daniil-medvedev-ms701#!infosys-3), this is the page for the  Jannik Sinner vs. Daniil Medvedev match.\n",
      "\n",
      "In the past people were able to access the CourtVision data to scrap, directly via the API like shown above used similarly in this [GitHub Repo](https://github.com/ryurko/hawkeye) (the API used to return JSON)\n",
      "\n",
      "I'm able to view the data in the website looking at the div.CourtContainer properties for the CourtVision data, however i need a reliable way to build a database of as many years as possible and many matches as possible, of CourtVision data.\n",
      "\n",
      "If the Frontend is translating the API response to JSON, there must be a way to reverse engineer this decryption, to make a scraper like this [GitHub Repo](https://github.com/ryurko/hawkeye) work again.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b0u0gt/decode_decrypt_request_response/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need Help with Accessing INCI Beauty Database\n",
      "Text:  Hi all,\n",
      "\n",
      "I'm looking to access or collect the **INCI Beauty database**, which has open info on product ratings and ingredients. Does anyone know of a **method to scrape** this data or if there's an **existing export** available? Interested in using it for **research purposes**.\n",
      "\n",
      "Appreciate any tips or resources you might have.\n",
      "\n",
      "Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b0j7hv/need_help_with_accessing_inci_beauty_database/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Where can I get Historical Data on the Options index: \"OMX30\".\n",
      "Text: &#x200B;\n",
      "\n",
      "Where can I get Historical Data on the Options index: \"OMX30\".\n",
      "\n",
      "It it listed on the NASDAQ Stockholm Exchange.\n",
      "\n",
      "I need data from between 2018-2023, and minimum 50,000 observations.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Data needed:\n",
      "\n",
      "\\- Daily closing prices of option premiums\n",
      "\n",
      "\\- Daily volume of the contracts\n",
      "\n",
      "\\- Different expiration contracts (30,60,90,180 day contracts)\n",
      "\n",
      "\\- Different strike prices (in-,at, out- of the money contracts)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "On this website [https://www.nasdaqomxnordic.com/optionsandfutures](https://www.nasdaqomxnordic.com/optionsandfutures) there's single data  \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b0i698/where_can_i_get_historical_data_on_the_options/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Library I Created to Migrate to Cloudflare with Puppeteer\n",
      "Text: Hi, I just released a new update. It's free and open source. It is not caught by Cloudflare. I hope it will benefit you.\n",
      "\n",
      "[https://github.com/zfcsoftware/puppeteer-real-browser](https://github.com/zfcsoftware/puppeteer-real-browser)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1azt6yl/library_i_created_to_migrate_to_cloudflare_with/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web scrapping\n",
      "Text: I want to write a python code to scrape the website  [https://www.bls.gov/news.release/cpi.t01.htm](https://www.bls.gov/news.release/cpi.t01.htm)  and return value of Food , Gasoline and Shelter at 2023-Jan.2024 and find their average\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "output should be like this\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Food : 0.4\n",
      "\n",
      "Gasoline : -3.3\n",
      "\n",
      "Shelter: 0.6\n",
      "\n",
      "average is : 0.76\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Here's my code so far, but I'm getting \"Failed to fetch data. Status code: 403\", any modification in my code? Thanks\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "    import requests\n",
      "    from bs4 import BeautifulSoup\n",
      "    \n",
      "    def scrape_inflation_data(url):\n",
      "        headers = {\n",
      "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
      "        \n",
      "        # Send a GET request to the URL with headers\n",
      "        response = requests.get(url, headers=headers)\n",
      "        \n",
      "        if response.status_code == 200:\n",
      "            print(\"Successfully fetched data.\")\n",
      "            \n",
      "            # Parse the HTML content using BeautifulSoup\n",
      "            soup = BeautifulSoup(response.text, 'html.parser')\n",
      "            \n",
      "            # Find the relevant table containing the data\n",
      "            table = soup.find('table', {'class': 'regular'})\n",
      "            \n",
      "            # Extract data for Food, Gasoline, and Shelter for Jan 2023 to Jan 2024\n",
      "            data_rows = table.find_all('tr')[1:]  # Skip header row\n",
      "            values = {'Food': None, 'Gasoline': None, 'Shelter': None}\n",
      "    \n",
      "            for row in data_rows:\n",
      "                columns = row.find_all('td')\n",
      "                category = columns[0].get_text().strip()\n",
      "    \n",
      "                if category in values:\n",
      "                    # Extract the inflation value for each category\n",
      "                    values[category] = float(columns[-1].get_text().strip())\n",
      "    \n",
      "            return values\n",
      "    \n",
      "        else:\n",
      "            print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
      "            return None\n",
      "    \n",
      "    def calculate_average(data):\n",
      "        # Filter out None values and calculate the average\n",
      "        valid_values = [value for value in data.values() if value is not None]\n",
      "        average = sum(valid_values) / len(valid_values) if valid_values else None\n",
      "        return average\n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        url = \"https://www.bls.gov/news.release/cpi.t01.htm\"\n",
      "        inflation_data = scrape_inflation_data(url)\n",
      "    \n",
      "        if inflation_data:\n",
      "            for category, value in inflation_data.items():\n",
      "                print(f\"{category} : {value}\")\n",
      "    \n",
      "            average_value = calculate_average(inflation_data.values())\n",
      "            print(f\"average is : {average_value}\")\n",
      "        else:\n",
      "            print(\"No data retrieved.\")\n",
      "    \n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1b08pxg/web_scrapping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Working Instagram scraper without logging in!\n",
      "Text: This my first working Instagram scraper. It works perfectly all you need is a list of Instagram account you want to scrape. \n",
      "\n",
      "This is my first major project, and very proud how far I've come in leaning python!. \n",
      "\n",
      "I'm open to here about how it can improved.\n",
      "\n",
      "This is my repository in git hub: [https://github.com/jhontotomato/Snapscrape.git](https://github.com/jhontotomato/Snapscrape.git) \n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1azx9sc/working_instagram_scraper_without_logging_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Puppeteer is being detected\n",
      "Text: Hi guys,\n",
      "\n",
      "I have a project that was originally written in Selenium in windows, but later I decided to move it to Puppeteer.  I shifted from Selenium to Puppeteer because I have 3-4 network adapters. I read somewhere that I can select which adapter the program uses in Puppeteer.\n",
      "\n",
      "I'm attempting to scrape a website, performing actions such as creating an account, signing in, posting content, and messaging someone. I'm using the mobile view feature with page.emulate(phone), where the phone model is obtained from Puppeteer's knownDevices list.I've tried various approaches like Puppeteer Extra, the Stealth plugin, and even using Puppeteer with a real browser, but the website always detects my scraping attempts. This happens even when I manually input credentials and perform other actions.\n",
      "\n",
      "My first question is, why am I being detected while using Puppeteer? And secondly, can I switch between network adapters when running the browser? For example, running three processes with each one using a different network adapter. Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1azxiyl/puppeteer_is_being_detected/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web Scrape clothing website\n",
      "Text:  Hey everyone,\n",
      "\n",
      "I'm looking to scrape data from Sellpy, an online marketplace that uses Algolia for its search functionality. I'm new to web scraping and could use some guidance on how to approach this effectively.\n",
      "\n",
      "Specifically, I'm interested in extracting product information such as names, links, measurements, and prices from Sellpy's listings. However, navigating the Algolia-powered search feature presents some challenges.\n",
      "\n",
      "If anyone has experience with scraping websites that use Algolia or have done similar projects, Id appreciate any advice  you can offer. Any suggestions for libraries or tools to use, insights into Algolia's API, or any tips for scraping dynamic websites, im open for any help!\n",
      "\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1azvhgf/web_scrape_clothing_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Some neat Powershell code to traverse tables\n",
      "Text: This gives you all the text and the right childnode for that particular text.\n",
      "\n",
      "\n",
      "\t$url = \"https://finviz.com/quote.ashx?t=TCMD&p=d\"\n",
      "\t$page_result = Invoke-WebRequest $url\n",
      "\t$html = ConvertFrom-Html $page_result\n",
      "\t$tables= $html.SelectNodes('//table')\n",
      "\tcls\n",
      "\n",
      "\tfunction TraverseNodes($node, $indentLevel, $tablenumber, $totaltext) {\n",
      "\t    Write-Host (\" \" * $indentLevel) $node.innertext.trim()\n",
      "\t    Write-Host (\" \" * $indentLevel) $totaltext\n",
      "\t    if ($node.HasChildNodes) {\n",
      "\t\t$i=0\n",
      "\t\tforeach ($childNode in $node.ChildNodes) {\n",
      "\t\t    $newtotaltext = $totaltext + \".childnodes[\" + $i + \"]\"\n",
      "\t\t    TraverseNodes $childNode ($indentLevel + 2) $i $newtotaltext\n",
      "\t\t    $i++\n",
      "\t\t}\n",
      "\t    }\n",
      "\t}\n",
      "\n",
      "\tif ($tables -ne $null) {\n",
      "\t    $j=0\n",
      "\t    foreach ($table in $tables) {\n",
      "\t\tWrite-Host \"Table:\"\n",
      "\t\t$newtotaltext = \"tables[\"+$j + \"]\"\n",
      "\t\tTraverseNodes $table 0 $j $newtotaltext\n",
      "\t\t$j++\n",
      "\t    }\n",
      "\t} else {\n",
      "\t    Write-Host \"No tables found in the HTML document.\"\n",
      "\t}\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1azk8tk/some_neat_powershell_code_to_traverse_tables/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Automating Browser processes in background \n",
      "Text: Hello I’m trying to build a web application that does web automation in the background.\n",
      "\n",
      "Say for example a user enters a product name and submits, this should kick off a background process that opens a bunch of websites and searches for the product, finds and pre-order the product.\n",
      "\n",
      "Since it’s a web app, I need this to be able to run efficiently without slowing down the server.\n",
      "What’s the best way to proceed with this task and what tool could be able to do this at scale \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1azk5rt/automating_browser_processes_in_background/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I have a job that I could easily automate completely with web scraping LinkedIn\n",
      "Text: Hi all, I have a freelance job that makes me find specific profiles of people from a list of companies they give me. If I could make a simple webscraper that simply takes all of the employees and a description of their role from a company's linkedin profile I would be able to completely automate my work. I would have to do only 100 or so requests per week. I've tried on my own but LinkedIn's anti scraping tools are too good for me. Do any of you know of any tool online I can pay for to do this for me? Any help appreciated. Its frustrating because it would be such a simple and stupid scraping I would need to do to just automate everything.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aze0ge/i_have_a_job_that_i_could_easily_automate/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Faster than Python?\n",
      "Text: I had a webscraper tool built in python & puppetmaster but it is slowwwww. What would be the better option?\n",
      "\n",
      "Edit: I am creating 100s of PDFs with a product page & a checkout page showing that an item will deliver to a specific location \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1az8rpd/faster_than_python/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Amazon web scraping\n",
      "Text: I want to Write python code to scrape amazon data from search results, python code should read each url of search result and scrape the following contents\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Product Name,\n",
      "\n",
      "Categories,\n",
      "\n",
      "Product Description,\n",
      "\n",
      "Price,\n",
      "\n",
      "Discounted Price,\n",
      "\n",
      "Variant SKU Code,\n",
      "\n",
      "Variant Label 1,\n",
      "\n",
      "Variant Value 1,\n",
      "\n",
      "Variant Label 2,\n",
      "\n",
      "Variant Value 2,\n",
      "\n",
      "Tax Value (%),\n",
      "\n",
      "HSN\tProduct Weight (kg), and \n",
      "\n",
      "Image links (up to 24 for each product group)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "and save data in csv file\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "here are search keywords\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "flower bulb\n",
      "\n",
      "flowers seeds -namdhari\n",
      "\n",
      "imported seed sygenta taaaki bennery asia pacific\n",
      "\n",
      "vegetabe seed\n",
      "\n",
      "namdhari seed\n",
      "\n",
      "flora valley\n",
      "\n",
      "unique seed\n",
      "\n",
      "team seed\n",
      "\n",
      "sakaata\n",
      "\n",
      "frenchi seed\n",
      "\n",
      "sunrise seeds\n",
      "\n",
      "cocopet\n",
      "\n",
      "Fertilizer -NPK DAP UREA BIOVITA (1KG TO 5 KG)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "PESTICIDEE & FUNGISIDE\n",
      "\n",
      "PLANTS\n",
      "\n",
      "BIGS PLANTS\n",
      "\n",
      "SESAONL SEEDLING\n",
      "\n",
      "PERMANENT SEEDLINGS\n",
      "\n",
      "LAWN COVER PLANTS\n",
      "\n",
      "CARPET GRASS\n",
      "\n",
      "ARTIFICFICAL GRASS\n",
      "\n",
      "SECULLANTS\n",
      "\n",
      "INDOOR PLANTS\n",
      "\n",
      "OUTDOOR PLANTS\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1azkoc0/amazon_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Storing data\n",
      "Text: Hi Reddit. This is my first post here and I would like to ask you for advice regarding data storing. I'm currently working on an image scraping project, so far I'm saving the images to my filesystem and doing some manipulations with them. All data has a certain structure and file architecture. However, I can't decide how to store the images properly after manipulation step. Initially, I thought about storing it in a database, but it seems to me that storing a large number of blob objects is not the best approach. I also thought about S3, however, the project I am working on is academic (related to my thesis) and I really wouldn’t want to spend extra money. Or is storing data on a local file system also a pretty good solution? Does anyone have any advice on what can be done about this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ayxl0g/storing_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Finding if there’s any news article about list of vendors related to fraud.\n",
      "Text: I have a list of companies, about 200,000 I want to write a script to find if there’s a news article about these businesses regarding fraud. \n",
      "\n",
      "Basically it would input the name of one article is found, returns true if not returns false. That’s all I’m looking for. \n",
      "\n",
      "Reading about scraping google, I see there’s a lot of work around for that. What else can I do to work around this? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ayyomo/finding_if_theres_any_news_article_about_list_of/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Large Amount of PDF’s - General Advice\n",
      "Text: I have a someone who wants me to extract PDF’s from a judiciary website that contains court documents.\n",
      "\n",
      "It has a simple captcha that was easy to crack with some binarization and denoising techniques using OpenCV and PyTesseract. The inner HTML, the content I am after, is generated and retrieved via AJAX calls.\n",
      "\n",
      "The problem is that there’s an upward amount of 12+ million PDF’s on the site alone which as you all can obviously guess takes a lot of storage to hold all that. \n",
      "\n",
      "Anyone have any general advice on what you’d do in this situation?\n",
      "Website is attached below:\n",
      "\n",
      "https://judgments.ecourts.gov.in/\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ayiudf/scraping_large_amount_of_pdfs_general_advice/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Puppeteer supremacy?\n",
      "Text: Hello I just wanted to get your guys' opinions on which if puppeteer is the end all of browser automation. From what I've read puppeteer seems to mimic the browser the best, even loading tiktok livestreams which selenium and playwright are unable to do. That is not even including pupetteer extra and stealth plugin. I guess the only wonder is if I can prevent RTC leak on pupetteer as I was unable to do that with selenium and playwright. But it seems like at least from what i've read is that puppeteer runs the game and selenium and playwright are just mistakes who need to be erased from existence. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ayomqx/puppeteer_supremacy/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need Advice for a Large-scale Web Scraping Project: VPNs vs. Proxies\n",
      "Text: Hi everyone,\n",
      "\n",
      "I'm embarking on a substantial web scraping project and could use some guidance. I'm targeting a website with around 4 million product pages, available in two languages, which brings the total to about 8 million pages. The silver lining is that these pages can be accessed via .json links, offering a way to minimize traffic impact. The site is protected by Cloudflare, but I've managed to bypass this using VPN provider so far.\n",
      "\n",
      "In my tests, I've successfully run 5 Docker containers concurrently under a single VPN account, each using different IPs. However, this is just the start. I plan to scale my operations to scrape up to 100 million pages. This scaling brings me to a crossroads: should I invest in 3-4 VPN accounts, or would proxies be a better route? I've never used proxies before, so I'm particularly interested in insights about their effectiveness, cost, and how they might compare to using multiple VPN accounts for a project of this scale.\n",
      "\n",
      "Any advice, experiences, or tips you can share would be incredibly valuable, especially regarding handling large-scale scraping projects, managing IP rotation, and staying under the radar of protections like Cloudflare.\n",
      "\n",
      "Thanks in advance for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1axb69t/need_advice_for_a_largescale_web_scraping_project/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for iMacros add-on alternative for Google Chrome's browser since in June 2024 they will update to manifest 3.0 and iMacros is with manifest 2.0 is no longer supported to be updated?\n",
      "Text: Hello,\n",
      "\n",
      "I'm currently using iMacros add-on for my Google Chrome browser which works fine for the latest version of it. I have some automations recordings I have with it, and it's very fast and light, compared to using something like chromedriver watir automations. And I like that you don't need any special mode of the browser to run it.\n",
      "\n",
      "I read that Google plan in June 2024 to update their browser to require manifest 3.0 which means many browser add-ons will stop working unless the add-ons are updated to manifest 3.0, and iMacros is end of life and no longer supported.\n",
      "\n",
      "Will there be another way to continue to use the iMacros add-on despite of that?\n",
      "\n",
      "Any ideas or alternatives?\n",
      "\n",
      "Thank you.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1axbc4g/looking_for_imacros_addon_alternative_for_google/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best way to scrape truepeoplesearch without getting blocked\n",
      "Text: I am using python selenium to scrape through truepeoplesearch.com and also using proxies from anyip.io but still getting detected and cloudflare check pops up after some requests \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1axa0sq/best_way_to_scrape_truepeoplesearch_without/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Cant accept Cookies\n",
      "Text: Selenium cant find „accept“-Button\n",
      "\n",
      "I cant accept Cookies on immoscout24 via a Click() with Selenium uc. My selectors are fine but still get „Bad selector“ error… I tried scroll to view and Click or Switched iframes. Any other ideas?\n",
      "URL: https://i.redd.it/ga24av3zv2kc1.jpeg\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Making a fetch request to Google search URL\n",
      "Text: Hi all! 👋\n",
      "\n",
      "I’m trying to deploy a very simple Express.js API that, given a few query parameters, constructs a Google search query and performs a simple fetch request on that URL (since Google pages are server side rendered, you can try this by using Postman and sending a GET to any query url). This works totally fine locally and I’m able to manipulate the returned HTML, but when I deploy this (using fly.io) my request gets blocked by Google and I get a captcha. \n",
      "\n",
      "All I need is the HTML - I do all of the parsing logic using Cheerio and don’t see the need to spin up a headless browser instance (using Puppeteer or Playwright) for this. \n",
      "\n",
      "Any recommendations for how to get around the blocked fetch request on my deployed API? Maybe I should try a different deployment strategy? Open to any and all recommendations… been scratching my head all day on this one.  \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1awymdn/making_a_fetch_request_to_google_search_url/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best way to get a value (mailing address) from a list of URLS\n",
      "Text: I have a list of URLS. I need to collect a mailing address that is located on each page. I know there are many ways to go about this, but figure, why reinvent the wheel. So what i'm looking for is a tool where i give it a URL, it spits out the mailing address. Better yet, I give it a list of URLS and it spits out a list of addresses. (Csv, excel, whatever)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1awxpu6/best_way_to_get_a_value_mailing_address_from_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: No code scraping w/ No Scraping ToS Questions\n",
      "Text: Curious on some no-code scraping questions I am having a difficult time verifying on google (for a report I’m writing, of course). \n",
      "\n",
      "1. Are the no-code scrapers able to be utilized on sites with no-scraping in the Terms of Service? Or do they actively not allow their own program to prevent legal issue?\n",
      "\n",
      "2. If you have to log into that site w/ credentials ~ is it pretty much a guarantee that the website is going to be able to tell you’re scraping?\n",
      "\n",
      "3. Any scrapers that can go undetected and act very human like? Is it best to scrape low levels, will low levels of data be easily detected? \n",
      "\n",
      "4. Any no-code scrapers that are good for handling a one time scrape that align with items above ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1awxo97/no_code_scraping_w_no_scraping_tos_questions/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Guys could you point me out in best direction to periodically get price for several international stocks?\n",
      "Text: Title. International stocks (UK, Sweden, Hong Kong, etc)\n",
      "\n",
      "I just need quote/price. Historical is fine (like 15-30 min delay). Very low call volume (like 10 stocks with 4 calls per day each).\n",
      "\n",
      "As far as i undestand:\n",
      "\n",
      "-, apis likely won't work for this. Cause most cover only US. Or some specific international market. Since i need lots of international markets i will need to use several apis (if i manage to find them). Plus international are mostly paid.\n",
      "\n",
      "-, scraping yahoo finance or trading view seems best option (with such low volume). Am i right?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1awn8lj/guys_could_you_point_me_out_in_best_direction_to/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What are some other ways to pass path parameters?\n",
      "Text: Hello, I am scraping this website that has a json endpoint. I am able to request the site as `https://site/.json` and I get returned a json file of the site's content.\n",
      "\n",
      "I am trying to scrape a page on that site that requires some path parameters such as `https://site/?param1=value1&param2=value2`, I tried these paths but didn't work:  \n",
      "`https://site/.json/?param1=value1&param2=value2`  \n",
      "`https://site/?param1=value1&param2=value2/.json`\n",
      "\n",
      "I also tried passing those parameters in my HTTP request in a params dictionary, still did not work.\n",
      "\n",
      "The site has no documentation.\n",
      "\n",
      "Anyone has any idea how to accomplish this?\n",
      "\n",
      "Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1awkq8u/what_are_some_other_ways_to_pass_path_parameters/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do you notify yourself when something is going wrong?\n",
      "Text: In my job, I often have to write small scrapers to gather information. Usually, I can create and deploy them easily across different servers. However, setting up notifications for when something goes wrong is not so straightforward. I prefer using email as a channel to notify myself, clients, and others. But, to set up email notifications, I need to use services like Mailgun or something similar, and configuring them for different domains can be challenging.\n",
      "\n",
      "Do you encounter similar issues? How do you deal with them? Is there a very simple API available that allows sending 50-100 emails per month without the complex setup required by Mailgun, Mailchimp, or other services?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aw7x7t/how_do_you_notify_yourself_when_something_is/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do pros handle giant scale webscraping projects?\n",
      "Text: I'm interested in learning how you organize your scraping?\n",
      "\n",
      "I can't imagine people writing scripts for each website themself so there has to be some level of abstraction and streamlining no?\n",
      "\n",
      "Say you want to scrape a million websites. Do you first download the Html/css/js files and look whether it is static, ssrendered or dynamic to categorize into requests vs render needed? \n",
      "\n",
      "Do you cluster Selectors/classes etc and then build out a general scraping script for each cluster?\n",
      "\n",
      "Maybe all of that is obsolete now and you just take the html/css/js and give it to an AI to spit out a scraping script(?)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1avy295/how_do_pros_handle_giant_scale_webscraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need to understand how influencer marketing platforms scrape Instagram\n",
      "Text: I'm developing a SaaS platform for Influencer Marketing, but I'm facing challenges with scraping data from Instagram and identifying influencers. Despite creating tools to scrape and store data efficiently, my Instagram account gets suspended quickly, leaving me uncertain about the next steps. \n",
      "\n",
      "However, I've noticed platforms like Modash and InfluencerMarketingAI have vast databases of influencers and offer their data through APIs. Unfortunately, utilizing their APIs is costly for me as I'm operating on a bootstrap budget. Can someone provide guidance on how to proceed? \n",
      "\n",
      "Additionally, these platforms offer insights into influencers' followers, including metrics like fake followers and audience demographics. I aim to create a similar system but need assistance on where to begin.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aw3x9r/need_to_understand_how_influencer_marketing/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping College Faculty Pages\n",
      "Text: I’ve been reading a lot of history books lately, and sometimes it’s super annoying to find non-pop history books on more niche topics. So I want to scrape ~20 different university History faculty pages for a local database to make it easier to find potential work on Native American history or whatever. \n",
      "\n",
      "A lot of them are just static HTML pages, and I’ve scraped those before but I’d like to avoid it this time if possible. Unfortunately though, when I go into devtools they’re not making any API calls as far as I can tell so I’m sort of at a loss. They all run a bunch of JavaScript files which I think may be executing some queries, but I don’t know the language. Here’s the University of Michigan’s page as an example https://lsa.umich.edu/history/people/faculty.directory.html \n",
      "\n",
      "Is the solve at this point to just suck it up and go through the HTML or learn JavaScript so I can read those files? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aw5w1i/scraping_college_faculty_pages/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for help to scrape an employee only ecommerce website\n",
      "Text: Hi, \n",
      "\n",
      "I am looking for a method to scrape only the new products added to an employee only ecommerce website.  Actually I am looking for the new listings only.\n",
      "\n",
      "The new products are added at random times during the day but they have limited sizes and units so trying to find a way to get quick notifications as soon as new products are listed. \n",
      "\n",
      "Not sure if scraping is the solution to happy to get any ideas. \n",
      "\n",
      "Thanks in advance. \n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aw5d31/looking_for_help_to_scrape_an_employee_only/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Seeking Advice on Integrating Human Captcha Resolution with Scrapy\n",
      "Text:  \n",
      "\n",
      "Hey everyone,\n",
      "\n",
      "I'm currently working on a web scraping project using Scrapy, and I've encountered some challenges when dealing with websites that have captchas. I've been trying to figure out a way to integrate human intervention for resolving captchas within my scraping workflow, and then delegate the control back to Scrapy to continue with the scraping process.\n",
      "\n",
      "My goal is to create a program that can detect when a captcha appears during scraping, pause the process, prompt a human to resolve the captcha manually, and then resume scraping once the captcha has been resolved.\n",
      "\n",
      "I've looked into various approaches, such as using external captcha solving services, but I'm interested in creating a more custom solution where the user can interact with the captcha directly.\n",
      "\n",
      "If anyone has experience or suggestions on how to achieve this integration between Scrapy and human captcha resolution, I would greatly appreciate any insights or pointers you could provide.\n",
      "\n",
      "Thanks in advance for your help!\n",
      "\n",
      " \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aw2sks/seeking_advice_on_integrating_human_captcha/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any advice on how to take my scraping to the next step\n",
      "Text: Recently I have been using programs such as taper monkey and selenium to scrape. I have explored utilising the XHR api key for scraping when available.\n",
      "\n",
      "My question really is, how can I learn more technologies as the skills I have now are vulnerable to anti-bot software. Any good books or resources, I have only looked into undetected-chromedriver and proxies but is there anything else I need to know to scrape websites. Currently looking to scrape Vinted if that helps :)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1avuzne/any_advice_on_how_to_take_my_scraping_to_the_next/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web Scraping by Selenium\n",
      "Text:  \n",
      "\n",
      "So I am trying to get a text from this website [https://dexscreener.com/solana/7bzzop3qb2zk3r7wqrzjs5fpeeergdy3hgzxxrn97aey](https://dexscreener.com/solana/7bzzop3qb2zk3r7wqrzjs5fpeeergdy3hgzxxrn97aey) There is a button that when you click automatically copies the contact address of the cryptocurrency. I use selenium. Why? Because the website has cloudflare antidetection and after a long time I finally found code that can bypass it. So I need code that would be able to navigate to that page and click that button and print the pasted text. [copy button](https://i.stack.imgur.com/lezwj.png)\n",
      "\n",
      "I only managed to write code for bypassing the cloudflare antibot detection, I tried using chatgpt to write me the code for getting the text but I keep getting errors. I am not that experienced with coding though.\n",
      "\n",
      "Here is the code;\n",
      "\n",
      "from time import sleep from DrissionPage import ChromiumPage\n",
      "\n",
      "p = ChromiumPage() p.get('https://dexscreener.com/')\n",
      "\n",
      "sleep(5)\n",
      "\n",
      "https://preview.redd.it/v5qfxdy4tvjc1.png?width=1914&format=png&auto=webp&s=4bc9b143d30064451de942eb7927763903455849\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aw53ro/web_scraping_by_selenium/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Deduplication via previews\n",
      "Text: I am trying to download large quantities of images from Reddit, but many of the images are duplicates. I *could* deduplicate them by requesting all of the images and putting them in a hash table after they are downloaded, but this would require me to send a ton of requests, and the whole purpose of deduplication is to keep the execution time low in the first place.\n",
      "\n",
      "Because Reddit loads low-resolution previews of most images before loading the full image, **I thought I could find some way of grabbing these previews from the local storage or cache because they have already been loaded in.** This would keep me from having to send requests for the images. However, I am unable to find any information on this technique, and I don't even know if it's possible. I would think the data exists in local storage, but I don't know how to get to it.\n",
      "\n",
      "For reference, I am using a JS script in the Firefox dev tools console.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aw4w6u/deduplication_via_previews/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: VPS Provider for high scale scraping project\n",
      "Text: I'm looking for a reputable VPS provider for a highly ambitious scraping project. \n",
      "\n",
      "When my product is done, i'll be making roughly 3000 HTTP requests per second. I'm aware this will require multiple VPS' but I want to find servers that will keep my overhead low. \n",
      "\n",
      "I want the results from the requests to come in near-instantly, and they already do from the endpoint so any other slowness will be coming from my end. \n",
      "\n",
      "I need a provider with reasonable prices that can give me unlimited bandwith and ideally 1-5gbps internet speeds. \n",
      "\n",
      "It can't be less frequently than per second, and I will be using caching to lower the load on my servers with redis. \n",
      "\n",
      "I've done a lot of research and for pricepoints OVH looks okay but i've heard horror stories about their customer service. IONOS also offers the specs and pricing I need but it caps off at 1 gbps. \n",
      "\n",
      "Looking for any better options, i'm curious what you guys use. \n",
      "\n",
      "Requests can return anything from 50-500KB in JSON data, averaging probably the 300 range. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aw0loo/vps_provider_for_high_scale_scraping_project/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need Help for Web Scraping a Government Website\n",
      "Text: Hi! I have never tried web scraping before but this is my last resort since the website I need to download data from does not have API. What it has is its Quick Report Plugin which cannot accommodate my needs.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Basically, the website is [https://nsap.nfrdi.da.gov.ph/explorer-light](https://nsap.nfrdi.da.gov.ph/explorer-light) . It is where I can look up Fish Landings Data in the Philippines per Region, Province, Species, Gear, or a combination. The data I need is the **monthly catch data** in my province for the fish *Sardinella lemuru* under the family Clupeidae. The only way I can get this is from the Data Explorer>Catch Data By Species>Per Province>Specify Year, Species Family, and Species Name. This method would mean that I need to do this for 6 times to cover 6 years and manually copy each month's value since I can't export it.  \n",
      "\n",
      "\n",
      "Can anyone help me with this? I thought of web scraping but I don't have any experience on this. Go to the website if you want to try it for yourself.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1avg3s1/need_help_for_web_scraping_a_government_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: All in one solution for web scraping for devs?\n",
      "Text: Let’s say you’re a dev? Are there any frameworks available to start web scraping out of the box for technical users? (Taking scalability, configurability etc into account)\n",
      "\n",
      "I am currently building this tool, may go open source. \n",
      "\n",
      "Like how is great expectations for data quality but its uses libraries like pandas as its core probably. A web scraping framework that uses libraries like scrapy, beautiful soup etc? \n",
      "\n",
      "Anything like this available?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1avpqg0/all_in_one_solution_for_web_scraping_for_devs/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What are bots built with?\n",
      "Text: so I'm new to web scraping, and I found this list of bots:\n",
      "\n",
      "[https://radar.cloudflare.com/traffic/verified-bots](https://radar.cloudflare.com/traffic/verified-bots)\n",
      "\n",
      "I assume that not every bot has a dedicated team that builds them from scratch, so what kinds of frameworks/stacks do they use? Any place where I can read up on the systems design of people who have first hand experience building these?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1av8xwu/what_are_bots_built_with/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I scrape a dynamically loaded page blocking third party cookies?\n",
      "Text: I’m trying to scrape a fairly well known maritime website. They’re known for being tough to scrape, and obviously people that have figured it out will not publicly post how, so I’m determined to figure it out myself. \n",
      "\n",
      "I bought a premium account to see more details (the publicly available free details aren’t enough). But when I set my headers and cookies in my program, when it runs it returns “Third party cookies will be blocked.” And doesn’t end up scraping. Any advice on how to achieve this? \n",
      "\n",
      "I’m using C# with selenium & chromedriver. I’m open to python as well if there’s better packages there, but for me C# would be ideal\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1auznrg/how_can_i_scrape_a_dynamically_loaded_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: When i'm Scraping with Scrapy Sometimes i have a 200 and can access, sometimes i have a 403 and can't access\n",
      "Text: I'm quite new and I'm trying to scrape the website [https://fr.ra.co/](https://fr.ra.co/)\n",
      "\n",
      "My problem is the following: I succeeded yesterday to have a 200 response with faking user agent, but after some other tries I've been getting 403. The next day (today) the first time I tried it was showing 200, then after some tries it became 403, but then after some retries again 200, and now it says 403 and it looks definitive for at least the day. What could happen to produce that result?\n",
      "\n",
      "I have mostly followed this tutorial to begin: [https://scrapeops.io/python-scrapy-playbook/scrapy-403-unhandled-forbidden-error/](https://scrapeops.io/python-scrapy-playbook/scrapy-403-unhandled-forbidden-error/)\n",
      "\n",
      "Here what I've done in my setting.py\n",
      "\n",
      "    ROBOTSTXT_OBEY = False  DOWNLOADER_MIDDLEWARES = {     'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,     'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,     'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,     'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401, }  FAKEUSERAGENT_PROVIDERS = [     'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # This is the first provider we'll try     'scrapy_fake_useragent.providers.FakerProvider',  # If FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us     'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # Fall back to USER_AGENT value ]  USER_AGENT = '<Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36>'  \n",
      "\n",
      "And this is my header:\n",
      "\n",
      "      HEADERS = {     \"Accept\": \"*/*\",     \"Accept-Encoding\": \"gzip, deflate, br\",     \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",     \"Content-Length\": \"4122\",     \"Content-Type\": \"application/x-www-form-urlencoded\",     \"Origin\" : \"https://fr.ra.co\",     \"Referrer\" : \"https://fr.ra.co/\",     \"Sec-Ch-Ua\" : \"'Not A(Brand';v='99', 'Google Chrome';v='121', 'Chromium';v='121'\",     \"Sec-Ch-Ua-Mobile\" : \"?0\",     \"Sec-Ch-Ua-Platform\" : \"Windows\",     \"Sec-Fetch-Dest\" : \"empty\",     \"Sec-Fetch-Mode\": \"cors\",     \"Sec-Fetch-Site\": \"cross-site\",     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\" } \n",
      "\n",
      "And this is the response I have when I shell and request response.text  \n",
      ":\n",
      "\n",
      "    \"info\": {     \"version\": \"2\",     \"statusCode\": 403,     \"statusMessage\": \"\",     \"headers\": {       \"date\": \"Tue, 20 Feb 2024 06:31:24 GMT\",       \"content-type\": \"text/html;charset=utf-8\",       \"access-control-allow-origin\": \"*\",       \"cache-control\": \"max-age=0, private, no-cache, no-store, must-revalidate\",       \"set-cookie\": [         \"datadome=os6pDl~6tWinZAoaxnbXzIYtz_zu8pK_6kz~JZPZa~ieOXSiWcULss3_cjXiupa5~rlgrIh7OHy7XnQWKhyA7H0gmskyWCuT3Bclk8c9NlsnxabSXpnrQR9ScqcWlwIt; Max-Age=31536000; Domain=.ra.co; Path=/; Secure; SameSite=Lax\"       ],       \"accept-ch\": \"Sec-CH-UA,Sec-CH-UA-Mobile,Sec-CH-UA-Platform,Sec-CH-UA-Arch,Sec-CH-UA-Full-Version-List,Sec-CH-UA-Model,Sec-CH-Device-Memory\",       \"access-control-allow-credentials\": \"true\",       \"access-control-expose-headers\": \"x-dd-b, x-set-cookie\",       \"charset\": \"utf-8\",       \"pragma\": \"no-cache\",       \"x-datadome\": \"protected\",       \"x-datadome-cid\": \"AHrlqAAAAAMAx6PttWoNTCAAwMZ8vA==\",       \"vary\": \"Accept-Encoding\",       \"server\": \"cloudflare\",       \"cf-ray\": \"8584b4d86ac04bc7-BUF\",       \"content-encoding\": \"gzip\"     }   },   \"body\": \"<html><head><title>ra.co</title><script src=\\\"/cdn-cgi/apps/head/tp1ZHJ6G4oyYkj6qAewQ1BNJKD4.js\\\"></script><style>#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}</style></head><body style=\\\"margin:0\\\"><p id=\\\"cmsg\\\">Please enable JS and disable any ad blocker</p><script data-cfasync=\\\"false\\\">var dd={'rt':'c','cid':'AHrlqAAAAAMAx6PttWoNTCAAwMZ8vA==','hsh':'107A2F9ACF118F5EFF46550CD47084','t':'fe','s':41462,'e':'9002aa6e033fb2de28bfcfd7685eb8bda998528879bf79b1a51783b0125365db','host':'geo.captcha-delivery.com'}</script><script data-cfasync=\\\"false\\\" src=\\\"https://ct.captcha-delivery.com/c.js\\\"></script></body></html>\\n\",   \"extractor\": {     \"result\": {       \"items\": []     }   } } \n",
      "\n",
      "So I set ROBOTTXT  \n",
      " to false and use Scrapy-Fake-Useragent and I also use my own User\\_Agent.\n",
      "\n",
      "I doubt it could come from an IP ban simply because with my computer, I can access the website. (And I'm not sure I understand why I could get 404 if I use the same IP and user agent as my computer and figured out it works on my computer.)\n",
      "\n",
      "Using [https://checkforcloudflare.selesti.com/](https://checkforcloudflare.selesti.com/), it shows me that the website is using Cloudfare, do you think it's where I should have a look?\n",
      "\n",
      "I tried to modifiy the user agent, to set a header (but I think the Scrapy-Fake-Useragent already do that work?) to modify the delay time, to disable the robot.txt  \n",
      ".\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1avbunw/when_im_scraping_with_scrapy_sometimes_i_have_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping with selenium and pyautogui\n",
      "Text: Sometimes, it just seems like the simplest solution would just be to take a screenshot, find the element I want based on pixels, have my pointer move to that location, and click.  Obviously, this will make my scraper run way slower and be heavily platform dependent, but for my purposes, I don't care.  The issue is that if I use pyautogui, I can't use my computer for anything else while scraping, because it will interfere with screenshotting and cursor movement. Does anyone have or know of a solution to this?  I'd prefer to avoid any kind of containerization or pushing to the cloud, because that will come with its own issues. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1auz80u/scraping_with_selenium_and_pyautogui/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: TruepeopleSearch.com scrapper\n",
      "Text: I am developing a [truepeoplesearch.com](https://truepeoplesearch.com) scrapper using Python selenium but facing issues with the captcha and tried everything to bypass it like undetected-chromedriver and selenium-stealth, but none is working.   \n",
      "If someone has a solution for this, I would appreciate the help. And if there is no way around it, then where I can buy residential proxies to integrate into bot.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1av2ux5/truepeoplesearchcom_scrapper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Auto-scrape web with given words by automatically going into each nested links?\n",
      "Text: Thinking of scraping all data in particular news article websites with a given word by automatically going into each nested links. For instance, if I want to scrape data about 'COVID-19', I want the automated system to go into each nested links (ie. New York Times) and output each URL of the news articles that has the word 'COVID-19' in the article. Is this even possible?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1auwem0/autoscrape_web_with_given_words_by_automatically/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: No-code Web Scraping (real-time too!)\n",
      "Text: Am I the only one who is often in the same situation?  \n",
      "\n",
      "\n",
      "\\`\\`\\`\n",
      "\n",
      "I want to find/copy some information from public websites. For example from a Notion database or a private (authenticated) dashboard, or whenever from the internet. I want to paste that info/values into my code/excel/pdf/website and have them update in real time. Something similar to variables and integrated APIs, a developer would say.      \n",
      "\\`\\`\\`\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Yet I haven't found a reliable solution. Usually, most web scraping solutions (even no code tools) only allow me to send the data to a Google Sheet or Airtable. I want to copy data from anywhere on the internet and use them wherever I need to, in a no-code way. *Any suggestions? Have you faced similar problems?*  \n",
      "\n",
      "\n",
      "Thank you in advance!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1auw4h3/nocode_web_scraping_realtime_too/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Clicking ALL Buttons in Web Page\n",
      "Text: Hi all,\n",
      "\n",
      "I am doing this scaping project where I am trying to get all text in a given web page. The way I am trying to do this is using Playwright package and BeautifulSoap in Python. Firstly,  I use BeautifulSoap as follows:\n",
      "\n",
      "    soup = BeautifulSoap(html_content, 'html_parser')\n",
      "    text1 = soup.find_all('p')\n",
      "    text2 = soup.find_all('span')\n",
      "\n",
      "These obtain the <p> tag and <span> tag text. However, there are text hidden under <button> tags, and the way to do this - as far as I understand - is to click the  buttons and then use `soup.find_all()` method to get the text. For this, I am doing the following commands:\n",
      "\n",
      "    elements = page.locator(\"button\")\n",
      "    n = elements.count()\n",
      "    for i in range(n):\n",
      "       elements.nth(i).click()\n",
      "\n",
      "However, when executed, it takes long time before it throws a runtime error due to exceed time.  I tried different `timeout`values but to no avail. Would you please help in this?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Many thanks \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1auv63q/clicking_all_buttons_in_web_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Chrome's \"Copy as cURL\"\n",
      "Text: I was wondering if anyone knows if Chrome's \"Copy as cURL\" command generates exact same copy of the HTTP request made in the browser? Are there any differences that could make browser request fail, but cURL request work?\n",
      "\n",
      "Here's a bit of background of what I'm trying to achieve.\n",
      "\n",
      "I'm writing a scraper for [https://www.toyota.com/owners/vehicle-specification/](https://www.toyota.com/owners/vehicle-specification/) website to pick up vehicle specs for the given VIN. I did manage to make my script log in without problem, but requests to fetch vehicle specs fail with `Cross-Origin Resource Sharing error: PreflightMissingAllowOriginHeader`. Interestingly if I pause the scripts execution, use \"Copy as cURL\" command on the same failed request and run it in the terminal, I get 200 response with the data.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aut744/chromes_copy_as_curl/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ts it possible to parse all website on google for email address with python?\n",
      "Text: I don't want to go one by one to website and scrap emails. How is it real to parse through google for specific website and only then scrap email.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1auvvax/ts_it_possible_to_parse_all_website_on_google_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any idea how I could scrap a list on that page?\n",
      "Text: I'd like to scrap all items from that list page: https://debank.com/proposal \n",
      "\n",
      "I've tried multiples alternatives on python scripts, I've tested with those libraries approach already:   \n",
      "\\- BeautifulSoup: it struggles with JavaScript-rendered content.\n",
      "\n",
      "\\- HTMLSession (from requests\\_html): also didn't work\n",
      "\n",
      "\\- Requests: same\n",
      "\n",
      "\\- Selenium WebDriver: I had issues trying locally, I use google chrome 121.0.6167.184, and it seems there is no version of ChromeDriver for that, only up to 114 I guess, also I don't think it would work on heroku.  \n",
      "\n",
      "\n",
      "Since I will deploy it on heroku, it shouldn't need my manual action to do something manually on each interaction. Does anyone know what I should do for that?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aupome/any_idea_how_i_could_scrap_a_list_on_that_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How not to get banned scraping large amounts of data.\n",
      "Text: I have created a selenium script that scrapes every property of rightmove.com and information for each one. For a bussiness I am creating I ideally want to update this every day. There should be around 800,000 properties. I would need to have around 5 instances of my script running at the same time. When testing my scraper i have not been banned yet. How could I do this and not get IP banned. when running this script pretty much 24/7. I am planning to run it on a vps.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1auk4mx/how_not_to_get_banned_scraping_large_amounts_of/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Anyone own an online business that uses web scraping?\n",
      "Text: Just curious about the potential costs if I were to create a website to display the data I’m scraping. I’m currently programming something to use for myself as it has made me a significant amount of income over the past couple weeks, but thought I might be able to sell it as a monthly subscription to other people. \n",
      "\n",
      "Note: The data I’m scraping must be displayed to users as close to instant as possible. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1auau4x/anyone_own_an_online_business_that_uses_web/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Online Course\n",
      "Text: Hi all.\n",
      "\n",
      "I'm hoping someone could kindly help.\n",
      "\n",
      "I have paid 2.5k for an online course alongside my employment, and after requesting a PDF copy to make it easier for me to study, it was declined therefore I'm having to read a prominently text-based course from a screen (I'm a bit old fashioned). Although there are several video based links this is not important.\n",
      "\n",
      "Now, forgive me as I don't really understand scraping, yet I was advised that it might be an option. Would anyone please point me in the right direction and offer some advice whether scraping is suitable to extract all text and images to a PDF?\n",
      "\n",
      "Maybe I'm asking in the wrong place so apologies if I am.\n",
      "\n",
      "Thank you in advance \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1au9bw0/online_course/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Subreddit scraping help\n",
      "Text: Hello everyone, I need to scrape all subreddits related to tea for a  research project. Initially, I used PRAW but discovered that the  submission limit cannot exceed 1,000. I intended to download data from  this [source](https://academictorrents.com/details/9c263fc85366c1ef8f5bb9da0203f4c8c8db75f4), but it is too large for my computer, and I'm not sure if it contains the subreddits I want either. I also don't know how  to use a virtual machine to process it. Can anybody help me retrieve all  the data from /tea and /puer, starting from January 1, 2023, up to the  present, in the following format.\n",
      "\n",
      " 'Post ID': post.id,  \n",
      " 'Post Title': post.title,  \n",
      " 'Post Content': post.selftext,  \n",
      " 'Post Date': post\\_date.strftime('%Y-%m-%d %H:%M:%S'),  \n",
      " 'Comment ID': comment.id,  \n",
      " 'Comment Body': comment.body,  \n",
      " 'Comment Date': comment.strftime('%Y-%m-%d %H:%M:%S')\n",
      "\n",
      "Thanks a ton!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aub894/subreddit_scraping_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: scrapy only gives the proper output sometimes\n",
      "Text: i am trying to scrape old.reddit.com videos and i am not sure what could be causing the inconsistency.\n",
      "\n",
      "my xpath:\n",
      "\n",
      "    //a[@data-event-action='thumbnail']/@href\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1au0c6h/scrapy_only_gives_the_proper_output_sometimes/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Fun scraping ideas\n",
      "Text: Hello,  \n",
      "\n",
      "\n",
      "I am doing a course in social data science, and I have an assignment in web scraping. However, there is full freedom as to what to scrape and visualize (I could do the weight and height of tour de france cyclists or inflation levels in different countries). Do you have any fun and creative ideas? I am getting a bit lost in the freedom, hehe..   \n",
      "\n",
      "\n",
      "Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1au4fbp/fun_scraping_ideas/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Puppeteer extra detected by cloudflare \n",
      "Text: I am using puppeteer extra with puppeteer-extra-plugin-stealth but I get detected by cloudflare even when I set up the user agent and some other args i keep getting to the cloudflare page and I tried to check the human input but it keeps redirecting to the cloudflare iframe. Is there a solution for that ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1atbb5n/puppeteer_extra_detected_by_cloudflare/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trying to scrape website text gives only a tiny bit of data\n",
      "Text: I'm trying to get text from the website ligand.com\n",
      "\n",
      "When I use\n",
      "\n",
      "    import urllib.request as urllib2\n",
      "    req = urllib2.Request(link, headers=hdr)with urllib2.urlopen(req) as response:rawtext = response.read()\n",
      "    \n",
      "    \n",
      "\n",
      "All I get is\n",
      "\n",
      "    b'<html><head><link rel=\"icon\" href=\"data:;\"><meta http-equiv=\"refresh\" content=\"0;/.well-known/sgcaptcha/?r=%2F&y=ipc:174.81.32.128:1708229047.782\"></meta></head></html>'\n",
      "\n",
      "rather than the page html. What is going on here?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1atmc0b/trying_to_scrape_website_text_gives_only_a_tiny/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trying to scrape website text gives only a tiny bit of data\n",
      "Text: I'm trying to get text from the website ligand.com\n",
      "\n",
      "When I use\n",
      "\n",
      "    import urllib.request as urllib2\n",
      "    req = urllib2.Request(link, headers=hdr)with urllib2.urlopen(req) as response:rawtext = response.read()\n",
      "    \n",
      "    \n",
      "\n",
      "All I get is\n",
      "\n",
      "    b'<html><head><link rel=\"icon\" href=\"data:;\"><meta http-equiv=\"refresh\" content=\"0;/.well-known/sgcaptcha/?r=%2F&y=ipc:174.81.32.128:1708229047.782\"></meta></head></html>'\n",
      "\n",
      "rather than the page html. What is going on here?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1atmc08/trying_to_scrape_website_text_gives_only_a_tiny/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to tackle elements that change its id periodically.\n",
      "Text: I've noticed some elements that changes its ids in a certain period of time, to avoid easy scraping i guess. What is the best approach to deal with this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1atf8if/how_to_tackle_elements_that_change_its_id/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What modules should I add to an open source project that combines headless browsing with LLMs, without reinventing the wheel?\n",
      "Text: Hi,   \n",
      "I have been building a project that helps you do any browser automation, scraping, or functional testing tasks with the help of combining headless browsing with LLMs that can identify the UI elements of a webpage and make actions on top of it. Currently the project has only done an early release, but I think this one could have several modules that makes is easy for a web scraping user to have a customised LLM agent in their web scraping tasks. What modules should I further add, as the current landscape of LLMs  and scraping has a lot of new components in many popular projects?   \n",
      "\n",
      "\n",
      "  \n",
      "Here is the project link and I have explained pretty much everything I am doing in the project readme and description: [https://github.com/kindsmiles/pyvigate](https://github.com/kindsmiles/pyvigate)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1asydcd/what_modules_should_i_add_to_an_open_source/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping / Automation with user creds\n",
      "Text: Hi I am working on an automation / scraping tool and i need help with what i am trying to accomplish. I plan to allow others users to use my service. The service I am building requires login to third party websites. The websites utilize regular email pass login and the normal signin with google, microsoft etc.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "My first thought was to just store hashed or encrypted user creds in db and use that way. Is there a way i can use oauth or something else to do the same thing? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1askbj8/scraping_automation_with_user_creds/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to increase reliability?\n",
      "Text: I have a fairly basic python script using BeautifulSoup and requests that is grabbing a small amount of public data from a large archive, but it always fails after about 20-30 successful grabs. I wait a while (like an hour), try again, and it gets a few more. it seems like maybe there's some sort of protection on the server end, or maybe there's something else going on? I've tried using tenacity, but it seems like I have to wait too long before retries for it to be useful.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ase618/how_to_increase_reliability/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape Twitter comments\n",
      "Text: How can I scrape 20000 twitter comments from a post? Do you know any tools? I'm having problems using API\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1as9dly/how_to_scrape_twitter_comments/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Accessing node inside particular div that might occur elsewhere with same attribute value. How to narrow find_element to particular region\n",
      "Text: How to select specific nodes under specific div. I want a pointer to the `img` node under `div` with `class=level2` in the following html. The whole html is here\\[1\\]\n",
      "\n",
      "Why `level2_div.find_element()` keeps points to img present in parent node. How to narrow the focus of find\\_element to nodes under its content?\n",
      "\n",
      "    BLUEBOX = '//div[@class=\"bluebox\"]'\n",
      "    COLLAPSE_BTN = '//div[@class=\"collapse-arrow\"]/img]'\n",
      "    LEVEL2_DIVS = '//div[@class=\"level2\"]'\n",
      "    LEVEL3_DIVS = '//div[@class=\"level3\"]'\n",
      "    \n",
      "    # for purpose of this question the html contains only one bluebox div,\n",
      "    # but actually there are multiple hence level1_divs\n",
      "    level1_divs = driver.find_elements(By.XPATH, BLUEBOX)\n",
      "    for i in range(len(level1_divs)):\n",
      "          level1_divs = driver.find_elements(By.XPATH, BLUEBOX)\n",
      "          level1_div = level1_divs[0]\n",
      "    \n",
      "          button = level1_div.find_element(By.XPATH, COLLAPSE_BTN)\n",
      "          button.click() #<<< this will populate the level2 divs\n",
      "    \n",
      "          level2_divs = level1_div.find_elements(By.XPATH, LEVEL2_DIVS)\n",
      "          for j in range(len(level2_divs)):\n",
      "                level2_divs = level1_div.find_elements(By.XPATH, LEVEL2_DIVS)\n",
      "                level2_div = level2_divs[j]\n",
      "    \n",
      "                try:\n",
      "                      #this always points to collapse img under level1_div even\n",
      "                      #though i use level2_div as the starting point. Why?\n",
      "                      button = level2_div.find_element(By.XPATH, COLLAPSE_BTN)  # <<<<<<<<<\n",
      "                      button.click()\n",
      "    \n",
      "                      level3_divs = level2_div.find_elements(By.XPATH, LEVEL3_DIVS)\n",
      "                      for k in range(len(level3_divs)):\n",
      "                            level3_divs = level2_div.find_elements(By.XPATH, LEVEL3_DIVS)\n",
      "                            level3_div = level3_divs[k]\n",
      "    \n",
      "                            process_link(level3_div)\n",
      "                except:\n",
      "                      process_link(level3_div)  #process link under <div class=\"link\"></div>\n",
      "    \n",
      "\n",
      "Example HTML fragment\n",
      "\n",
      "      <div class=\"bluebox\">\n",
      "          <div class=\"\">\n",
      "            <div class=\"collapse-arrow\">\n",
      "              <img src=\"collapse.gif\">\n",
      "            </div>\n",
      "            <div class=\"link\"> <a href=\"A01\">A01</a> </div>\n",
      "          </div>\n",
      "          <div class=\"clearfix\">\n",
      "            <p>  Highest level text</p>\n",
      "          </div>\n",
      "          <div class=\"additional_links\">\n",
      "            <div class=\"level2\">\n",
      "              <div>\n",
      "                <div class=\"collapse-arrow\">\n",
      "                  <img src=\"collapse.gif\">                 <<<<<<< This one \n",
      "                </div>\n",
      "                <div class=\"link\"><a href=\"A01_0.txt\">A01_0</a></div>\n",
      "                <div class=\"additional_links\">\n",
      "                  <div class=\"level3\">\n",
      "                    <div>\n",
      "                      <div class=\"collapse-arrow\">\n",
      "                      </div>\n",
      "                      <div class=\"link\"><a href=\"A01_00.txt\">A01_00</a></div>\n",
      "                      <div class=\"clearfix\">\n",
      "                        <div>\n",
      "                          <p> second level text </p>\n",
      "                        </div>\n",
      "                        <div>\n",
      "                        </div>\n",
      "                      </div>\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\[1\\] [https://pastebin.com/XfmLkHva](https://pastebin.com/XfmLkHva) \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1as7vzc/accessing_node_inside_particular_div_that_might/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Hook post server endpoint to send updates\n",
      "Text: How can I establish persistent connection with a server endpoint using a POST request to receive updates? I don’t want to send repeated request to see if any new update has come, instead I want the server to automatically push me updates. I played with sse and web sockets, but connection always end with a single request/response. If there’s anyone who knows any advance technique make server send updates automatically to me when it comes instead of me making multiple post request to them. Some info: The endpoint is powered by PHP and Plesklin. Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1as7q62/hook_post_server_endpoint_to_send_updates/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Resources for fictional news articles for scraping \n",
      "Text: Hi,\n",
      "\n",
      "I am working on a project for my dissertation involves webscraping. I am looking for resources to get URLs to news articles (fictional or none fictional) that allow scraping. Topics should rage from political to opinion pieces.\n",
      "\n",
      "Whenever I find recourses they are always for finance but I just want to find websites that allow scraping for news articles. I just need to get the URL (you might be able to tell this is stressing me out).\n",
      "\n",
      "Thanks in advance for any help. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ary6b2/resources_for_fictional_news_articles_for_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Legally Scraping from Ebay\n",
      "Text: If I were to scrape the price and date of sold items from Ebay, does that violate Ebay's scraping policy? I just want it for a personal project. Thank you\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1arwajk/legally_scraping_from_ebay/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Hi everyone , I am trying to figure out how to webscrape from the redirected links .\n",
      "Text: So the page i want to webscrape [https://www.searchfunder.com/deal/exchange](https://www.searchfunder.com/deal/exchange) page but i think we need to first pass the login through [https://www.searchfunder.com](https://www.searchfunder.com/deal/exchange) and pass through some initial redirected links and some how move to [https://www.searchfunder.com/deal/exchange](https://www.searchfunder.com/deal/exchange) page and get the full grown html of it.  \n",
      "\n",
      "\n",
      "So i have figured this much out that i need to use request\\_html to render the page/deal/exchange as this page does dynamic js loading. But the current task is how to land to this page first and then scrape ????  \n",
      "\n",
      "\n",
      "Please let me know of any tryouts i can do\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1arl45x/hi_everyone_i_am_trying_to_figure_out_how_to/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Twitter scraping\n",
      "Text: Hi, does anyone know how the find  URLs of videos on X/Twitter? I'm trying to scrape some tweets but I can't find video links in their code using XPath.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1arhtqx/twitter_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Bun for web scraping? \n",
      "Text: I always use puppeteer for scrap alternative websites, but I feel to slow about it, do you think bun makes it faster or it's not gonna be so different?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aqxh2s/bun_for_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Text classifier built in scraper\n",
      "Text: Hi Everyone! \n",
      "\n",
      "I have used web scraping only once, but need to use it for my research again. I’m going to scrape public channels in Telegram (I need only posts’ texts and date), but the problem is I do not need all posts from these channels. What I need is only posts about one specific topic. So my main question is: is it possible to build in the text classifier in scraper? So the scraper goes to the channel, scrapes it, identifies if the topic is relevant or no, and puts into csv table only those posts that are on a specific topic. Is it possible to do? And if yes, any advices on it would be extremely appreciated. \n",
      "Thank you so much in advance ❤️\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aqqaob/text_classifier_built_in_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Playwright or Puppeteer?\n",
      "Text: Hey. I'm in a situation where I have to choose either Puppeteer or Playwright. I'm interested in nothing else but maximum efficiency and stability, knowing that my scripts take hours/days to finish.\n",
      "\n",
      "Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aql654/playwright_or_puppeteer/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trying to scrape some manga but the image are JS and shuffled\n",
      "Text: Trying to scrape some manga (https://mangareader.to/read/akira-46/en/chapter-1) but using something like JDownloader, I get the images shuffled into a grid. Python script I used doesn't seem to parse any images. Using inspector I found the images follow a like this (an example of one of the pages):\n",
      "\n",
      "<div class=\"iv-card shuffled loaded\" data-url=\"https://c-1.mreadercdn.com/_v2/0/0dcb8f9eaacfd940603bd75c7c152919c72e45517dcfb1087df215e3be94206cfdf45f64815888ea0749af4c0ae5636fabea0abab8c2e938ab3ad7367e9bfa52/2c/65/2c65696bd8504c362d8b01db021b15b2/2c65696bd8504c362d8b01db021b15b2_1100.jpeg?t=515363393022bbd440b0b7d9918f291a&amp;ttl=1908547557\">\n",
      "                    <div class=\"card-loading\">\n",
      "                        <div class=\"c-l-area\">\n",
      "                            <div class=\"paper-loading\"></div>\n",
      "                            <p class=\"mb-0\">Loading...</p>\n",
      "                        </div>\n",
      "                    </div>\n",
      "                <canvas width=\"727\" height=\"1100\" class=\"image-vertical\"></canvas></div>\n",
      "\n",
      "Any help?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aqom9m/trying_to_scrape_some_manga_but_the_image_are_js/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Instagram hashtags\n",
      "Text: Hey I'm very new to this, is it possible to scrape the number of times a specific hashtag has been used? How hard/ easy is it to set up? What tools are the best to use to use?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aqh3a7/instagram_hashtags/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What to do with 503 code?\n",
      "Text: Hey all! I'm kinda new to web scraping. I need to scrape data from millions of URLs. But when I start making the requests, I sometimes get a 503. I know for one that 429 is the code for being rate limited, but what's up with 503? What is its purpose and what do I need to do with that? Should I wait for sometime and request the same URL again. Or is the URL broken? Also, is it an error on my end or the server's end? Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aqf4qc/what_to_do_with_503_code/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web scraping multiple sites\n",
      "Text: I’m trying to develop a scraper that takes in a link of news site and returns a list of article urls for articles present in the homepage. It should be able to handle a variety of sites so I can’t hardcode specific html structures. \n",
      "\n",
      "The approach I’m thinking of is extracting all links from the html then excluding any links that are present in the header/footer or point to external sites. This approach was successful in eliminating a lot of links but there are still some false positives. (For example, not all websites have a header/footer so I still get links from there and sometimes there are footer links outside the footer structure) Does anyone know how I can exclude other links or has a better approach?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aqamdb/web_scraping_multiple_sites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Low Level Useful?\n",
      "Text: Hey I was just curious whether low level languages are useful to you in any specific scenario when you are scraping a huge volume?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aq9o86/low_level_useful/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web Scraping Instagram simulating the official app\n",
      "Text: I am developing software through which, after users log in to Instagram and obtain a bearer token enabling private calls to Instagram, they can synchronize their Instagram data.\n",
      "\n",
      "This synchronization procedure consists of a series of consecutive API requests to Instagram: obtaining user info (username, fullname, etc.), obtaining followers, obtaining following, and obtaining posts. I have replicated the requests exactly as they are made in the official app down to the smallest detail.\n",
      "\n",
      "I need this procedure to be performed by the user as many times as they want without having to re-login each time to obtain a new token.\n",
      "\n",
      "The issue is that every 2-3 synchronizations, Instagram responds with a 'challenge required' error due to suspected scraping, forcing the user to log in again and effectively invalidating the bearer token. I have also tried introducing a delay between each request, but the result is the same.\n",
      "\n",
      "I suspect that some requests made by the official app allow the token not to be invalidated because they are considered 'secure' and 'not-scraping' , but I cannot figure out which ones.\n",
      "\n",
      "Does anyone have any information on this that could be helpful? Does anyone have an idea of how Instagram identifies certain API requests as suspicious scraping, leading to the invalidation of the session?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1apvp2q/web_scraping_instagram_simulating_the_official_app/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping with the power of GPT\n",
      "Text: Hey guys before a couple of months with working on a project one of the stages i needed a scraper to scraper a number of websites and to return a certain JSON format always. I didn't want to do it manually because its boring after looking for a lot of solutions I came up with this solution using OpenAI, instructor, Pydantic.  \n",
      "Give me your suggestion and how can i improve this project, any suggestion will be helpful.  \n",
      "[https://github.com/Mamdouh66/Scraper-GPT](https://github.com/Mamdouh66/Scraper-GPT)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aq0bwc/scraping_with_the_power_of_gpt/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Capture Entire Website with Screenshots ?\n",
      "Text: Hi ! I'm trying to screenshot all the pages of a website, but there are so many pages that it's taking me forever to do a web capture one by one. Is there any way to extract these pages quicker ?\n",
      "\n",
      "I'm open to paying for it if there's a good website that does this kind of stuff....\n",
      "\n",
      "Any help is greatly appreciated !\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1appmrg/capture_entire_website_with_screenshots/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How is your setup?\n",
      "Text: I have taken upon myself (the job want to save money by doing it inhouse) to setup a scraping bot(s) scraping our three main competitors. I am using Python with requests (JSON) and Selenium to load Javascript(html, CSS selectors) and save it all to csv files. I have tried bot Scraping Bee and Oxylabs proxies and it seems Oxylabs are faster (For my geolocation maybe). Then some Python scripts for merging, backup and calculations. I am gonna run all on an local windows PC with windows scheduler for the scripts and powerautomate to copy the files to Sharepoint and give an small report on a dedicated channel on Teams.\n",
      "\n",
      "This seems to me as an 'easy' way out. Maybe in the future it would involve for example an AWS/azure backend to handle compute, storage and an sql database. (Maybe an SSAS cube to PowerBI)..\n",
      "\n",
      "My projectfolder looks kinda like this:\n",
      "\n",
      "..backup\n",
      "..csv\n",
      "..log\n",
      "..utils\n",
      "..credentials\n",
      "ScrapeStore1.py\n",
      "ScrapeStore2.py\n",
      "ScrapeStore3.py\n",
      "\n",
      "\n",
      "Some questions I have..\n",
      "1. What are your favorite proxyrotators?\n",
      "2. Any good Utils? (Like \"I\" have programmed (thanks copilot) an script that fetches the URLs from the sitemaps in robots.txt that works pretty nicely)\n",
      "3. Worth building an GUI for parameters? (Sleep time, specefic categories, etc)\n",
      "4. How do you setup your infrastructure from scraping to delivery to visuals?\n",
      "5. How do you setup your project folder?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1apo3g8/how_is_your_setup/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: learning Web scraping\n",
      "Text:  \n",
      "\n",
      "Any recommendations for courses to learn web scraping using Python for data analysis?  \n",
      " \n",
      "\n",
      "Can I master the skill in one month? I am really good with Python, and I have an IT background.  \n",
      " \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1apqzws/learning_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is it Normal for a Python Selenium Web Scraper to Take 4 Days for 40k Pages?\n",
      "Text:  Hey everyone,\n",
      "\n",
      "I recently ordered a web scraper built with Python and Selenium, and I've been running it to extract data from approximately 40,000 pages. However, I've noticed that it's taking around 4 days to complete the extraction process, and I need to keep my PC always on during this time.\n",
      "\n",
      "I'm wondering if this is normal for this sort of script to run for such a long duration. Is there any way to optimize it or speed up the process? Alternatively, would switching to a tool like Octoparse be more efficient?\n",
      "\n",
      "I'm relatively new to web scraping, so any insights or advice would be greatly appreciated. Thanks in advance for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ap5tah/is_it_normal_for_a_python_selenium_web_scraper_to/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Enterprise Web Scraping: What to look out for\n",
      "Text: Hello r/webscraping,\n",
      "\n",
      "I see a lot of similar questions on this subreddit and thought I would add my 2 cents and try and cover a lot of the pitfalls I see when people start trying to scrape at scale. If you're asking the question \"how do I scrape 100 million pages in a month that run javascript/keeps blocking me/will be maintainable long term\", this guide might be for you.\n",
      "\n",
      "# Context\n",
      "\n",
      "I'm a Senior Engineer who has specialized in specifically web automation for a few years now. I currently oversee about \\~100 million requests a month and lead a small team in my endeavors. I've had the chance to research and implement most current tooling and hope to provide folks here with the most information I possibly can (while trying to stay inside the sub's rules 😃). This \"guide\" will mostly cover high-levels of requests, Websites that utilize Javascript, and bot detection (as these are what I have the most experience dealing with).\n",
      "\n",
      "# Tech Stack\n",
      "\n",
      "There is a multitude of different options, but the ones I typically shoot for on a project are:\n",
      "\n",
      "\\- Typescript\n",
      "\n",
      "\\- Puppeteer (or puppeteer-extra depending)\n",
      "\n",
      "\\- AWS (SQS, RDS, EC2)\n",
      "\n",
      "# Proxies\n",
      "\n",
      "Proxies mask your origin IP address from the website. These are EXTREMELY important if you plan to make a bunch of requests to one site (or multiple). There are a bunch of proxy services that are fine to use, but they all have their downsides, unfortunately. If you have to cover a bunch of requests to a bunch of websites, and there is a chance they are blocking IPs or verifying the credibility of the IP through some online flagging database, then I would recommend going with a larger, more credible proxy service. The goal is to have clean and fast proxies. If they aren't clean, you can easily get blocked. If they aren't fast, they will increase your infra pricing and possibly cause your jobs to fail. I typically use services that have an IP pool in the millions and utilize a few at a time in case of outages or an uptick in failures.\n",
      "\n",
      "# Captchas\n",
      "\n",
      "The ultimate robot stopper.... not. There are a ton of captcha-solving services on the market that you can just pay for API usage and never have to worry about again. Pricing and speeds vary. I've found that AI-based solvers are the best *sometimes*. AI solvers are the fastest and the cheapest, but the best ones I've used can't solve every kind of captcha (IIRC HCaptchas are the problem), so if you're solving for multiple sites, you may need a few different solutions. I'd recommend this anyway because if there is ever an outage (which does occur when there are captcha updates), then you have a backup for when jobs start failing. A little extra code will automatically switch over services when stuff starts failing 😃\n",
      "\n",
      "# Browsers\n",
      "\n",
      "The one thing that probably matters the most when interacting with bot detection at scale. These solutions are somewhat new to the market. I've even made my own in some cases, and this is probably the one thing that I don't see mentioned frequently (if at all?) on this sub. There is a bunch of cool browser tooling out there that have their particular use cases. Some are licensed out containers, some are connection-based. That being said, they all do a somewhat similar job. Introduce entropy into the browser and mask the CDP connections to the browser. When interacting with the browser via a script (and technically without), there a leaks everywhere that make it easy for big bot solutions to figure out what's up. There's simple stuff that can be fixed with the scraping libs out there (user agents, etc), but there is also stuff like canvas/webgl fingerprinting that isn't as fixable with these libraries. Most large-scale bot detection tools use quite a few fingerprinting tools that get quite in-depth. I would not recommend trying to tackle these solutions solo if you don't have years to spend doing research and learning about the nuances of the space.\n",
      "\n",
      "# Infra\n",
      "\n",
      "I've only found AWS to be \"the one\" in terms of being able to scale up to a level that I require. Sorry if this breaks rule 2, but this is what I've used and seen success with. Other solutions are going to be difficult to maintain and develop long term. I specifically utilize EC2/ECS for the scraping portion because tooling like Lamda/Fargate (although cheaper) doesn't offer the privileges that more \"aggressive\" scraping might require.has\n",
      "\n",
      "# Clustering\n",
      "\n",
      "A must when trying to achieve millions of jobs a month. My solution for this is at a few different levels. Node has some built-in packages that allow for clustering which is great for maximizing machine usage and optimizing scale costs. Next would be utilizing ASGs in AWS to scale up the number of machines we are using. After that, we would accept requests from a queuing service) doesn't offer the privileges that more \"aggressive\" scraping might require.\n",
      "\n",
      "# Queuing\n",
      "\n",
      "Queuing is great for this stuff. Jobs take an unknown amount of time and can run extremely long if there is an outage somewhere. I  would recommend this all day and if you don't currently have a queue for your jobs and you are looking to scale, do it.\n",
      "\n",
      "# Retries\n",
      "\n",
      "Failures are inevitable, but you don't have to let all that precious data getaway. If you want to do this at scale, we need to determine if a job has failed and have a system in place for getting that data again. This is where queuing is important. Having tooling where you know if something has failed and being able to add it back into the queue is so important at a large scale that I shouldn't even have to mention it. Don't forget this.\n",
      "\n",
      "# Cost Savings\n",
      "\n",
      "There are tons of places for you to save money on this. Negotiating infra, captcha, browser, and proxy costs down to understanding every single request you make. Proxies can get expensive. There is great tooling in Puppeteer (extra?) that lets you manage each request and even bypass your proxy and download it straight to you. I would say just make sure if you do this, know which requests your allowing, and which you are letting bypass or you could run into some issues. Essentially, we should look to optimize to have the least amount of requests, and the least amount of data downloaded as possible without jeopardizing our identity.\n",
      "\n",
      "# Metrics\n",
      "\n",
      "It's easy to see if your scripts are working locally, but sometimes not everything is as easy in the cloud. This is one of the most important things if you plan to scale is understanding your requests. Please, please, please utilize reporting tools so you know that the data that you are getting is correct and is coming in at the size that you need. There are no ifs, ands, or buts. Especially if you are dealing with clients on your project.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "There are a ton of variables in large-scale web scraping that need to be accounted for. Bot detection, rising costs, and cumbersome tooling are just a few you WILL encounter. I wish you the best of luck in your endeavors and hope this guide provided a little guidance into where you should start looking or continue your journey.\n",
      "\n",
      "# P.S. some useful open-source docs\n",
      "\n",
      "[Puppeteer-extra](https://github.com/berstend/puppeteer-extra)\n",
      "\n",
      "[Dark Knowledge](https://github.com/prescience-data/dark-knowledge)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aosgks/enterprise_web_scraping_what_to_look_out_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Chipotle Menu Price Web Scraping\n",
      "Text: Hello All!\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I am new to this subreddit and I recently thought of an idea that I would like to explore. I want to gather Chipotle (and maybe other restaurants) data on menu prices and their corresponding rewards point values in order to see how much value one can get by redeeming various rewards. Ideally, the web/mobile app would update and values would change as well. I was also thinking this could be scaled and expanded to a point where users could check close-by locations and see which locations have better prices and/or rewards exchange values.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I recently came across this [GitHub Repo](https://github.com/stets/chipotle_scraping_demo) where there is code to search by location and find Chipotles near you, but I wasn't able to tell if I could get the specific data I am looking for just from the code in this repo. There is a [Youtube Video](https://www.youtube.com/watch?v=ZFoQleFUH9Y) here as well for reference if needed.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Any help would be greatly appreciated. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1apfdcj/chipotle_menu_price_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Suggestion for Httpx/Aiohttp based web scraping framework for Python\n",
      "Text: Hi folks,\n",
      "\n",
      "Have You come across framework as mature as Scrapy based on Httpx/Aiohttp?\n",
      "\n",
      "Scrapy’s core is twisted. Architecture is great. Pipelines. Middleware specially.\n",
      "\n",
      "Thank You\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1apd0ut/suggestion_for_httpxaiohttp_based_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping ad websites for commercial use\n",
      "Text: Could I actually be sued for scraping commercial advertisement website data from multiple websites, aggregating it, repackaging it in a different, more convenient display format, with linking back to the original websites, slapping an ad banner on and hosting it as a public  website?\n",
      "\n",
      "Any cases of this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ap981p/scraping_ad_websites_for_commercial_use/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Gold Standard Enterprise Proxy Provider?\n",
      "Text: For those that have experience with large scale scraping operations (millions of requests per month), what specific companies or services do you use? There are a few I’ve used such as BrightData, OxyLabs, and IPRoyal, but is there a clear gold standard in your mind for what the best service is at a large scale?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ap2hq1/gold_standard_enterprise_proxy_provider/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Airbnb scraper made pure in Python\n",
      "Text: Hello everyone, I would like to share this web scraper.\n",
      "\n",
      "The project will get Airbnb's information including images, description, price, title ..etcIt also supports full search support\n",
      "\n",
      "[https://github.com/johnbalvin/pybnb](https://github.com/johnbalvin/pybnb)\n",
      "\n",
      "I absolutely hate using tools like Selenium, Puppeteer, Playwright, and the like; that's exactly why I went with pure HTTP requests instead\n",
      "\n",
      "Install:\n",
      "\n",
      "`$ pip install gobnb`\n",
      "\n",
      "Usage:\n",
      "\n",
      "`from gobnb import *`\n",
      "\n",
      "`data = Get_from_room_url(room_url,currency,\"\")`\n",
      "\n",
      "let me know what you think\n",
      "\n",
      "thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aoiibc/airbnb_scraper_made_pure_in_python/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Wayback Machine/ Archive - Twitter hashtags\n",
      "Text: Is there any Python package to link to the wayback machine archive of twitter to search for particular hashtags rather than usernames? \n",
      "\n",
      "# \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aobexi/wayback_machine_archive_twitter_hashtags/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Seleniumbase UC and block specific urls\n",
      "Text: Hello,\n",
      "\n",
      "I am trying to block some urls from loading in order to have fast rendering of web page (css, js, images)\n",
      "\n",
      "Eg, I want to load example.com and block all example.com/assets/* requests.\n",
      "\n",
      "In order to have a proxyless system, are there any solution that I can follow?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ao9iv7/seleniumbase_uc_and_block_specific_urls/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Robots.txt disallowed pages issue on an e-commerce website\n",
      "Text: > Curiosity kills the cat!\n",
      "\n",
      "I'm quite new at web scraping. I wonder that what happens if i scrape data from robots.txt disallowed pages?\n",
      "\n",
      "- What is the mechanism of work this kind of e-commerce websites?\n",
      "- Can i use this scraped data in my personal web crawler or spider projects to compare the product prices?\n",
      "- Is there any restriction in server side?\n",
      "- Do server forbids my access  or directly blocks my IP or any other access info?\n",
      "Here is the part of the `robots.txt` :\n",
      "```\n",
      "User-agent: *\n",
      "Allow: / [ somepagehere]\n",
      "Disallow: /m/\n",
      "Disallow: /mc/\n",
      "Disallow: /siparis-takip-durumu\n",
      "Disallow: /kiyasla/\n",
      "Disallow: /brand/\n",
      "````\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aodzyj/robotstxt_disallowed_pages_issue_on_an_ecommerce/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to bypass cloudflare in python?\n",
      "Text: I was trying to scrape this website: \n",
      "https://www.truepeoplesearch.com/\n",
      "\n",
      "But i got blocked.\n",
      "I've tried some configs for selenium but it didn't work, i also try using Undetectable Chromedriver but i got the same result.\n",
      "\n",
      "I'm new with cloudflare so are there other options that i could do?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1antcu1/how_to_bypass_cloudflare_in_python/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Getting 403 on every device except my machine\n",
      "Text: Hi! I am having an issue that I can simply not explain. I am scraping [kleinanzeigen.de](https://kleinanzeigen.de) using proxies, which seems  to work perfectly on my machine, but if I dockerize the application or have anyone else execute the code it willl return a 403 error. I know for a fact that the proxy is being used on every machine, since I can see the requests going out on the proxy dashboard. I have also tried adding several request headers with no succes.\n",
      "\n",
      "Dockerfile:\n",
      "\n",
      "    FROM python:3.10.12-slim\n",
      "    \n",
      "    # Set the working directory to /app\n",
      "    WORKDIR /app\n",
      "    \n",
      "    # Copy the current directory contents into the container at /app\n",
      "    COPY . /app\n",
      "    \n",
      "    # Install any needed packages specified in requirements.txt\n",
      "    RUN apt-get update -y && \\\n",
      "        apt-get install -y postgresql postgresql-contrib && \\\n",
      "        rm -rf /var/lib/apt/lists/* && \\\n",
      "        pip install --no-cache-dir -r requirements.txt && \\\n",
      "        rm -rf /root/.cache && \\\n",
      "        apt-get autoremove -y\n",
      "    \n",
      "    # STACKOVERFLOW\n",
      "    ENV PYTHONUNBUFFERED=1\n",
      "    \n",
      "    CMD [\"python\", \"main.py\"]\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Python code fragment:\n",
      "\n",
      "    def request_with_proxy(url, headers={}):\n",
      "        # Add random user agent to headers\n",
      "        headers[\"User-Agent\"] = user_agent_rotator.get_random_user_agent()\n",
      "        # Configure proxy\n",
      "        try:\n",
      "            proxy_url = f'http://{os.environ[\"PROXY_USER\"]}:{os.environ[\"PROXY_PASSWORD\"]}@p.webshare.io:80'\n",
      "            proxies = {\n",
      "                'http': proxy_url,\n",
      "                'https': proxy_url\n",
      "            }\n",
      "        except:\n",
      "            raise TypeError(\"MISSING PROXY ENVIRONMENT VARIABLES PROXY_USER AND PROXY_PASSWORD\")\n",
      "    \n",
      "        # Retry 3 times before crashing\n",
      "        for _ in range(ATTEMPTS):\n",
      "            try:\n",
      "                response = requests.get(url, headers=headers, proxies=proxies, timeout=TIMEOUT)\n",
      "                print(response)\n",
      "                print(response.status_code)\n",
      "                return response\n",
      "            except Exception as E: print(E)\n",
      "\n",
      "Any ideas? Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1anunaq/getting_403_on_every_device_except_my_machine/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scale webscraping with selenium?\n",
      "Text: What is the best way to scale an application that uses Selenium? To be more specific, I thought of a backend application using Flask or Jango that starts, on an endpoint, a service using Selenium.\n",
      "\n",
      "The problem is that this seems like a huge memory bottleneck to scale a project like this to hundreds or thousands of users.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1anp4s4/how_to_scale_webscraping_with_selenium/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Playwright browser getting detected after scraping 25,000 pages\n",
      "Text: Hello, I am scrapping a website protected by an anti-bot service, the service is very advanced. I managed to bypass it using common tricks + proxies + captcha solving services. Everything worked like magic and I was scraping using multiple instances. However, today the program stopped working and all the automated browsers are getting blocked. \n",
      "\n",
      "The weird thing is I can use my main Chrome browser and access the website with no problem. How do such services detect automated browsers? (I have checked all the headers sent/time zone/ locales... nothing special)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1amyqly/playwright_browser_getting_detected_after/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need to scrape 10 million links within a 28 day timeframe. Any Advice?\n",
      "Text: As the title suggests im trying to scrape 10 million urls (from the same provider) in a month time frame. I run into 429's after about 1000 requests. I'm new to web scraping but not new to programming and I decided to just use python as it would be simple. Everything works besides the rate limiting. I am not opposed to spending money on proxies and whatnot, I just want to know the place im going to would actually be useful and not just get me locked out after 10000 requests.\n",
      "\n",
      "If you guys have any advice on this as I really have no clue where to start in procy rotating. If you have any advice on how to make an IP last longer before being 429'd aswell that would be great cause as of right now im obviously bot like im doing 8 urls in a multihreaded batch and just grabbing the html with a python request.\n",
      "\n",
      "Thanks everyone!\n",
      "\n",
      "Oh, and its a rolling 28 days so I will run it again the month after etc. thats why the time constraint\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1amwzkj/need_to_scrape_10_million_links_within_a_28_day/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Which ai tools can scrape social media for new posts?\n",
      "Text: Not exactly web scraping but I am interested to find an ai tool that can scrape facebook, instagram, quora, reddit and similar websites where people will discuss and mention particular keywords.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Any suggestions?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1amm0ox/which_ai_tools_can_scrape_social_media_for_new/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: stuck following official scrapy tutorial\n",
      "Text: \n",
      "URL: https://imgur.com/a/zNrcCeI\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is this possible\n",
      "Text: I am new to scraping but I am having issues using the scrapy shell to view the site I want to get to. I am using the scrapy tutorial project but I just wanted to see if it was possible to get to this site. This is the robots.txt file. Should I abandon this project because so much is banned or is this just because I jumped the gun and need to figure it out? Thank you!\n",
      "\n",
      "&#x200B;\n",
      "\n",
      " User-agent: \\* \n",
      "\n",
      "\\#Prevent Bot Crawl of applied search filters \n",
      "\n",
      "Disallow: /search/\\*  \n",
      "\n",
      "\\#Prevent Bot Crawl of deprioritised pages\n",
      "\n",
      "Disallow: /\\*/selling/\\* \n",
      "\n",
      "Disallow: /\\*/sold/\\* \n",
      "\n",
      "Disallow: /\\*/likes/\\* \n",
      "\n",
      "Disallow: /\\*/other/\\*  \n",
      "\n",
      "\\#Prevent Bot Crawl of Query Params \n",
      "\n",
      "Disallow: \\*?from=\\*  \n",
      "\n",
      "\\#Prevent Bot Crawl category & brand filters \n",
      "\n",
      "Disallow: \\*categories=\\*\n",
      "\n",
      " Disallow: \\*subcategories=\\* \n",
      "\n",
      "Disallow: \\*brands=\\* \n",
      "\n",
      "Disallow: \\*sizes=\\* \n",
      "\n",
      "Disallow: \\*priceMin=\\*\n",
      "\n",
      " Disallow: \\*priceMax=\\* \n",
      "\n",
      "Disallow: \\*hasFreeShipping=\\* \n",
      "\n",
      "Disallow: \\*isDiscounted=\\* \n",
      "\n",
      "Disallow: \\*colours=\\* \n",
      "\n",
      "Disallow: \\*conditions=\\*  \n",
      "\n",
      "User-agent: Yandex \n",
      "\n",
      "Crawl-delay: 2.0 \n",
      "\n",
      "\\# sets a 2-second timeout \n",
      "\n",
      "Disallow: /search/?q=\\*& \n",
      "\n",
      "Disallow: \\*categories=\\* \n",
      "\n",
      "Disallow: \\*subcategories=\\* \n",
      "\n",
      "Disallow: \\*brands=\\* \n",
      "\n",
      "Disallow: \\*sizes=\\* \n",
      "\n",
      "Disallow: \\*priceMin=\\* \n",
      "\n",
      "Disallow: \\*priceMax=\\* \n",
      "\n",
      "Disallow: \\*hasFreeShipping=\\* \n",
      "\n",
      "Disallow: \\*isDiscounted=\\* \n",
      "\n",
      "Disallow: \\*colours=\\* \n",
      "\n",
      "Disallow: \\*conditions=\\*  \n",
      "\n",
      "User-agent: GemIndexer \n",
      "\n",
      "Disallow: /  User-agent: \n",
      "\n",
      "PetalBot Disallow: / \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1amgh8v/is_this_possible/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: store details from google, is it possible to scrape that data ?\n",
      "Text: I was given a task(intern) to scrape details of local businesses in different categories globally.\n",
      "\n",
      "Can someone guide me.?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1amfrel/store_details_from_google_is_it_possible_to/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Word Press. Scrape images from wp-content/uploads/\n",
      "Text: Any suggestions to scrap all image files from a series of subfolders under domain.com/wp-content/uploads/\n",
      "\n",
      "Inside /uploads/ are sequental years, 2018,2019 etc etc.   \n",
      "\n",
      "\n",
      "Any advice appreciated! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ame3pg/word_press_scrape_images_from_wpcontentuploads/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Seeking advice on my data subscription model and industry norms\n",
      "Text: Hi. I'm offering a leads data subscription model and developing my own API to facilitate it. The subscription includes receiving 2000 new leads monthly for a fixed rate. With each passing month, a new set of leads is served through the API. As I'm relatively new to the concept of data subscription models, I'm wondering whether subscribers should only access the current month's dataset or also retain access to data from previous months during their subscription period.\n",
      "\n",
      "To illustrate with an example:\n",
      "\n",
      "\\- The subscription provides 2000 rows per month.\n",
      "\n",
      "\\- Alice subscribes to the dataset in January and downloads batch #1.\n",
      "\n",
      "\\- In February, Alice downloads batch #2, containing the new leads.\n",
      "\n",
      "According to industry norms, would Alice anticipate ongoing access to batch #1 as well, perhaps through a versioning parameter or similar mechanism?\n",
      "\n",
      "While I understand that the subscription terms are dictated by the seller, as a regular data subscriber what is more common?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1am5n6a/seeking_advice_on_my_data_subscription_model_and/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Tumblr image server\n",
      "Text: Hello,\n",
      "\n",
      "I'm not sure if web scraping would be the way to go. Looking for images that are lost on Tumblr but I know still exists on Tumblr image server. I've been able to find a couple photos through Internet archive and pulling the image source URL. But haven't been able to get them all. I am looking to pull find ever URL used by the image.tumblr.com and query/sort through them with the address parameters. \n",
      "\n",
      "Would this be possible or should I look somewhere else?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ambu27/scraping_tumblr_image_server/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to approach this data scraping project\n",
      "Text: Hi there!\n",
      "\n",
      "So I’m a little informed but not great. \n",
      "\n",
      "My company requires us to scrape Amazon and capture supplements fact data from supplement product labels.\n",
      "\n",
      "That’s of course in addition to:\n",
      "\n",
      "- msrp\n",
      "- reviews\n",
      "- bought this month\n",
      "- type\n",
      "- count of capsules / weight of powder \n",
      "- about this item\n",
      "- reviews \n",
      "- ratings\n",
      "- title name\n",
      "- page link \n",
      "\n",
      "I’ve already got octoparse pulling most of it ok. But how do I approach the labels?\n",
      "\n",
      "Literally the ceo wants to be able to look at all the competitors mg of vitamin c for example in a vitamin c supplement and then figure it out how it relates to retail price.\n",
      "\n",
      "So he wants to be able to be like:\n",
      "\n",
      "- ok this has 1 mg of vitamin c\n",
      "- the retail price is $10\n",
      "- there are 10 capsules\n",
      "- serving size is 1 capsule \n",
      "\n",
      "$10 / 10 capsules = $1 capsule\n",
      "$1 capsule / 1 mg of vitamin c = $1 per mg of vitamin c\n",
      "\n",
      "And we want to pull the data at mass. As I said octoparse has been ok with some issues but the hand calculating takes forever.\n",
      "\n",
      "There has to be a service right? Or a better way?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ama8n1/how_to_approach_this_data_scraping_project/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Reddit for a public dataset — is it legal?\n",
      "Text: Let’s say that I want to create a dataset that connects users with their top-10 favourite communities. The dataset doesn’t contain any information other than `username:subreddit` entries. The dataset contains 10s of thousands rows and will only be useful for educational purposes (trying out graph clustering algorithms). I collected the data using Reddit API or web scraping, and want to make it public (by uploading it on GitHub or Kaggle.com).\n",
      "\n",
      "Is that legal? Does it come under any personal information protection laws? If it’s legal, is it against Reddit TOC?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1am16j6/scraping_reddit_for_a_public_dataset_is_it_legal/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Anybody figure out how to turn this into a money maker?\n",
      "Text: I can whip together a pretty reliable scraper, pretty fast. Just wondering if anybody is making fun money out of this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aly2ko/anybody_figure_out_how_to_turn_this_into_a_money/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I get past this captcha?\n",
      "Text: Hey guys!\n",
      "\n",
      "I have been scraping a certain website for a few months now. However, this week they added a very complex catpcha that I don't know how to get through now. \n",
      "\n",
      "Basically, you should align the image on the right with the direction the hand in the image on the left is pointing.\n",
      "\n",
      "Does anyone have any way to resolve this problem?  \n",
      "\n",
      "\n",
      "Thank you very much for all your help.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "https://preview.redd.it/3xdj22v56dhc1.png?width=524&format=png&auto=webp&s=1c710cf0bae83ffd405396244e17d120d5098f97\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1alvt5f/how_can_i_get_past_this_captcha/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trying to Solve captcha\n",
      "Text: [https://stackoverflow.com/questions/77961370/try-solving-google-captcha](https://stackoverflow.com/questions/77961370/try-solving-google-captcha)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "First off, excuse me, I'm still learning\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "How he solved the captcha in this video starting at second 27\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "[https://youtu.be/0h504QFDyxc?si=RckXYelnrvWyWHFA](https://youtu.be/0h504QFDyxc?si=RckXYelnrvWyWHFA)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Every time I solve the captcha on this site, it tells me that something is wrong. How do I overcome this problem?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I tried the code api 2captcha It did not succeed in solving it I use a library seleniumbase To control the browser\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Continue changing between Centers for 30 minutes, then log out. You will find that Cloud Flare has disappeared and Google Captcha has appeared. If you fill in the data, it tells you that there are empty fields. I do not know how to bypass that.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "videos to the problem [https://drive.google.com/drive/folders/1QLqdwmPSjhnmC17qmhCyKrx4pK-zNYom](https://drive.google.com/drive/folders/1QLqdwmPSjhnmC17qmhCyKrx4pK-zNYom)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I stopped the api and installed a browser add-on that solves the captcha. What should I do? I am tired of this problem\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1alu7fb/trying_to_solve_captcha/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Healthcare Scrape Tool\n",
      "Text: Hello, I'm looking for a Chrome extension app that will scrape a webpage (on-demand) to capture the data entered into a web form (mostly text, checkboxes and dropdowns), then parse and API that data to a 3rd party system.  The purpose of this workflow is to reduce manual data entry between two systems.  For example, a user will enter the information into the web form; then this scrape will automatically populate another application.  This is a healthcare application, so the app must be HIPPA compliant.  Does anyone have any recommendations?   \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1altsi9/healthcare_scrape_tool/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Bing package tracking - Carrier list\n",
      "Text: The Bing package tracking website seems to work great for scrapping UPS shipment status: [https://www.bing.com/packagetrackingv2?packNum=](https://www.bing.com/packagetrackingv2?packNum=)<TrackingNumber>&carrier=<CarrierName>\n",
      "\n",
      "Anyone have an idea how to find the list of European carriers?\n",
      "\n",
      "Tried: DPD, DHL ... but doesn't seem to be working\n",
      "\n",
      "Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1alpg6h/bing_package_tracking_carrier_list/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Are Terms of Service legally binding?\n",
      "Text: Can I show a future employer a webscraper I built with rotating proxies, or is that a no-go?\n",
      "\n",
      "Thank you for any advice or resources.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aljqdv/are_terms_of_service_legally_binding/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping a Substack I pay for?\n",
      "Text: Would it be legal to scrape a Substack site that I pay for?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1almepu/scraping_a_substack_i_pay_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Certified web scraper?\n",
      "Text: I've been web scraping for a few years and currently doing so as a freelancer next to my regular job. My first introduction to web scraping was in a data analysis course, but this was a miniscule part of the whole course. I was able to learn almost everything from YouTube, trial and error (and a lot of hair pulling), and from asking people like in this growing community.\n",
      "Since my original studies were not IT/CS based, I wanted to know if there are any courses and/or certifications that would help me stand out when applying for a web scraping engineer job, as well as hone in my skills better?\n",
      "I know web scraping itself as a discipline is still fairly new but if I were to look for certificates that would help me stand out, what should I look for and where?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1al3tcm/certified_web_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping Instagram\n",
      "Text: Hey, I really need help for an Project that is due Friday.  \n",
      "I need to scrape 1000+ instagram user account's last three Posts for hashtags includet in the caption.  \n",
      "How can i scrape this in Python? I need to make this around 1x per Day.  \n",
      "All of the Existing packages like instagrap, instaloader ... fail after \\~ 20 Attempts.  \n",
      "Selenium too gets blocked after \\~40 tries  \n",
      "\n",
      "\n",
      "Does anyone know how do get around this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1algn3n/webscraping_instagram/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Hey I'm totally new to scraping. Been watching a couple videos and wanted some info regarding getting leads\n",
      "Text: I would love to partner up if this is possible as it has unlimited income potential. What I'm looking for is a way to generate leads through finding the organic info from visitors on other companies website. Such as Sunrun or Momentum solar and tapping into people who have logged into their website? Is this a real or totally dreaming over here?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1albbjs/hey_im_totally_new_to_scraping_been_watching_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Legality of scraping House.gov pdf links\n",
      "Text: Hello everyone. I am trying to download all financial disclosure reports for US house members. You can download an xml file from house.gov that has doc ID from their website, and if you look at the pdfs you're able to download from the website, theres a pattern that goes <year>/<docId>.pdf. I was wondering the legality of downloading all financial reports with a script by just iterating through their structures pdf links. I know it's public information but I am worried about querying so much data in a way that is not accessible via their API but instead through a tedious manual search function that they have. I just want to take some extra precaution since its a government website haha\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1akwoyx/legality_of_scraping_housegov_pdf_links/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do I scrape Twitter DM Status not logged in\n",
      "Text: Been trying to scrape DM status on profiles. It's easy enough logged in using the dev tools or puppeteer. \n",
      "\n",
      "There's a \"can\\_dm\" field that can be scraped which is set to true or false. Problem is, need heaps of accounts to do this at scale due to rate limiting.\n",
      "\n",
      "I tried pulling the DM status not authenticated but they hide \"can\\_dm\". I can pull almost every other bit of a profile not logged in except the DM status.\n",
      "\n",
      "Trying to come up with a method so I can scrape this and just rotate proxies all while not being logged in. \n",
      "\n",
      "Appreciate any thought!  \n",
      "\n",
      "\n",
      "https://preview.redd.it/8whuwvmhj3hc1.png?width=1278&format=png&auto=webp&s=10e35e852a139e8bb99df2b6eb2502b546743c90\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1akv8ie/how_do_i_scrape_twitter_dm_status_not_logged_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I need scrape my posts from FB group\n",
      "Text: Hello guys,\n",
      "\n",
      "My father is member of 2 FB group (1 is private, other is public) and he want to collect only his posts with date. Btw there is more than thousand post from him :). Is there a any automatic script for that? Easy way to collect as even text file. Post with date.\n",
      "\n",
      "Thanks in advance\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1akxugv/i_need_scrape_my_posts_from_fb_group/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Zillow Scraping\n",
      "Text: I am beyond stuck at the moment:\n",
      "\n",
      "I only need information from certain addresses\n",
      "\n",
      "Where I currently am: high level code\n",
      "\n",
      "- utilizing python , webdriver ~> Seleniumwire\n",
      "- webdriver.firefox()\n",
      "\n",
      "Driver get zillow/specific address\n",
      "Time sleep(5)\n",
      "\n",
      "For requests in driver: \n",
      "\n",
      "Try\n",
      "Load(response body)\n",
      "Except\n",
      "Continue\n",
      "\n",
      "Driver quit\n",
      "\n",
      "Two captcha windows pop up and it keeps saying try again and again and again even though i click and hold and the check mark passes \n",
      "\n",
      "Appreciate any advice on how to move forward, im open to restarting and using another library in python, yes prob not the best way but i prefer python\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1akx8o1/zillow_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Job Listings Scraping\n",
      "Text: Has anyone come across an API (paid or free) that has access to *most* job openings in the USA? This is to pull information and not push information to job boards.\n",
      "\n",
      "I've only come across scraping solutions that run on sites such as Jooble - which is a lower quality site.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1akgluf/job_listings_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any good open source alternative to Octoparse?\n",
      "Text: I like the software but it can be a bit resource intensive and confusing at times. Nonetheless, the pricing is a bit steep lol\n",
      "\n",
      "Does anyone else have other recommendations for similar no-code webscrapers that are open source?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1akd96c/any_good_open_source_alternative_to_octoparse/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Tiktok scraping: How to avoid captcha?\n",
      "Text: Trying to scrape urls from the infinite scroll feed, before getting to that feed a captcha always pops up that I have to manually solve. \n",
      "\n",
      "I've tried using residential proxies to no avail. \n",
      "\n",
      "Any tips?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1akb7ta/tiktok_scraping_how_to_avoid_captcha/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Twitter Scraping Limits and how to extend them\n",
      "Text: Hi all, we have recently built a Twitter Tweet and Profile scraper which deposits all scraped information into Sheets. We need to scrape approx 3000 tweets/profiles a day, and I am aware the limits are 600 per day per unverified account right?\n",
      "\n",
      "I have bought 24 Twitter Accounts (Scraper runs hourly, planned one account per hour to be safe), however I'm having a little trouble finding a proxy solution.\n",
      "\n",
      "My main question is - is it unsafe to run all 24 (scraper only) accounts from the one VPS IP, even though they are only navigating Twitter and not performing any actions (e.g. likes, tweets etc.)? If not safe, is anyone aware of any proxy services which offer cheap rotating (Data center if thats also safe!) proxys which can automatically rotate at least hourly? Much appreciated\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ake772/twitter_scraping_limits_and_how_to_extend_them/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How I can get instagram graphql query_hash?\n",
      "Text: Uncommon question but related to scraping. Instagram uses `query_hash` to improve GraphQL network performance by reducing the request size, query\\_hash is a random string of characters and numbers that summed up - random letters + random numbers (for example: 4sa6as5jld8k3ldf... ) gives ammount of exactly 32 characters. However few years ago query\\_hash was available in browser dev tools in network tab section. Now it's not (at least not named to anything related to query\\_hash) I wonder how to get it these days? I'm seraching for quey\\_hash for user post likers/ user followers so I can scrape their data. I did some reserach I was able to find few posibilities - they use minification to hide the query\\_hash, moved it somewhere else (for example hidden in javascript) or did something else. I hope somebody who may know that topic will share. Any information will be helpful!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ak3op8/how_i_can_get_instagram_graphql_query_hash/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: i am trying to scrape a website https://www.sama.gov.sa/en-US/RulesInstructions/Pages/FinanceRulesAndRegulations.aspx it has pagination in the button my code is not clicking on the button i am using this\n",
      "Text: next\\_button= await page.locator('//a\\[@class=\"ms-commandLink ms-promlink-button ms-promlink-button-enabled\"\\]')         if not next\\_button:             break  # Exit the loop if there's no \"Next\" button          await next\\_button.click()\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1akac0s/i_am_trying_to_scrape_a_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ever experience ISP provided routers blocking requests?\n",
      "Text: In testing a scraping script, I noticed if I batched a thousand or more requests(to a proxy) at once I would get 98% failure rate with an \"ETIMEDOUT\" error- and more suspicious, other sites just from my browser failed.  After turning on a VPN all this disappeared.  The only thing I can think of causing this is my router is doing some kind of DDOS prevention.  Any idea on what could be causing it?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ak2f3r/ever_experience_isp_provided_routers_blocking/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any experience scraping facebook groups?\n",
      "Text: The facebook api does not expose public group posts and comments unless you're the group admin. Which is why I resort to a regular approach. To use the search, an account is needed, and I tried automating the registration to counter the draconian rate limits imposed by the website, but I haven't gotten any further than the confirmation view, which if any of the proxies got lucky and a confirmation code is sent, the account keeps being suspended unless confirmed from the browser (not selenium, it doesn't work). Got any useful tips in that regard?\n",
      "I'm using a python requests approach with proxy rotation. Any idea why most if not all requests are detected despite emulating the browser's requests, cookies, headers, ...?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ajhavi/any_experience_scraping_facebook_groups/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping the Samsung TV App Store\n",
      "Text: Anyone have experience scraping the samsung TV app store? Wondering if there's any APIs to it, or a new website to a list of their apps. I used to this URL (samsung.com/us/appstore/browse) to grab info but now it forwards all apps to 404s. Any help/hints appreciated.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ajx9dc/scraping_the_samsung_tv_app_store/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What's the best web Scraping project you've done or thought of doing ?\n",
      "Text: Hi 👋, I'm just wondering. Just drop your project. I don't care how stupid or genius it sounds.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ajc9bh/whats_the_best_web_scraping_project_youve_done_or/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is it possible to scrape hidden data contained in JS object, which is cleared from the DOM after page load?\n",
      "Text: I'm developing a scraper which uses playwright and a proxy network to scrape company overview information and reviews from Glassdoor. Here is an example URL:\n",
      "\n",
      "[https://www.glassdoor.com/Reviews/NVIDIA-Reviews-E7633.htm?filter.countryId=1&filter.countryId=3](https://www.glassdoor.com/Reviews/NVIDIA-Reviews-E7633.htm?filter.countryId=1&filter.countryId=3)\n",
      "\n",
      "Strangely when I was testing it before on a different company with this URL:\n",
      "\n",
      "[https://www.glassdoor.com/Reviews/eBay-Motors-Group-Reviews-E4189745.htm?filter.countryId=1&filter.countryId=3](https://www.glassdoor.com/Reviews/eBay-Motors-Group-Reviews-E4189745.htm?filter.countryId=1&filter.countryId=3)\n",
      "\n",
      "I was able to extract the 'apolloState' object that I'm interested in, which is contained within a script tag in the HTML directly. But now for NVIDIA it doesn't seem to be able to find it.\n",
      "\n",
      "As far as I understand, Glassdoor uses GraphQL to populate data such as company reviews. I'm interested in the script tag which contains the object 'window.appCache' as shown in the screenshot in the element tab of the devtools.\n",
      "\n",
      "[elements tab, window.appCache](https://preview.redd.it/d769r0b4yqgc1.png?width=5120&format=png&auto=webp&s=efc5a84ffabd061966cd34516c1c276b5e8db409)\n",
      "\n",
      "However, as you can see it is only javascript with no properties containing the reviews data, but when I check the source HTML in the sources tab of the devtools, as shown in the screenshots, the 'window.appCache' object has an 'apolloState' property which contains json objects with all the overview and reviews data I want to extract.\n",
      "\n",
      "[sources tab, window.appCache](https://preview.redd.it/oiqmlk45yqgc1.png?width=5120&format=png&auto=webp&s=46803afba8fd68c9f51d87089ff6c7d04d59081b)\n",
      "\n",
      "[sources tab, apolloState](https://preview.redd.it/7y6p4qldyqgc1.png?width=5120&format=png&auto=webp&s=5241e0407f9e30359e5f1f322b6044ab9cd46cc8)\n",
      "\n",
      "Am I right in my understanding, that javascript runs the script on page load to populate the elements of the page with its corresponding key value pairs from the apolloState's json and then the script removes the ‘apolloState’ object from the DOM? If this is correct, should I then be trying to extract the 'apolloState' object before javascript runs the script? And how should I go about doing this?\n",
      "\n",
      "Also, not sure if it is correct to refer to the 'apolloState' as an object or property, apologies for that.\n",
      "\n",
      "When I refresh the page and filter by document requests in the network tab of the devtools, I can also find the 'apolloState' object in the script tag, as shown in these screenshots:\n",
      "\n",
      "[network tab, window.appCache](https://preview.redd.it/n30jpldkyqgc1.png?width=5120&format=png&auto=webp&s=86e7b1e596f553f93369aec541368bb3a50ef016)\n",
      "\n",
      "[network tab, apolloState](https://preview.redd.it/cq9wjormyqgc1.png?width=5120&format=png&auto=webp&s=5be1269132b4b514533b765766455bcc36d231aa)\n",
      "\n",
      "I've been searching for an answer to this question, but I haven't found anything that completely answers it for me. If anyone can shed some light on the issue it would be greatly appreciated, thank you in advance.\n",
      "\n",
      "Also, here is the relevant python code which should extract the 'apolloState' object from the HTML.\n",
      "\n",
      "I think the problem is in the \\`find\\_hidden\\_data()\\` function, or more accurately that once the page loads the 'apolloState' object is removed from the DOM so the function can’t find it.\n",
      "\n",
      "I left out the Url class that builds the URLs for me as well since this is already quite a long question. You'll notice it is called to test the script in the \\`if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\\` block.\n",
      "\n",
      "Here is the Github repository for the project, in case anyone wants to take a deeper look at the entire code: [https://github.com/qma9/glassdoor-scraper](https://github.com/qma9/glassdoor-scraper)\n",
      "\n",
      "    from bs4 import BeautifulSoup\n",
      "    from playwright.async_api import async_playwright\n",
      "    from typing import Optional, Dict, Tuple, List\n",
      "    from datetime import datetime\n",
      "    from enum import Enum\n",
      "    import json\n",
      "    import re\n",
      "    import asyncio\n",
      "    from dotenv import load_dotenv\n",
      "    import sys\n",
      "    import os\n",
      "    \n",
      "    # Load .env file\n",
      "    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
      "    load_dotenv()\n",
      "    \n",
      "    from log.setup import logger, setup_logging\n",
      "    \n",
      "    # Configure logging\n",
      "    setup_logging()\n",
      "    \n",
      "    # Bright Data headless browser authentication credentials\n",
      "    cred = {\n",
      "        \"host\": os.getenv(\"HOST\"),\n",
      "        \"username\": os.getenv(\"USERNAME\"),\n",
      "        \"password\": os.getenv(\"PASSWORD\"),\n",
      "    }\n",
      "    auth = f'{cred[\"username\"]}:{cred[\"password\"]}'\n",
      "    browser_url = f'wss://{auth}@{cred[\"host\"]}'\n",
      "    \n",
      "    \n",
      "    def find_hidden_data(result: str) -> dict:\n",
      "        \"\"\"\n",
      "        Extract hidden web cache (Apollo Graphql) from Glassdoor page HTML.\n",
      "        It's either in NEXT_DATA script or direct apolloState js variable.\n",
      "    \n",
      "        Args:\n",
      "            result (str): The HTML content of the Glassdoor page.\n",
      "    \n",
      "        Returns:\n",
      "            dict: The extracted hidden web cache data.\n",
      "    \n",
      "        \"\"\"\n",
      "        # Create a BeautifulSoup object from the response text\n",
      "        soup = BeautifulSoup(result, \"html.parser\")\n",
      "    \n",
      "        # Find all script tags that contain the appCache data\n",
      "        script_tags = soup.find_all(\n",
      "            \"script\", string=lambda text: text is not None and \"window.appCache\" in text\n",
      "        )\n",
      "    \n",
      "        data = {}\n",
      "    \n",
      "        for script_tag in script_tags:\n",
      "            # Extract the JavaScript code from the script tag\n",
      "            javascript_code = script_tag.string\n",
      "    \n",
      "            # Find the start and end of the appCache JSON object\n",
      "            start = javascript_code.find(\"window.appCache = \") + len(\"window.appCache = \")\n",
      "            end = javascript_code.find(\n",
      "                \";\", start\n",
      "            )  # find the semicolon after the JSON object\n",
      "    \n",
      "            # Extract the appCache JSON object\n",
      "            app_cache_json = javascript_code[start:end]\n",
      "    \n",
      "            # Parse the appCache JSON object\n",
      "            app_cache = json.loads(app_cache_json)\n",
      "    \n",
      "            # Extract the apolloState data from the appCache\n",
      "            data.update(app_cache.get(\"apolloState\", {}))\n",
      "    \n",
      "        if not data:\n",
      "            logger.error(\n",
      "                f\"No Apollo Graphql or window.appCache js variable found\",\n",
      "                extra={\"soup\": soup.prettify()},\n",
      "            )\n",
      "    \n",
      "        return data\n",
      "    \n",
      "    \n",
      "    def parse_overview(apollo_state: str) -> Dict[str, str | int]:\n",
      "        \"\"\"\n",
      "        Parse overview from Glassdoor page HTML.\n",
      "    \n",
      "        Args:\n",
      "            result (str): The HTML content of the Glassdoor page.\n",
      "    \n",
      "        Returns:\n",
      "            Dict[str, str | int]: A dictionary containing the parsed overview data.\n",
      "    \n",
      "        \"\"\"\n",
      "        overview_data = {}\n",
      "        cache = apollo_state  # find_hidden_data(result)\n",
      "        if not cache:\n",
      "            logger.error(f\"No hidden data found in Glassdoor page html\")\n",
      "        else:\n",
      "            try:\n",
      "                root_query = cache[\"ROOT_QUERY\"]\n",
      "                overview_data = next(\n",
      "                    (\n",
      "                        value\n",
      "                        for key, value in root_query.items()\n",
      "                        if key.startswith(\"employerReviewsRG\")\n",
      "                        and isinstance(value, dict)\n",
      "                        and value.get(\"__typename\") == \"EmployerReviewsRG\"\n",
      "                    ),\n",
      "                    {},\n",
      "                )\n",
      "            except KeyError:\n",
      "                logger.error(f\"ROOT_QUERY key not found in cache\")\n",
      "    \n",
      "        try:\n",
      "            # Extract company overview information\n",
      "            overview = {\n",
      "                \"employer_id\": int(\n",
      "                    overview_data[\"employer\"][\"__ref\"].split(\":\")[1]\n",
      "                    if \"employer\" in overview_data\n",
      "                    else None\n",
      "                ),  # comment out later once all companies are retrieved\n",
      "                # \"employer_name\": overview_data[\"employer\"][\"name\"],\n",
      "                \"number_of_pages\": overview_data[\"numberOfPages\"],\n",
      "                \"all_reviews_count\": overview_data[\"allReviewsCount\"],\n",
      "                \"rated_reviews_count\": overview_data[\"ratedReviewsCount\"],\n",
      "                \"overall_rating\": overview_data[\"ratings\"][\"overallRating\"],\n",
      "                \"ceo_name\": (\n",
      "                    overview_data[\"ratings\"][\"ratedCeo\"][\"name\"]\n",
      "                    if overview_data[\"ratings\"][\"ratedCeo\"] is not None\n",
      "                    and \"name\" in overview_data[\"ratings\"][\"ratedCeo\"]\n",
      "                    else None\n",
      "                ),\n",
      "                \"ceo_rating\": overview_data[\"ratings\"][\"ceoRating\"],\n",
      "                \"recommend_to_friend_rating\": overview_data[\"ratings\"][\n",
      "                    \"recommendToFriendRating\"\n",
      "                ],\n",
      "                \"culture_and_values_rating\": overview_data[\"ratings\"][\n",
      "                    \"cultureAndValuesRating\"\n",
      "                ],\n",
      "                \"diversity_and_inclusion_rating\": overview_data[\"ratings\"][\n",
      "                    \"diversityAndInclusionRating\"\n",
      "                ],\n",
      "                \"career_opportunities_rating\": overview_data[\"ratings\"][\n",
      "                    \"careerOpportunitiesRating\"\n",
      "                ],\n",
      "                \"work_life_balance_rating\": overview_data[\"ratings\"][\n",
      "                    \"workLifeBalanceRating\"\n",
      "                ],\n",
      "                \"senior_management_rating\": overview_data[\"ratings\"][\n",
      "                    \"seniorManagementRating\"\n",
      "                ],\n",
      "                \"compensation_and_benefits_rating\": overview_data[\"ratings\"][\n",
      "                    \"compensationAndBenefitsRating\"\n",
      "                ],\n",
      "                \"business_outlook_rating\": overview_data[\"ratings\"][\n",
      "                    \"businessOutlookRating\"\n",
      "                ],\n",
      "            }\n",
      "    \n",
      "        except KeyError as e:\n",
      "            logger.error(f\"Key {e} not found in overview_data\")\n",
      "            raise\n",
      "    \n",
      "        return overview\n",
      "    \n",
      "    \n",
      "    def parse_reviews(apollo_state: str) -> Dict[str, Dict[str, str | int]]:\n",
      "        \"\"\"\n",
      "        Parse data from Glassdoor page HTML.\n",
      "    \n",
      "        Args:\n",
      "            result (str): The HTML content of the Glassdoor page.\n",
      "    \n",
      "        Returns:\n",
      "            Dict[str, Dict[str, str | int]]: A dictionary containing parsed review data.\n",
      "                The keys are review IDs and the values are dictionaries containing\n",
      "                various attributes of the review, such as review ID, employer ID,\n",
      "                date and time, ratings, job details, location, pros and cons, etc.\n",
      "        \"\"\"\n",
      "        cache = apollo_state  # find_hidden_data(result)\n",
      "        if not cache:\n",
      "            logger.error(f\"No hidden data found in Glassdoor page html\")\n",
      "        else:\n",
      "            try:\n",
      "                root_query = cache[\"ROOT_QUERY\"]\n",
      "                reviews_data = next(\n",
      "                    (\n",
      "                        value[\"reviews\"]\n",
      "                        for key, value in root_query.items()\n",
      "                        if key.startswith(\"employerReviewsRG\")\n",
      "                        and isinstance(value, dict)\n",
      "                        and value.get(\"__typename\") == \"EmployerReviewsRG\"\n",
      "                        and \"reviews\" in value\n",
      "                    ),\n",
      "                    [],\n",
      "                )\n",
      "            except KeyError:\n",
      "                logger.error(f\"ROOT_QUERY key not found in cache\")\n",
      "    \n",
      "        try:\n",
      "            # Extract city and job title names\n",
      "            city_job_title = {\n",
      "                key: value\n",
      "                for key, value in cache.items()\n",
      "                if key.startswith((\"City\", \"JobTitle\"))\n",
      "            }\n",
      "        except KeyError as e:\n",
      "            logger.error(f\"Key {e} not found in cache\")\n",
      "    \n",
      "        try:\n",
      "            # Extract reviews\n",
      "            reviews = {}\n",
      "    \n",
      "            for review in reviews_data:\n",
      "                extracted_review = {\n",
      "                    \"review_id\": review[\"reviewId\"],\n",
      "                    \"employer_id\": int(review[\"employer\"][\"__ref\"].split(\":\")[1]),\n",
      "                    \"date_time\": datetime.fromisoformat(\n",
      "                        review[\"reviewDateTime\"].replace(\"T\", \" \")\n",
      "                    ),\n",
      "                    \"rating_overall\": review[\"ratingOverall\"],\n",
      "                    \"rating_ceo\": (\n",
      "                        review[\"ratingCeo\"] if review[\"ratingCeo\"] is not None else None\n",
      "                    ),\n",
      "                    \"rating_business_outlook\": review[\"ratingBusinessOutlook\"],\n",
      "                    \"rating_work_life_balance\": review[\"ratingWorkLifeBalance\"],\n",
      "                    \"rating_culture_and_values\": review[\"ratingCultureAndValues\"],\n",
      "                    \"rating_diversity_and_inclusion\": review[\"ratingDiversityAndInclusion\"],\n",
      "                    \"rating_senior_seadership\": review[\"ratingSeniorLeadership\"],\n",
      "                    \"rating_recommend_to_friend\": review[\"ratingRecommendToFriend\"],\n",
      "                    \"rating_career_opportunities\": review[\"ratingCareerOpportunities\"],\n",
      "                    \"rating_compensation_and_benefits\": review[\n",
      "                        \"ratingCompensationAndBenefits\"\n",
      "                    ],\n",
      "                    \"is_current_job\": bool(review[\"isCurrentJob\"]),\n",
      "                    \"length_of_employment\": review[\"lengthOfEmployment\"],\n",
      "                    \"employment_status\": review[\"employmentStatus\"],\n",
      "                    \"job_ending_year\": (\n",
      "                        review[\"jobEndingYear\"]\n",
      "                        if review[\"jobEndingYear\"] is not None\n",
      "                        else None\n",
      "                    ),\n",
      "                    \"job_title\": (\n",
      "                        review[\"jobTitle\"][\"text\"]\n",
      "                        if review[\"jobTitle\"] is not None and \"text\" in review[\"jobTitle\"]\n",
      "                        else (\n",
      "                            review[\"jobTitle\"][\"__ref\"]\n",
      "                            if review[\"jobTitle\"] is not None\n",
      "                            and \"__ref\" in review[\"jobTitle\"]\n",
      "                            else None\n",
      "                        )\n",
      "                    ),\n",
      "                    \"location\": (\n",
      "                        review[\"location\"][\"__ref\"]\n",
      "                        if review[\"location\"] is not None\n",
      "                        else None\n",
      "                    ),\n",
      "                    \"pros\": review[\"pros\"],\n",
      "                    \"cons\": review[\"cons\"],\n",
      "                    \"summary\": review[\"summary\"],\n",
      "                    \"advice\": review[\"advice\"],\n",
      "                    \"count_helpful\": review[\"countHelpful\"],\n",
      "                    \"count_not_helpful\": review[\"countNotHelpful\"],\n",
      "                    \"is_covid19\": bool(review[\"isCovid19\"]),\n",
      "                }\n",
      "    \n",
      "                # Add job title\n",
      "                if extracted_review[\"job_title\"] in city_job_title:\n",
      "                    extracted_review[\"job_title\"] = city_job_title[\n",
      "                        extracted_review[\"job_title\"]\n",
      "                    ][\"text\"]\n",
      "    \n",
      "                # Add city name\n",
      "                if extracted_review[\"location\"] in city_job_title:\n",
      "                    extracted_review[\"location\"] = city_job_title[\n",
      "                        extracted_review[\"location\"]\n",
      "                    ][\"name\"]\n",
      "    \n",
      "                # Add review to reviews\n",
      "                reviews[review[\"reviewId\"]] = extracted_review\n",
      "    \n",
      "        except KeyError as e:\n",
      "            logger.error(f\"Key {e} not found in review\", extra={\"review\": review})\n",
      "            raise\n",
      "    \n",
      "        return reviews\n",
      "    \n",
      "    \n",
      "    async def scrape_data(\n",
      "        url: str, max_pages: Optional[int] = None\n",
      "    ) -> Tuple[Dict[str, int | float], Dict[str, Dict[str, str | int]]] | None:\n",
      "        \"\"\"Scrape Glassdoor reviews listings from reviews pages (with pagination).\n",
      "    \n",
      "        Args:\n",
      "            url (str): The URL of the reviews page to scrape.\n",
      "            max_pages (Optional[int], optional): The maximum number of pages to scrape. Defaults to None.\n",
      "    \n",
      "        Returns:\n",
      "            Tuple[Dict[str, int | float], Dict[str, Dict[str, str | int]]]: A tuple containing the overview information and the scraped reviews.\n",
      "    \n",
      "        \"\"\"\n",
      "        logger.info(\"scraping reviews from %s\", url)\n",
      "    \n",
      "        async with async_playwright() as pw:\n",
      "            browser = await pw.chromium.connect_over_cdp(browser_url)\n",
      "            page = await browser.new_page()\n",
      "    \n",
      "            # Only allow document requests\n",
      "            await page.route(\n",
      "                \"**/*\",\n",
      "                lambda route, request: (\n",
      "                    route.continue_()\n",
      "                    if request.resource_type == \"document\"\n",
      "                    else route.abort()\n",
      "                ),\n",
      "            )\n",
      "    \n",
      "            # Navigate to the first page\n",
      "            await page.goto(url, timeout=120000)\n",
      "    \n",
      "            # Create a locator for the script\n",
      "            script_locator = page.locator(\"body > script:nth-child(4)\")\n",
      "    \n",
      "            # Wait for the script to load\n",
      "            await script_locator.wait_for(timeout=60000)\n",
      "    \n",
      "            # Extract the apolloState property from the window.appCache object\n",
      "            apollo_state = await page.evaluate(\"() => window.appCache.apolloState\")\n",
      "    \n",
      "            if not apollo_state:\n",
      "                logger.error(\"window.appCache.apolloState is not present in the script tag\")\n",
      "                return None\n",
      "    \n",
      "            overview = parse_overview(apollo_state)\n",
      "            reviews = parse_reviews(apollo_state)\n",
      "    \n",
      "            total_pages = overview[\"number_of_pages\"]\n",
      "    \n",
      "            if max_pages and max_pages < total_pages:\n",
      "                total_pages = max_pages\n",
      "    \n",
      "            logger.info(\n",
      "                \"scraped first page of reviews of %s, scraping remaining %d pages\",\n",
      "                url,\n",
      "                total_pages - 1,\n",
      "            )\n",
      "    \n",
      "            for page_num in range(2, total_pages + 1):\n",
      "                page_url = Url.change_page(url, page=page_num)\n",
      "                await page.goto(page_url, timeout=120000)\n",
      "                script_locator = page.locator(\"body > script:nth-child(4)\")\n",
      "                await script_locator.wait_for(timeout=60000)\n",
      "    \n",
      "                # Check if the window.appCache object is present in the script\n",
      "                app_cache_present = await page.evaluate(\n",
      "                    \"() => !!window.appCache\",  # This JavaScript code checks if window.appCache is defined\n",
      "                    script_locator.first(),  # This is the element to execute the JavaScript code in\n",
      "                )\n",
      "    \n",
      "                if not app_cache_present:\n",
      "                    logger.error(\n",
      "                        \"window.appCache is not present in the script tag on page %d\",\n",
      "                        page_num,\n",
      "                    )\n",
      "                    continue  # Skip this page and move on to the next one\n",
      "    \n",
      "                result = await page.content()\n",
      "    \n",
      "                if result:  # Check if the page was successfully scraped\n",
      "                    new_reviews = parse_reviews(result)\n",
      "                    reviews.update(new_reviews)\n",
      "                else:\n",
      "                    logger.error(\"failed to scrape %s\", page_url)\n",
      "    \n",
      "                # Add a delay\n",
      "                await asyncio.sleep(1)\n",
      "    \n",
      "            logger.info(\n",
      "                \"scraped %d reviews from %s in %d pages\", len(reviews), url, total_pages\n",
      "            )\n",
      "    \n",
      "        return overview, reviews\n",
      "    \n",
      "    \n",
      "    if __name__ == \"__main__\":\n",
      "        # test script\n",
      "        overview, reviews = asyncio.run(\n",
      "            scrape_data(\n",
      "                Url.reviews(\n",
      "                    \"NVIDIA\",\n",
      "                    \"7633\",\n",
      "                    regions=[Region.UNITED_STATES, Region.CANADA_ENGLISH],\n",
      "                ),\n",
      "                max_pages=1,\n",
      "            )\n",
      "        )\n",
      "        print(\"\\n\\n\")\n",
      "        print(\"Overview:\", overview)\n",
      "        print(\"\\n\\n\")\n",
      "        print(\"Reviews:\", reviews)\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ajdvnl/is_it_possible_to_scrape_hidden_data_contained_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What are the best options for web scraping with low level languages ?\n",
      "Text: I've scraped the Web using languages like Python and Typescript many many times. But these days, I'm kind of into low-level languages like cpp & rust. I wonder what other low-level languages and libraries people use to scrape the web. So can you do me a favour and drop what you know in the comments?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aiz6wk/what_are_the_best_options_for_web_scraping_with/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Furniture scraping help\n",
      "Text: Hii people, I am quite new to scraping, although a bit familiar with python and have also used beautiful soup for very basic stuff recently. Currently I am tasked with writing a python program to get images from ikea with short description of the images and with the filters which the ikea website provides such cost, colour, material etc.  \n",
      "Anyone ahs any suggestions on how to approach this or anyone has any readymade code for this?  \n",
      "Any help would be great, thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ajbkgn/furniture_scraping_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Puppeteer keeps timing out, why? [Docker, Puppeteer, NodeJS]\n",
      "Text: Right now I'm trying to archive a page with Docker and Puppeteer, here is my Dockerfile and here is my TypeScript code. My code works perfectly fine with Bun on my host machine so I dont know why its not working. I keep getting a custom Timeout error after 30 seconds, even with it disabled it just waits forever for google.com  \n",
      "\n",
      "\n",
      "First I use Bun (also tried with node:slim) and I download Chromium\n",
      "\n",
      "    FROM oven/bun:latest as base\n",
      "    \n",
      "    # We don't need the standalone Chromium\n",
      "    ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD true\n",
      "    \n",
      "    # Install Google Chrome Stable and fonts\n",
      "    # Note: this installs the necessary libs to make the browser work with Puppeteer.\n",
      "    RUN apt-get update && apt-get install gnupg wget -y && \\\n",
      "      wget --quiet --output-document=- https://dl-ssl.google.com/linux/linux_signing_key.pub | gpg --dearmor > /etc/apt/trusted.gpg.d/google-archive.gpg && \\\n",
      "      sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list' && \\\n",
      "      apt-get update && \\\n",
      "      apt-get install google-chrome-stable -y --no-install-recommends && \\\n",
      "      rm -rf /var/lib/apt/lists/*\n",
      "     \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Here is my docker-compose.yml just in case\n",
      "\n",
      "    version: '3'\n",
      "    services:\n",
      "      m4carchiver-frontend:\n",
      "        image: node:18\n",
      "        working_dir: /app\n",
      "        environment:\n",
      "          - NODE_ENV=development\n",
      "        ports:\n",
      "          - \"3000:3000\"\n",
      "        volumes:\n",
      "          - ./frontend:/app\n",
      "        command: >\n",
      "          sh -c \"npm install && npm run dev\"\n",
      "        networks:\n",
      "          - m4carchiver_network\n",
      "    \n",
      "      m4carchiver-backend:\n",
      "        build:\n",
      "          context: ./backend\n",
      "        #image: oven/bun:latest\n",
      "        working_dir: /app\n",
      "        volumes:\n",
      "          - ./backend:/app\n",
      "        ports:\n",
      "          - \"80:80\"\n",
      "          - \"443:443\"\n",
      "          - \"4000:4000\"\n",
      "        command: >\n",
      "          sh -c \"bun run index.ts\"\n",
      "        networks:\n",
      "          - m4carchiver_network \n",
      "    \n",
      "    networks:\n",
      "      m4carchiver_network:\n",
      "        driver: bridge\n",
      "    \n",
      "\n",
      "  \n",
      "Then I open up a new browser\n",
      "\n",
      "        browser = (browser !== undefined) ? browser : await puppeteer.launch({\n",
      "            executablePath: '/usr/bin/google-chrome',\n",
      "            headless: 'new',\n",
      "            args: [ \n",
      "                '--disable-gpu',\n",
      "                '--disable-dev-shm-usage',\n",
      "                '--disable-setuid-sandbox',\n",
      "                '--no-sandbox'\n",
      "            ],\n",
      "        });\n",
      "    \n",
      "        //some other code...\n",
      "    \n",
      "        // Create a new page\n",
      "        const page = await browser.newPage();\n",
      "        //await page.setDefaultTimeout(0);\n",
      "        //await page.setRequestInterception(true);\n",
      "        //page.on('response', onResponse);\n",
      "    \n",
      "        // Navigate to the specified URL\n",
      "        console.log(\"Opening new page...\");\n",
      "        await page.goto('https://www.google.com/');\n",
      "        \n",
      "        // Wait for a moment to ensure the page is loaded\n",
      "        await Promise.race([\n",
      "            new Promise(resolve => setTimeout(resolve, 60000)),     // Timeout of 60000ms\n",
      "            page.waitForNavigation({ waitUntil: 'networkidle0' })   // Wait until there is no more network activity\n",
      "        ]);\n",
      "    \n",
      "        // Close the page\n",
      "        page.close();\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aj1jqp/puppeteer_keeps_timing_out_why_docker_puppeteer/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Udemy courses web scrapping\n",
      "Text: So there's a extension that's let you download any video you are currently watching (Video DownloadHelper) but there is a limitations to it you can download.\n",
      "\n",
      "Obviously you can copy url and can do manually but when there is site protection such as cloud flare then you can't do it using JDownloader directly you have to use the extension but then again there is a limit for free users.\n",
      "\n",
      "So is there a way to automate the scrapping the url and then somehow bypass cloudflare protection.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ai18r6/udemy_courses_web_scrapping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Someone need to build llm based webscrapping\n",
      "Text: I hate this!!!!   \n",
      "\n",
      "\n",
      "need llm based webscrapping tool which has good ux  Someone need to build this,   \n",
      "\n",
      "\n",
      "no one like to do this  \n",
      "\n",
      "\n",
      "need llm based webscrapping tool that has good ux  Someone need to build this,   \n",
      "\n",
      "\n",
      "https://preview.redd.it/gwc1qm3mtcgc1.png?width=898&format=png&auto=webp&s=7ae1f10351c577bbe49e69674dc8e9375de123c1\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ahtxq3/someone_need_to_build_llm_based_webscrapping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Chrome extension for PDF download automation\n",
      "Text: Hello everyone, \n",
      "\n",
      "I’m searching for a chrome extension (not an external app) that enables me to automate two specific actions without coding \n",
      "\n",
      "- automatically navigate from one page to another \n",
      "- clicking a button to download a PDF \n",
      "\n",
      "Any recommendations ? Thanks for the help !\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ah0cde/chrome_extension_for_pdf_download_automation/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help creating xpath\n",
      "Text: Hi, I’m brand new to webscraping and I’m doing some basic practice. Im stumped on creating xpaths for pos, age, G, Cmp, Att, and college. Here is link to data and what I have so far. \n",
      "\n",
      "https://www.pro-football-reference.com/years/2017/draft.htm\n",
      "URL: https://i.redd.it/xjsdvhrc72gc1.jpeg\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Monthly Self-Promotion Thread - February 2024\n",
      "Text: Hello and howdy, digital miners of /r/webscraping!\n",
      "\n",
      "The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n",
      "\n",
      "* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n",
      "* Maybe you've got a ground-breaking product in need of some intrepid testers?\n",
      "* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n",
      "* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n",
      "\n",
      "Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n",
      "\n",
      "Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ag8gb2/monthly_selfpromotion_thread_february_2024/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help extracting locations from store finder page\n",
      "Text: Hi there, I'm new here and am having a hard time navigating [this page](https://jolly-cannabis.com/a/store-locator/list), specifically how to extract the locations without having to copy them one by one. Anyone have any tips for someone who has little to no coding experience / doesn't have budget to join any apps?\n",
      "\n",
      "Thank you all for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1agkwhg/need_help_extracting_locations_from_store_finder/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there a zero budget way to scrape 20 TikTok links for public facing engagement metrics?\n",
      "Text: For context, we outsource work to TikTok creators. For our reports, it is such a manual and tedious process to manually navigate to each link and pull the number of views, saves, shares and comments into a spreadsheet.  \n",
      "\n",
      "\n",
      "It feels like such a simple, logical task to automate and wouldn't spam requests but I simply can't find success using ImportFromWeb/ImportFromXML formulas or even trying to create a GPT agent. \n",
      "\n",
      "Any advice at ALL would be appreciated!   \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ag8c2a/is_there_a_zero_budget_way_to_scrape_20_tiktok/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Self-built Python Webscraper not working... help?\n",
      "Text: Hello,\n",
      "\n",
      "I'm a beginner developer punching way above my weight.\n",
      "\n",
      "All I want is to feed into my python script two inputs: \n",
      "\n",
      "1. a .csv containing URLs to crawl\n",
      "2. a .csv to append scraped email addresses to\n",
      "\n",
      "Such that if I run:\n",
      "\n",
      "python3 pyScrape.py pyTest.webarchive-001.csv pyEmails.csv\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I should be able to open pyEmails.csv in x hours and have a list of emails.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Unfortunately when I run the script, nothing happens. No error, no output.\n",
      "\n",
      "I used Selenium because I read that it's best for navigating URLs and clicking more links.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "    import csv\n",
      "    import re\n",
      "    from urllib.parse import urlparse\n",
      "    from selenium import webdriver\n",
      "    from selenium.webdriver.chrome.options import Options\n",
      "    from concurrent.futures import ThreadPoolExecutor\n",
      "    \n",
      "    # Function to validate URL\n",
      "    def is_valid_url(url):\n",
      "      try:\n",
      "        result = urlparse(url)\n",
      "        return all([result.scheme, result.netloc])\n",
      "      except ValueError:\n",
      "        return False\n",
      "    \n",
      "    # Function to append data to a CSV file\n",
      "    def append_to_csv(file_path, data):\n",
      "      with open(file_path, 'a', newline='') as file:\n",
      "        writer = csv.writer(file)\n",
      "        writer.writerow(data)\n",
      "      print(f\"New email added: {data}\")\n",
      "    \n",
      "    def scrape_website(website, output_file_name):\n",
      "      options = Options()\n",
      "      options.add_argument('--headless')  # Run Chrome in headless mode\n",
      "      options.add_argument('--no-sandbox')\n",
      "      options.add_argument('--disable-dev-shm-usage')\n",
      "    \n",
      "      browser = webdriver.Chrome(options=options)\n",
      "    \n",
      "      if is_valid_url(website):\n",
      "        print(f\"Crawling URL {website}\")\n",
      "        try:\n",
      "          browser.get(website)\n",
      "          page_content = browser.page_source\n",
      "    \n",
      "          email_regex = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
      "          extracted_emails = re.findall(email_regex, page_content)\n",
      "    \n",
      "          if extracted_emails:\n",
      "            for email in extracted_emails:\n",
      "              append_to_csv(output_file_name, [email.lower()])\n",
      "    \n",
      "          # Extract links from the page\n",
      "          links = browser.find_elements_by_tag_name('a')\n",
      "          for link in links:\n",
      "            href = link.get_attribute('href')\n",
      "            if href:\n",
      "              if is_valid_url(href):\n",
      "                print(f\"Crawling Link {href}\")\n",
      "                try:\n",
      "                  browser.get(href)\n",
      "                  linked_page_content = browser.page_source\n",
      "                  linked_page_emails = re.findall(email_regex, linked_page_content)\n",
      "    \n",
      "                  if linked_page_emails:\n",
      "                    for email in linked_page_emails:\n",
      "                      append_to_csv(output_file_name, [email.lower()])\n",
      "                except Exception as e:\n",
      "                  print(f\"Error accessing link {href}: {str(e)}\")\n",
      "                  continue\n",
      "              elif href.startswith('mailto:'):\n",
      "                email = href[7:]\n",
      "                append_to_csv(output_file_name, [email.lower()])\n",
      "        except Exception as e:\n",
      "          print(f\"Error accessing {website}: {str(e)}\")\n",
      "          return\n",
      "    \n",
      "      browser.quit()\n",
      "    \n",
      "    def scrape_emails(websites_file, output_file_name):\n",
      "      print(\"Script started.\")\n",
      "      with open(websites_file, 'r') as file:\n",
      "        reader = csv.reader(file)\n",
      "        websites = [row[0] for row in reader]\n",
      "    \n",
      "      with ThreadPoolExecutor() as executor:\n",
      "        for website in websites:\n",
      "          executor.submit(scrape_website, website, output_file_name)\n",
      "    \n",
      "      print(f\"Scraping completed. The {output_file_name} file has been updated with new unique emails.\")\n",
      "    \n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ag6sbs/selfbuilt_python_webscraper_not_working_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I scrape all of the available URLs of a listing site?\n",
      "Text: How can I scrape all of the available URLs of a listing site? for example, I want to get all available URLs of a marketplace site. I guess they have around 3,000 items. I'm a python & javascript expert. I know how to scrape data from URLs that I can access to but I don't know how to get all URLs. Should I scrape from search engines? or is there a way to see website URLs directory?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ag1t5h/how_can_i_scrape_all_of_the_available_urls_of_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How legal is it to scrape data from TikTok\n",
      "Text: I'm working on a project where I need to access and display publicly available data from TikTok user profiles, such as engagement metrics and video statistics. I'm considering developing a web scraper for this purpose but I am uncertain about the legal implications.\n",
      "\n",
      "Has anyone here undertaken a similar project, and if so, how did you navigate these legal waters?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1affa2m/how_legal_is_it_to_scrape_data_from_tiktok/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Wealth Manager Software Providers\n",
      "Text: Within the Wealth Management space there are software providers that offer performance reporting (how is your portfolio doing over the past month, qtr, yr, etc.). \n",
      "\n",
      "Wealth manager websites have links to longin URLs. Within this URL it says the name of the service provider.\n",
      "\n",
      "I’m wondering if you can scrape the web and pull the wealth manager url along with the url of the service provider.\n",
      "\n",
      "Example\n",
      "\n",
      "Company URL - https://gentrustwm.com\n",
      "\n",
      "Client Login URL - https://gentrustwm.com/client_login\n",
      "\n",
      "Link for login -  https://id.addepar.com/oauth2/authorize?response_type=code&scope=session&client_id=iverson&state=%7B%7D&code_challenge=9fccba72f75becdec848eeed4c8b1e8ce958685e054d3dcda2180cad563d8d81&redirect_uri=https%3A%2F%2Fgtwm.addepar.com%2Foauth2%2Fcb&firm=gtwm-  \n",
      "\n",
      "I’m looking to grab the company URL and where it says Addepar. \n",
      "\n",
      "Is this possible?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1afirfk/wealth_manager_software_providers/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: ShapeSecurity's VM: Part 3\n",
      "Text: \n",
      "URL: https://www.botting.rocks/shapesecuritys-javascript-vm-part-3/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Bypassing ReCaptcha in 2024?\n",
      "Text: I'm currently trying to scrape a webpage periodically which sadly requires me to solve a Google Captcha. Are there any free or paid working methods to bypass ReCaptcha? The endpoint requires `captcha_response` and `visibleCaptcha` values to allow the request.\n",
      "\n",
      "Thanks in Advance\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1af1u0h/bypassing_recaptcha_in_2024/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any reliable source for my Scraping project.\n",
      "Text: I am looking to scrape all the  Optometry/Eye Care Clinics in the USA.\n",
      "\n",
      "Data I want to scrape are Practice Name, business email, and website, for each clinic.\n",
      "\n",
      "My initial thought for this was Google search paired with Clinic's website.\n",
      "\n",
      "What are your suggestions?a  and are there any public directories where I can find all the data easily.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1aeuirh/any_reliable_source_for_my_scraping_project/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Extracting data of a sub reddit.\n",
      "Text: Hi all.\n",
      "\n",
      "I am trying to extract all the data(data as in all posts) from a sub reddit. I am aware of PRAW but I think it has some limit. Is there any workarounds to do this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ady6zz/extracting_data_of_a_sub_reddit/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Python Web Scraping using asyncio (opinion needed)\n",
      "Text: I want to write an application that compiles links to national news bulletins from different sites using `asyncio` on Python and turns them into a bulletin containing personalized tags. Can you share your opinions about running `asyncio` with libraries such as `requests`, `selectolax` etc.?\n",
      "\n",
      "- Is this asynchronous programming necessary to write a structure that will make requests to multiple websites and compile and group the incoming links? Or is `time.sleep` enough?\n",
      "\n",
      "- Could it be more efficient to check links on pages with a simple web spider?\n",
      "\n",
      "- Apart from these, are there any alternative methods you can suggest?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ae1hsr/python_web_scraping_using_asyncio_opinion_needed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: AWS\n",
      "Text: Any one here has an experience with running Puppeteer, and puppeteer-cluster in AWS lambda and SQS?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1adtk6e/aws/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Booking.com scraping\n",
      "Text: Hello,\n",
      "\n",
      "I'm a student new to web scraping, currently working on a university project. We need to scrape data from booking.com, starting with 1 and extending to 37 pages.\n",
      "\n",
      "We attempted using a loop (for i in range(0, 901, 25)), but it's not working as expected. Could you please provide guidance on scraping all pages effectively?\n",
      "\n",
      "Here is the link if you need: [https://www.booking.com/searchresults.en-gb.html?label=gen173nr-1BCAEoggI46AdIM1gEaIgBiAEBmAEZuAEXyAEP2AEB6AEBiAIBqAIDuAKQqd2tBsACAdICJDFiM2JmMTk4LTI0ZDQtNDQ1YS1iM2JkLTExMGMxZDFjNDBkOdgCBeACAQ&sid=3ac9bb671d8a29bbb469e531db5d2ffd&aid=304142&ss=Londonas&ssne=Londonas&ssne\\_untouched=Londonas&lang=en-gb&src=index&dest\\_id=-2601889&dest\\_type=city&checkin=2024-05-03&checkout=2024-05-05&group\\_adults=2&no\\_rooms=1&group\\_children=0&nflt=ht\\_id%3D204&offset=0&soz=1&lang\\_changed=1](https://www.booking.com/searchresults.en-gb.html?label=gen173nr-1BCAEoggI46AdIM1gEaIgBiAEBmAEZuAEXyAEP2AEB6AEBiAIBqAIDuAKQqd2tBsACAdICJDFiM2JmMTk4LTI0ZDQtNDQ1YS1iM2JkLTExMGMxZDFjNDBkOdgCBeACAQ&sid=3ac9bb671d8a29bbb469e531db5d2ffd&aid=304142&ss=Londonas&ssne=Londonas&ssne_untouched=Londonas&lang=en-gb&src=index&dest_id=-2601889&dest_type=city&checkin=2024-05-03&checkout=2024-05-05&group_adults=2&no_rooms=1&group_children=0&nflt=ht_id%3D204&offset=0&soz=1&lang_changed=1) \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Thank you :))\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1adrcz9/bookingcom_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: scraping linkedin\n",
      "Text: I'm getting started with scraping linkedin. All I need is a local program to repeatedly process hundreds of linkedin URLs and determine if the respective URL account is active or not and fetching to three most recent posts. I don't want to use an API. One of the hurdles is surpassing linkedin blocking me because of repeated visits. Currently I'm running my code with vpn but that still requires me to reload an IP. How to surpass this hurdle and what other hurdles is it possible I face? any opinion is appreciated.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1admvxg/scraping_linkedin/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Bet 365 SCRAPE\n",
      "Text: Is there any website scraping the odds of bet365 or something similar? Showing the best odds or wtv?\n",
      "\n",
      "I want to scrape bet365 but seems like a difficult task, should I scrape the odds from another betting site or there is website re showing odds of bet365 online?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1adiein/bet_365_scrape/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking to Scrape A List of Thousands of Titles & Artists from A Website\n",
      "Text: Hello,\n",
      "\n",
      "I have a couple of websites that contain a list of thousands of movie and book titles and their artists/authors. They can't all fit in one page so there are maybe 30-50 per page. I want to grab the entire multi-page list and put it into a CSV list.\n",
      "\n",
      "My goal is to search my local libraries for these thousands of titles and see which are available for borrowing. It would take months or years to do this one at a time. I found a website where I could enter a book or movie title and it tells me if nearby libraries have it available. A great resource and it even accepts huge lists of titles to check, but the website requests an CSV file for that.\n",
      "\n",
      "The webpages I want to scrape the lists from are not open webpages and I must login to access. I thought I'd mention that in case credentials are an obstacle in web scraping.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1adi4qe/looking_to_scrape_a_list_of_thousands_of_titles/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking to scrape websites for business name, address of business, city, state.\n",
      "Text: I've never done a web scrape before, and looking to pull a bunch of location info (just basic as described in the title) to an excel sheet. Most of the websites have filters, and will want to utilize those filters. I am not a programmer, and looking for an easy solution that I can just pay for. I read something about Octoparse being decent? Looking for suggestions. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ad5ofa/looking_to_scrape_websites_for_business_name/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Amazon scraping\n",
      "Text:  \n",
      "\n",
      "I'm using [https://www.scraperapi.com/](https://www.scraperapi.com/) forscraping product data from Amazon, the problem i have is that sometimes the I don't receive fully rendered HTML. I know it sounds like the problem of this specific service, but It turns out that I can recreate this problem locally. I'm talking mainly about sections like: description and rich content. I'm scraping information once per day and I'm monitoring how those product data is changing, so I would like to get rid of random fluctuation through days.\n",
      "\n",
      "What I want is to ask you did you have simmilar problem? If yes, how did you solve it? If not, can you give me tips, how can i reach very high performance in scraping Amazon Data? The most important thing is consistency and reliability of scrapped data. I don't mind using 3th party solutions and I also don't see problem in large number of retries.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ade1d7/amazon_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to find clients to sell data to\n",
      "Text: Hello everyone! I recently learned Web Scraping with Python and now I'm facing a challenge.\n",
      "\n",
      "I don't need the data that I scrape, so I am looking to sell it.\n",
      "\n",
      "I'm looking for advice on where I can find clients to either:\n",
      "\n",
      "\\- Request certain data from me and buy it\n",
      "\n",
      "\\- Sell the data that I already have\n",
      "\n",
      "And which way is more preferable? Any help appreciated!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ad60t4/how_to_find_clients_to_sell_data_to/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I need to scrape bulk data of google business site URLs from the internet in my area. Is there any way to do that?\n",
      "Text: Ideally I am looking for some tool that will scrape from GMBs, Google Maps or any other directory businesses which have an active Google Business Site will be on. So i will need some kind of a scraper which would help me sort specifically for those.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1acxzf8/i_need_to_scrape_bulk_data_of_google_business/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Would like help scraping thousands of public domain stock images and sharing them for free\n",
      "Text: This is a long story to bear with me. So for some extra context, finding old stock photos isn't very easy. You can find blogs with a few of them here and there or old CD ROMs from the 90s, but if you want to find 30s-70s stock photography in bulk you're kinda screwed. These photos are really useful when wanting to create vintage looking advertisements and researching commercial art and historical aesthetics. During my search for these, I found H. Armstrong Roberts, basically the first stock photographer who started selling his photos commercially as early as 1920. Roberts library was distributed by a company named Retrofile in the 2000s along with an assortment of other old stock libraries that are currently unknown in their origin. Retrofile was bought in 2005 by Getty Images, and while a good chunk of those libaries existed on ClassicStock and RobertStock, those were also bought out by Getty later on. As of right now, Getty owns the entire Armstrong Roberts/Retrofile archive.\n",
      "\n",
      "Here's the thing: Roberts died in 1947. Not only did he die in 1947 but none of his photos had their copyright registered. Given that he died over 70 years ago, his entire body of work is public domain. Given that Getty also lists Retrofile's other archives under Roberts name even though he was already dead, theoretically these are also public domain until proof can be provided that they belonged to someone else. They are listed under his name, so they should be treated as such.\n",
      "\n",
      "Alamy currently hosts the ClassicStock/Roberts archive on their website, and while it isn't as complete as Getty's, what makes this special is images can be downloaded freely from Alamy using [downloader.la](https://downloader.la/). While they aren't super high quality, they are usable when working on a 1080p canvas.\n",
      "\n",
      "What I want to do is figure out a way to download the entire Roberts archive in bulk, watermark free, with the titles as their file names and then upload those to the Internet Archive (or some other file service). There is no reason why these photos should be behind a paywall and I think it would be of great use to many designers and researchers.\n",
      "\n",
      "I cannot program in Python or any other language, but I do have a very weak understanding in how to run said scripts and have done so before. I request someone who is interested in this project and would be open to writing a script that can download the entire collection in bulk using the same system that [downloader.la](https://downloader.la/) uses (or better if possible.)\n",
      "\n",
      "[The H. Armstrong Roberts collection can be viewed here.](https://www.alamy.com/stock-photo/?name=H.+ARMSTRONG+ROBERTS&pseudoid=CEA9A46E-33B4-4443-8B6A-26377F61F632&sortBy=relevant)\n",
      "\n",
      "I am not asking for you to store them yourself, I just need a python script or some other tool I can run and I'll take care of the files from there.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1acs0k8/would_like_help_scraping_thousands_of_public/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to bypass PerimeterX\n",
      "Text: PerimeterX (now Human Security) is an anti-bot software that leverages advanced machine learning and behavioral analytics to accurately identify and block malicious bot traffic in real-time. There is currently 153,000 websites using PerimeterX.\n",
      "\n",
      "# Recognize PerimeterX\n",
      "\n",
      "To identify PerimeterX's presence on a website, look for these characteristics:\n",
      "\n",
      "* **Internal property:** window.\\_pxAppId property.\n",
      "* **Collector XHR:** PerimeterX can operate with or without any external server, in case of external collector, those domains can be used: px-cdn.net, pxchk.net, px-client.net In case of an internal endpoint being used, it's format will likely follow this format: /rf8vapwA/xhr/api/v2/collector\n",
      "* **Cookies:** PerimeterX set the cookies: \\_px3, \\_pxhd, \\_px\\_vid\n",
      "\n",
      "# PerimeterX's Device Fingerprinting\n",
      "\n",
      "PerimeterX employs traditional techniques commonly observed in other anti-bot software, but with a particular focus on WebGL by assessing rendering capabilities. This approach goes beyond the usual strategy of merely collecting parameters and extensions, as seen in most anti-bot solutions.\n",
      "\n",
      "## General fingerprinting\n",
      "\n",
      "* `devicePixelRatio`\n",
      "* `hardwareConcurrency`\n",
      "* `localStorage`\n",
      "* `indexedDB`\n",
      "* `openDatabase`\n",
      "* `sessionStorage`\n",
      "* `cpuClass`\n",
      "* `Navigator.plugins`\n",
      "* `window.performance`\n",
      "\n",
      "## Behavior Analysis\n",
      "\n",
      "PerimeterX observes various specified events and compiles them into a consistent payload for their collector API endpoint. This suggests that they utilize behavioral analysis. As a result, it's important to be careful when activating these events. To closely replicate human interactions with the page, it is advisable to employ simulation libraries.\n",
      "\n",
      "* `touchstart`\n",
      "* `touchend`\n",
      "* `touchmove`\n",
      "* `touchenter`\n",
      "* `touchleave`\n",
      "* `touchcancel`\n",
      "* `mousedown`\n",
      "* `mouseup`\n",
      "* `mousemove`\n",
      "* `mouseover`\n",
      "* `mouseout`\n",
      "* `mouseenter`\n",
      "* `mouseleave`\n",
      "* `click`\n",
      "* `dblclick`\n",
      "* `scroll`\n",
      "* `wheel`\n",
      "\n",
      "`mouse` type events coordinates and details are tracked on the following attributes:\n",
      "\n",
      "* `coordination_start`\n",
      "* `coordination_end`\n",
      "* `movementX`\n",
      "* `movementY`\n",
      "* `clientX`\n",
      "* `clientY`\n",
      "\n",
      "For touch type events, the following attributes are tracked:\n",
      "\n",
      "* `touches`\n",
      "* `changedTouches`\n",
      "\n",
      "## Canvas Fingerprinting\n",
      "\n",
      "PerimeterX utilizes a technique involving the use of unicode special characters for fingerprinting canvas renderings. This method is based on the significant variability in how unicode renders high-entropy elements like emojis.\n",
      "\n",
      "**First test:** Renders all characters using \"8px sans-serif\" default font from `0x1F600` to `0x1F64F`.\n",
      "\n",
      "    a.font = \"8px sans-serif\";\n",
      "    for (var o = 1, c = 128512; c < 128591; c++)\n",
      "       a.fillText(_(\"0x\" + c.toString(16)), 8 * o, 8),\n",
      "       o++;\n",
      "    n = Q(a.canvas.toDataURL())\n",
      "\n",
      "**Second test:** Renders the following characters using \"6px sans-serif\" default font:\n",
      "\n",
      "    97, 667, 917, 1050, 1344, 1488, 1575, 1808, 1931, 2342, 2476, 2583, 2711, 2825, 2980, 3108, 3221, 3374, 3517, 3524, 3652, 3749, 3926, 4121, 4325, 4877, 5091, 5123, 6017, 6190, 6682, 7070, 11612, 20206, 27721, 41352, 43415, 54620, 55295\n",
      "\n",
      "And then the characters from 0x2699 to 0x26FF.\n",
      "\n",
      "**Results:** [https://imgur.com/a/oCyUD48](https://imgur.com/a/oCyUD48)\n",
      "\n",
      ">Notice: Result original resolution has been preserved, notice how small and pixely the rendering is, this allow better fingerprinting of anti-aliasing technique, and produce more entropy while being lighter.\n",
      "\n",
      "## WebGL fingerprinting:\n",
      "\n",
      "PerimeterX checks for specific Anisotropic extension to detect browser type, among those values:\n",
      "\n",
      "* `EXT_texture_filter_anisotropic`\n",
      "* `WEBKIT_EXT_texture_filter_anisotropic` (Safari)\n",
      "* `MOZ_EXT_texture_filter_anisotropic` (Firefox)\n",
      "\n",
      "This method is highly efficient for identifying browser type spoofing because the properties involved cannot be falsified through traditional JavaScript proxy or Function override techniques.\n",
      "\n",
      "**Attributes fingerprint:** PerimeterX check for the following WebGL attributes:\n",
      "\n",
      "* `RENDERER`\n",
      "* `SHADING_LANGUAGE_VERSION`\n",
      "* `VENDOR`\n",
      "* `VERSION`\n",
      "* `UNMASKED_VENDOR_WEBGL`\n",
      "* `UNMASKED_RENDERER_WEBGL`\n",
      "\n",
      "**WEBGL\\_debug\\_renderer\\_info:**\n",
      "\n",
      "* `ALIASED_LINE_WIDTH_RANGE`\n",
      "* `ALIASED_POINT_SIZE_RANGE`\n",
      "* `ALPHA_BITS`\n",
      "* `BLUE_BITS`\n",
      "* `DEPTH_BITS`\n",
      "* `GREEN_BITS`\n",
      "* `MAX_COMBINED_TEXTURE_IMAGE_UNITS`\n",
      "* `MAX_CUBE_MAP_TEXTURE_SIZE`\n",
      "* `MAX_FRAGMENT_UNIFORM_VECTORS`\n",
      "* `MAX_RENDERBUFFER_SIZE`\n",
      "* `MAX_TEXTURE_IMAGE_UNITS`\n",
      "* `MAX_TEXTURE_SIZE`\n",
      "* `MAX_VARYING_VECTORS`\n",
      "* `MAX_VERTEX_ATTRIBS`\n",
      "* `MAX_VERTEX_TEXTURE_IMAGE_UNITS`\n",
      "* `MAX_VERTEX_UNIFORM_VECTORS`\n",
      "* `MAX_VIEWPORT_DIMS`\n",
      "* `STENCIL_BITS`\n",
      "\n",
      "**getShaderPrecisionFormat:**\n",
      "\n",
      "* `VERTEX_SHADER`\n",
      "* `FRAGMENT_SHADER`\n",
      "* `VERTEX_SHADER`\n",
      "* `FRAGMENT_SHADER`\n",
      "* `HIGH_FLOAT`\n",
      "* `MEDIUM_FLOAT`\n",
      "* `LOW_FLOAT`\n",
      "\n",
      "**Rendering fingerprint:**\n",
      "\n",
      "Execute the following shaders for WebGL fingerprinting:\n",
      "\n",
      "**Vertex Shader:**\n",
      "\n",
      "    attribute vec2 attrVertex;\n",
      "    varying vec2 varyinTexCoordinate;\n",
      "    uniform vec2 uniformOffset;\n",
      "    \n",
      "    void main(){\n",
      "       varyinTexCoordinate = attrVertex + uniformOffset;\n",
      "       gl_Position = vec4(attrVertex,0,1);\n",
      "    }\n",
      "\n",
      "**Fragment Shader:**\n",
      "\n",
      "    precision mediump float;\n",
      "    varying vec2 varyinTexCoordinate;\n",
      "    void main() {\n",
      "       gl_FragColor = vec4(varyinTexCoordinate,0,1);\n",
      "    }\n",
      "\n",
      "Both are executed on a single program and then dumped using canvas.toDataURL.\n",
      "\n",
      "## Clipboard data\n",
      "\n",
      "Not sure how much this influence the bypass success rate, but PerimeterX does look for Clipboard Data as part of their fingerprinting process.\n",
      "\n",
      "## Fonts presence & rendering\n",
      "\n",
      "PerimeterX will attempt to render the text `mmmmmmmmmmlli` for the following fonts list:\n",
      "\n",
      "    \"Andale Mono\", \"Arial\", \"Arial Black\", \"Arial Hebrew\", \"Arial MT\", \"Arial Narrow\", \"Arial Rounded MT Bold\", \"Arial Unicode MS\", \"Bitstream Vera Sans Mono\", \"Book Antiqua\", \"Bookman Old Style\", \"Calibri\", \"Cambria\", \"Cambria Math\", \"Century\", \"Century Gothic\", \"Century Schoolbook\", \"Comic Sans\", ...\n",
      "\n",
      "# How to bypass PerimeterX\n",
      "\n",
      "Bypassing PerimterX's security requires a nuanced approach, especially considering its reliance on GPU rendering information to determine the operating system and device type.\n",
      "\n",
      "Here are some strategies:\n",
      "\n",
      "1. **GPU Rendering:** Emulate consumer-grade GPUs rather than professional hardware, as Akamai's algorithms are tuned to recognize and differentiate between them.\n",
      "2. **Behavioral Analysis:** Utilize tools like ghost-cursor (found at [https://npmjs.com/package/ghost-cursor](https://npmjs.com/package/ghost-cursor)) to simulate human-like cursor movements and keystrokes. Timing is crucial here; movements or keystrokes that are too rapid can be flagged as suspicious.\n",
      "\n",
      "# Learn more:\n",
      "\n",
      "We're talking about multiple ways to bypass PerimeterX, Akamai, NuData on the Web Scraping & Data Extraction discord server!\n",
      "\n",
      "Come say hi!\n",
      "\n",
      "[https://discord.com/invite/fHbbHTq4CQ](https://discord.com/invite/fHbbHTq4CQ)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ac34ob/how_to_bypass_perimeterx/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Downloading all pdfs (help)\n",
      "Text: Ive currently started working as a tutor and I'm trying to download all the resources (pdf files) from [https://www.physicsandmathstutor.com/](https://www.physicsandmathstutor.com/) e.g. past papers and quiz sheets\n",
      "\n",
      "Ive tried beautiful soup but it doesn't seem to folder them but instead just download everything\n",
      "\n",
      "e.g.[https://www.physicsandmathstutor.com/past-papers/gcse-maths/aqa-paper-1/](https://www.physicsandmathstutor.com/past-papers/gcse-maths/aqa-paper-1/) all the pdfs from this page download to the folder \\\\documents\\\\past-papers\\\\gcse-maths\\\\aqa-paper-1\\\\example-exam-paper.pdf\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Could anyone tell me if there is an easier way?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1acgab9/downloading_all_pdfs_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any help on how can I scrape resumes online from sources like LinkedIn and any other sites\n",
      "Text: I want to scrape around 5000 resumes and I don't know from where to start any help\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ac8xdh/any_help_on_how_can_i_scrape_resumes_online_from/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape Data From an Android App?\n",
      "Text: I am looking for a way to scrape data from an Android App for educational purposes. There is an app I have in mind that sells groceries and I would like to scrape the items along with their prices. From research I understand that to do this I need an emulator + HTTP Proxy, reverse engineer the API call that receives this data and use python requests to capture it. I have never worked with Emulators and HTTP proxies before; so here are the questions:\n",
      "\n",
      "1) Is there an easier way to do it? \n",
      "\n",
      "2) are there open source/free tools for HTTP proxy and emulators? I found ones online and they seem to be for a price.\n",
      "\n",
      "3) Any proper threads/resource I can use for educational purposes? I basically learned web scraping through selenium, beautiful soup and API but all from web pages. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "much thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1abp9wb/scrape_data_from_an_android_app/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to \"submit\" injected reCaptcha v2 solution when no button identified? (Selenium - Python)\n",
      "Text: Below is what I have so far, but it doesn't work.. I found a textarea marked `g-recaptcha-response` and injected into it a TwoCaptcha-generated solution. But I have no way to \"Submit\" it. Cannot locate any button and focusing on it and pressing Enter doesn't seem to work unless I'm doing something wrong.\n",
      "\n",
      "Any solutions are welcome.. The site generating the reCaptcha v2 prompts the challenge upon clicking \"submit\" on my search request. No \"Submit\" inside the reCaptcha area was found, just \"Verify\".\n",
      "\n",
      "target url: https://urlscan.io/\n",
      "\n",
      "```\n",
      "        # Solve the reCAPTCHA\n",
      "        result = solver.recaptcha(sitekey, driver.current_url)\n",
      "\n",
      "        # Inject the result into the hidden textarea\n",
      "        driver.execute_script('document.getElementById(\"g-recaptcha-response\").innerHTML = \"%s\"' % result['code'])\n",
      "\n",
      "        # Make the g-recaptcha-response textarea visible for interaction\n",
      "        recaptcha_response_area = driver.find_element(By.ID, \"g-recaptcha-response\")\n",
      "        driver.execute_script(\"arguments[0].style.display = 'block';\", recaptcha_response_area)\n",
      "\n",
      "        # Create an ActionChain to perform keyboard actions\n",
      "        actions = ActionChains(driver)\n",
      "\n",
      "        # Move to the textarea, focus on it, and simulate the Enter key press\n",
      "        actions.move_to_element(recaptcha_response_area).click().send_keys(Keys.ENTER).perform()\n",
      "\n",
      "```\n",
      "\n",
      "edit: formatting\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1abrqvs/how_to_submit_injected_recaptcha_v2_solution_when/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I optimise my web scraper?\n",
      "Text: I am a learning web scraping and want to build a python bot to track the prices for mobile phones on different websites like amazon, ebay etc.\n",
      "\n",
      "I have a list of 20-25 websites to start scraping from, some of these websites load data on runtime so I will need to use something like a headless browser for scraping. How scalable is this approach ? Also, are theere any alternative approaches to this ?\n",
      "\n",
      "Also the websites on my list are very different, is there a way  I can make a general scraper for all the websites?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1abh1ro/how_can_i_optimise_my_web_scraper/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to Build a Price Tracking Bot that utilizes real-time data 24/7\n",
      "Text: I see many people on Twitter create a price tracking bot which tracks real-time data of when a product drops in price.\n",
      "\n",
      "They get this data immediately, right when it drops. I'm not sure how this is possible for them to get real-time data without them getting rate limited.\n",
      "\n",
      "The only way I see that's possible is that they are constantly making the HTTP Request to the specific product 24/7 every second. But this seems too expensive. Especially since their price tracking bots can track thousands and thousands of products.\n",
      "\n",
      "So what technique are they using to get real-time data for when a product changes prices?\n",
      "\n",
      "If I were to currently attempt to make one, I would be forced to check prices like every hour or something(so I don't go over the rate limit). How are they bypassing that?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ab8bn4/how_to_build_a_price_tracking_bot_that_utilizes/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: In need of a simple tool to search a website for words that are typically not searchable.\n",
      "Text: Hello,  \n",
      "I need a tool that can pull up a search on a website for info that is typically shown only to logged in users but not searchable.\n",
      "\n",
      "To be more precise, this is the website: [https://ecase.justice.bg/Case](https://ecase.justice.bg/Case)\n",
      "\n",
      "Every single day courts from all around the country upload new cases with parties involved in them. The info is „public“ to logged in lawyers. To the rest of the public it’s also visible but names are „blanked out“ - James Brown would be J. B. \n",
      "\n",
      "In the website, one can search by numbers/years/courts and so on, not by name of a party involved in a case, and the info is there, it’s just not searchable. \n",
      "\n",
      "Googling the website and a name sometimes works for a couple of results but it’s not very helpful because it’s cases from way back that google has indexed.\n",
      "\n",
      "Is there any tool that I can sort of modify to search on the backend of the website and give me the case number/year that this person/company is involved in?   \n",
      "Something like the reddit camas search from years ago?  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1ab9pi0/in_need_of_a_simple_tool_to_search_a_website_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Instagram's ?__a=1&__d=dis no longer working\n",
      "Text: Instagram's \n",
      "\n",
      "    ?__a=1&__d=dis\n",
      "\n",
      "\n",
      "no longer seems to work, anyone has workarounds? Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19fgptv/instagrams_a1_ddis_no_longer_working/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Betting website scraping\n",
      "Text: Hello, I was wondering is it still possible to scrape bet365 with ease?\n",
      "\n",
      "If not, what alternative website would you recommend I switch to?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19fkctk/betting_website_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Livesource Sport\n",
      "Text: Hello, I am looking for away to watch sport live, or the closest to live?\n",
      "\n",
      "Is there any website streaming that is close to live without much delay, like 1-2 secondes delay?\n",
      "\n",
      "If you also have an api that say the stats of the game I would be interested in live too or mini delay.\n",
      "\n",
      "Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19fkcf0/livesource_sport/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can someone help me reverse engineer this store locator map?\n",
      "Text: I have never seen this before. I am trying to extract the store locations from this map and there is almost 0 indication of where the locations are stored or how they are fed into the map on the website.\n",
      "\n",
      "here is the url:  \n",
      "[https://www.waterdrop.fr/pages/revendeurs](https://www.waterdrop.fr/pages/revendeurs)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I have scoured the page source and the network tab for any indications but cannot find any. Any suggestions or recommendations would be highly appreciated!\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Tta\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19faxu3/can_someone_help_me_reverse_engineer_this_store/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How feasible is automatically scraping logo on large scale?\n",
      "Text: Say I have a list of many websites, and I want to scrape the logo from each of them. This process will need to be mostly automatic since the number of websites is huge (i.e. I can't read the html for each site). How feasible do you guys think this task is?\n",
      "\n",
      "I'm thinking about third-party solution. But not sure how reliable they are. Currently I'm considering [hexomatic.com](https://hexomatic.com) and [ritekit.com](https://ritekit.com). Does anyone have experience with these sites?\n",
      "\n",
      "Do you guys  have any tips on how to approach this kind of scraping? Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19f4dns/how_feasible_is_automatically_scraping_logo_on/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there a way to scrape Facebook business pages by creation date?\n",
      "Text: I need to find businesses created in 2023 or 2024.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19f1trf/is_there_a_way_to_scrape_facebook_business_pages/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Error: Scrapy : pyasn modules\n",
      "Text: Ok so i dont know what happened , but this started popping out, i didnt use scrapy for like months ... and then when i started working on a new project this happened ,   \n",
      "\n",
      "\n",
      "some info: \n",
      "\n",
      "On debian bookworm , using conda , even tried with python virtual environment, tried global installation tooo , python version 3.11.5.   \n",
      "\n",
      "\n",
      "Tried googling and suggestions were to try force upgrading pyasn mod's and even after that nothing so .......anyone facign the issue ? \n",
      "\n",
      "https://preview.redd.it/pigd0c261jec1.png?width=958&format=png&auto=webp&s=b310e2f8cc96d7edf9e5e4319d390b5b1e5a5f78\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19f2jmj/error_scrapy_pyasn_modules/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: ShapeSecurity's VM\n",
      "Text: Hello guys 8 days ago I started writing a series about ShapeSecurity's VM. This is one of the hardest antibots to reverse due to their dynamic Javascript VM that is a pain-in-the-ass to reverse.   \n",
      "\n",
      "\n",
      "One of the last good open-source repos about about ShapeSecurity was almost 5 years ago  \n",
      "[https://github.com/sonya75/starbucks-botdetection-cracked](https://github.com/sonya75/starbucks-botdetection-cracked)  \n",
      "\n",
      "\n",
      "Anyways here are part 1 and 2  \n",
      "[https://www.botting.rocks/shapesecuritys-javascript-vm-part-1/](https://www.botting.rocks/shapesecuritys-javascript-vm-part-2/)\n",
      "\n",
      "[https://www.botting.rocks/shapesecuritys-javascript-vm-part-2/](https://www.botting.rocks/shapesecuritys-javascript-vm-part-2/)  \n",
      "\n",
      "\n",
      "I still have a few more parts to write about and then I will start writing more about Incapsula.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19emrv2/shapesecuritys_vm/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: SeleniumBase and scrap speed\n",
      "Text: Seems that SeleniumBase waits about 4 seconds after the page is fully loaded, and I can't see the reason\n",
      "\n",
      "I have this demo code:\n",
      "\n",
      "    \n",
      "    from seleniumbase import Driver\n",
      "    driver = Driver(uc=True)\n",
      "    \n",
      "    def custom_get(url):\n",
      "        global driver\n",
      "        import time\n",
      "        epoch_time = int(time.time())\n",
      "        print(\"starting...\")\n",
      "        driver.uc_open_with_tab(url)\n",
      "        if not \"mysite\" in driver.get_title():\n",
      "            print(\"Error, getting new tab\")\n",
      "            driver.uc_open_with_reconnect(\n",
      "                url, reconnect_time=2\n",
      "            )\n",
      "        if not \"mysite\" in driver.get_title():\n",
      "            print(\"Error, CF is still there\")\n",
      "            if driver.is_element_visible('iframe[src*=\"challenge\"]'):\n",
      "                with driver.frame_switch('iframe[src*=\"challenge\"]'):\n",
      "                    driver.click(\"span.mark\")\n",
      "                    driver.sleep(2)\n",
      "        # print seconds passed\n",
      "        print(\"!seconds passed:\", int(time.time()) - epoch_time)\n",
      "        print(\"------------------ end\")\n",
      "    \n",
      "    custom_get(\"https://mysite\")\n",
      "    custom_get(\"https://mysite/\")\n",
      "    custom_get(\"https://mysite/\")\n",
      "    custom_get(\"https://mysite\")\n",
      "    custom_get(\"https://mysite/\")\n",
      "    custom_get(\"https://mysite/\")\n",
      "    custom_get(\"https://mysite/\")\n",
      "    exit()\n",
      "    \n",
      "\n",
      "but i get one request per 5 seconds, even if the page is fully rendered under 1 second  \n",
      "\n",
      "\n",
      "    starting...\n",
      "    !seconds passed: 5\n",
      "    ------------------ end\n",
      "    starting...\n",
      "    !seconds passed: 5\n",
      "    ------------------ end\n",
      "    starting...\n",
      "    !seconds passed: 5\n",
      "    ------------------ end\n",
      "    starting...\n",
      "    !seconds passed: 5\n",
      "    ------------------ end\n",
      "    starting...\n",
      "    !seconds passed: 5\n",
      "    ------------------ end\n",
      "    starting...\n",
      "    !seconds passed: 5\n",
      "    ------------------ end\n",
      "    starting...\n",
      "    !seconds passed: 5\n",
      "    ------------------ end\n",
      "    \n",
      "    Process finished with exit code 0\n",
      "    \n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19etvqr/seleniumbase_and_scrap_speed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Login on webpage using Playwright\n",
      "Text: Hi all, \n",
      "\n",
      "I wrote some Python code to login on a website called s[portsbet.com.au](https://Sportsbet.com.au). This code works flawless in my Windows environment. However, when I run it in a Linux environment with xvfb (I need the headed mode) then the code is unable to login in since it is detected as a bot. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "How can I resolve this?  Added the code below (with obviously, fake credentials)  \n",
      "\n",
      "`from playwright.sync_api import sync_playwright`  \n",
      "`import time`  \n",
      "`CHROMIUM_ARGS= [`  \n",
      " `'--no-sandbox',`  \n",
      " `'--disable-setuid-sandbox',`  \n",
      " `'--no-first-run',`  \n",
      " `'--disable-blink-features=AutomationControlled',`  \n",
      " `'--disable-dev-shm-usage'`  \n",
      "`]`  \n",
      "`playwright = sync_playwright().start()`  \n",
      "\n",
      "\n",
      "`browser = playwright.chromium.launch(headless=False, slow_mo=200, args=CHROMIUM_ARGS,ignore_default_args=[\"--enable-automation\"])`  \n",
      "`context = browser.new_context(`  \n",
      " `user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'`  \n",
      "`)`  \n",
      "`page = browser.new_page()`  \n",
      "`page.set_viewport_size({\"width\": 1600, \"height\": 1200})`  \n",
      "`page.goto(\"https://www.sportsbet.com.au/\")`  \n",
      "`time.sleep(3)`  \n",
      "`login_menu = page.locator('[data-automation-id=\"header-login-touchable\"]')`  \n",
      "`login_menu.click()`  \n",
      "`time.sleep(1)`  \n",
      "`username_button = page.locator(\"#username\")`  \n",
      "`password_button = page.locator(\"#password\")`  \n",
      "`username_button.fill(\"user\")`  \n",
      "`password_button.fill(\"pass\")`  \n",
      "`login_submit = page.locator('[data-automation-id=\"login-button\"]')`  \n",
      "`login_submit.click()`  \n",
      "`time.sleep(20)`  \n",
      "`browser.close()`\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19eo4x2/login_on_webpage_using_playwright/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Transform Your Android Phone into an Efficient Proxy Server\n",
      "Text:  First, you need to install the Localtonet app on your phone by download it from the Google Play Store here: [https://play.google.com/store/apps/details?id=com.localtonet.localtonetapp](https://play.google.com/store/apps/details?id=com.localtonet.localtonetapp)\n",
      "\n",
      "Next, go to [https://localtonet.com](https://localtonet.com/) and sign up for an account. Once you're signed up, go to the clipboard page and copy the AuthToken.\n",
      "\n",
      "On the Localtonet app on your phone, paste the AuthToken in. You should see an Android icon on the \"My tokens\" page.\n",
      "\n",
      "You can adjust the settings for how often the phone goes into airplane mode, and you can also generate a reset link.\n",
      "\n",
      "If  your phone is not rooted, when you generate a reset link, trigger it  once and the default assistant settings page will open. From there,  select Localtonet as the default assistant.\n",
      "\n",
      "Next, go to [https://localtonet.com/tunnel/proxyserver](https://localtonet.com/tunnel/proxyserver) and choose either HTTP or SOCKS proxy, then create it. Click \"Start\" to start the proxy server.\n",
      "\n",
      "You  can connect to the server using the IP address and port number  provided. You can also set a username and password in the tunnel  settings.\n",
      "\n",
      "Note that the SOCKS5 proxy now supports TCP/UDP.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19ebpvn/transform_your_android_phone_into_an_efficient/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Article results based on keywords\n",
      "Text: How would I go about scraping for articles and websites online that contain user given keyword.\n",
      "Like a python program where you input a keyword and the output is all the article/website urls that contain the keyword.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19ea5l5/article_results_based_on_keywords/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web scraping Udemy with Scrapy and Splash\n",
      "Text: Hi everyone, I'm having trouble scraping Udemy websites for courses data. For example, I tried to scrape /topic/web-development and extract its urls to courses. However, theses URLs are returned dynamically. So, I used splash and tried to write Lua script to wait for a few seconds to let the urls load but does not seem to work. Anyone has experience with this please?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19e7fri/web_scraping_udemy_with_scrapy_and_splash/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Anyone launching in container/kubernetes?\n",
      "Text: Was curious if people here scrape in bulk via multiple docker container/kubernetes or similar setups (proxmox)?\n",
      "\n",
      "I'm going to need to scrape dynamic content so can't really run headless or just request afaik.\n",
      "\n",
      "How much ram/container would I need on average? How many container can run on one thread if there is a delay (say 5 seconds per container after action/load)?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19dvk0a/anyone_launching_in_containerkubernetes/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Want to capture regular updates from a list of 100 websites\n",
      "Text: Specifically, I want to monitor a set of websites that publish updates to the law or any new laws. I just want for a tool to detect whenever there are updates/new additions to these sites and alert me and return the new content. I don’t know anything about webscraping, I have a little bit of programming experience.\n",
      "\n",
      "Is webscraping the right task for this? Are there any free to use webscraping tools that could do this for me?\n",
      "\n",
      "Also, could there be a functionality that filters new content by topic of law like by keywords, so to update me whenever a new content is added to one of the websites with these keywords in it?\n",
      "\n",
      "Is web scraping the right approach for this? Sorry, I’m not very knowledgeable.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19dxf0m/want_to_capture_regular_updates_from_a_list_of/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: vbox7 scraper made pure in go\n",
      "Text:  Hello everyone, with the recent announcement of vbox7 shout down, I created a bot so you can download the videos on your machine\n",
      "\n",
      "[https://github.com/johnbalvin/vbox7](https://github.com/johnbalvin/vbox7)\n",
      "\n",
      "I already deployed to lambda you can test it:\n",
      "\n",
      "[https://paanu7ylzlapritxhdkk3642ti0blceg.lambda-url.us-east-1.on.aws/?id=c6eee5c948&format=htm](https://paanu7ylzlapritxhdkk3642ti0blceg.lambda-url.us-east-1.on.aws/?id=c6eee5c948&format=htm)\n",
      "\n",
      "[https://paanu7ylzlapritxhdkk3642ti0blceg.lambda-url.us-east-1.on.aws/?id=3edf5e0539&format=html](https://paanu7ylzlapritxhdkk3642ti0blceg.lambda-url.us-east-1.on.aws/?id=3edf5e0539&format=html)\n",
      "\n",
      "[https://paanu7ylzlapritxhdkk3642ti0blceg.lambda-url.us-east-1.on.aws/?id=9b5b912a3f&format=html](https://paanu7ylzlapritxhdkk3642ti0blceg.lambda-url.us-east-1.on.aws/?id=9b5b912a3f&format=html)\n",
      "\n",
      "This will give only the final video url, I didn't deploy the full video download because the bill will increase a lot if I do it, but you can implement it yourself\n",
      "\n",
      "source: [https://www.reddit.com/r/DataHoarder/comments/19d8omx/this\\_videosharing\\_website\\_is\\_going\\_to\\_delete\\_99/](https://www.reddit.com/r/DataHoarder/comments/19d8omx/this_videosharing_website_is_going_to_delete_99/)\n",
      "\n",
      "let me know what you want me to add to to the bot\n",
      "\n",
      "Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19dpq7p/vbox7_scraper_made_pure_in_go/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: 403 Forbidden errors when requesting data from Indeed.\n",
      "Text: I am looking to build a job posting web scraper. I plan on scraping data off of Indeed, Linkedin, and Monster. I am currently using Cheerio and Axios to request and parse the HTML from Indeed but I am always getting a 403 forbidden error. I am looking for resources or tools that are available to get around this error.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19ducw4/403_forbidden_errors_when_requesting_data_from/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Had anyone done scraping on iPad apps before?\n",
      "Text: need help or at least some discussions\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19dty9d/had_anyone_done_scraping_on_ipad_apps_before/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Launch ephemeral headed chrome instances in Docker\n",
      "Text: Run your bots on Chrome in an isolated graphical environment with this Docker image and run script I wrote:\n",
      "\n",
      "* Prints out the WebSocket URL when the you run the launch script. This can be used with Playwright automation\n",
      "* Starts a VNC server in the Docker container, so you can enter and interact with the graphical environment.\n",
      "* TODO: Programattically modify the Xvfb/fluxbox parameters to produce reproducible/unique canvas fingerprints.\n",
      "\n",
      "[https://github.com/gbiz123/docker-chrome-vnc](https://github.com/gbiz123/docker-chrome-vnc)\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19dheqw/launch_ephemeral_headed_chrome_instances_in_docker/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: This videosharing website is going to delete 99% of user videos in a month, how can we download most of it?\n",
      "Text: \n",
      "URL: /r/DataHoarder/comments/19d8omx/this_videosharing_website_is_going_to_delete_99/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping a CDN of it's images and videos and etc...\n",
      "Text:  Hey I want to scrape all the images video and other files from this cdn link.([https://cdncf.ahlan.live/goods\\_list/anim/](https://cdncf.ahlan.live/goods_list/anim/2c52ffa5fbe690fbbd1d5f24cc83ed90.a?v=202401220702)) but i dont know how i should do it \n",
      "URL: https://www.reddit.com/r/webscraping/comments/19djpb4/scraping_a_cdn_of_its_images_and_videos_and_etc/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can't interact with website on scraper that is using selenium and working on docker container\n",
      "Text: I have a scraper that I want to run inside my pythton 3.8-slim docker container.\n",
      "\n",
      "I want to use selenium to open a headless browser, enter data to input boxes, submit via clicking a button and get results from the response page.\n",
      "\n",
      "I have everything ready and working on my local. But when I move them to the docker, I just can't seem to be able to click on the button.\n",
      "\n",
      "When I make some search about this I think it's because my OS in the container doesn't have GUI available and that's why?\n",
      "\n",
      "How can I do this, I can switch from selenium to playwright or similar stacks.\n",
      "\n",
      "Anyone with past exp/ideas?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19dix7p/cant_interact_with_website_on_scraper_that_is/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I don't use Scrapy. Am I missing out?\n",
      "Text: I tried out Scrapy some times ago, but I find it restrictive and not intuitive to me. I find the selector useful though. Hence currently my flow is request/selenium to get html > scrapy selector to parse > sql alchemy to transfer to db. And it works well. \n",
      "\n",
      "But I still have a nagging feeling that I may miss something, since Scrapy is the most common scraping framework. Hence I want to check with you guys if I miss out anything for not using Scrapy?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19ctve5/i_dont_use_scrapy_am_i_missing_out/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trying to get all pin ids in a pinterest board\n",
      "Text: I'm trying to create a free Chrome extension (that ideally wouldn't rely on the Pinterest APIs) that would allow the user to cycle through the pins in a pinterest board by using the left + right arrow keys.\n",
      "\n",
      "Seems like the ids in a board are already ordered correctly, so having ALL the ids in a board should allow for the navigation. \n",
      "\n",
      "Using the DOM, it seems like you can only get about 10 pins as it's dynamically generated. Any thoughts on how to proceed? Has anyone done this before?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19czzei/trying_to_get_all_pin_ids_in_a_pinterest_board/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selenium - issues on a scraper that runs on google-colab\n",
      "Text: &#x200B;\n",
      "\n",
      " \n",
      "\n",
      "i am trying to get data form a page\n",
      "\n",
      "see url = \"[https://clutch.co/il/it-services](https://clutch.co/il/it-services)\"\n",
      "\n",
      "The website i am trying to scrap from probably has some sort of anti-bot protection with CloudFlare or similar services, hence the scrapper need to use selenium with a headless browser like Headless Chrome or PhantomJS. Selenium automates a real browser, which can navigate Cloudflare's anti-bot pages just like a human user.\n",
      "\n",
      "Here's how i use selenium to imitate a real human browser interaction:\n",
      "\n",
      "but on Google-Colab it does not work propperly\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "    \n",
      "    import pandas as pd\n",
      "    from bs4 import BeautifulSoup\n",
      "    from tabulate import tabulate\n",
      "    from selenium import webdriver\n",
      "    from selenium.webdriver.chrome.options import Options\n",
      "    \n",
      "    options = Options()\n",
      "    options.headless = True\n",
      "    driver = webdriver.Chrome(options=options)\n",
      "    \n",
      "    url = \"https://clutch.co/il/it-services\"\n",
      "    driver.get(url)\n",
      "    \n",
      "    html = driver.page_source\n",
      "    soup = BeautifulSoup(html, 'html.parser')\n",
      "    \n",
      "    # Your scraping logic goes here\n",
      "    company_names = soup.select(\".directory-list div.provider-info--header .company_info a\")\n",
      "    locations = soup.select(\".locality\")\n",
      "    \n",
      "    company_names_list = [name.get_text(strip=True) for name in company_names]\n",
      "    locations_list = [location.get_text(strip=True) for location in locations]\n",
      "    \n",
      "    data = {\"Company Name\": company_names_list, \"Location\": locations_list}\n",
      "    df = pd.DataFrame(data)\n",
      "    df.index += 1\n",
      "    print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n",
      "    df.to_csv(\"it_services_data.csv\", index=False)\n",
      "    \n",
      "    driver.quit()\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "i get back this.   \n",
      "\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "    \n",
      "    SessionNotCreatedException                Traceback (most recent call last)\n",
      "    <ipython-input-6-a29f326dd68b> in <cell line: 41>()\n",
      "         39 # Example usage\n",
      "         40 url = 'https://clutch.co/il/agencies/digital'\n",
      "    ---> 41 scrape_clutch_digital_agencies_with_selenium(url)\n",
      "    \n",
      "    6 frames\n",
      "    /usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py in check_response(self, response)\n",
      "        227                 alert_text = value[\"alert\"].get(\"text\")\n",
      "        228             raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here\n",
      "    --> 229         raise exception_class(message, screen, stacktrace)\n",
      "    \n",
      "    SessionNotCreatedException: Message: session not created: Chrome failed to start: exited normally.\n",
      "      (session not created: DevToolsActivePort file doesn't exist)\n",
      "      (The process started from chrome location /root/.cache/selenium/chrome/linux64/120.0.6099.109/chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\n",
      "    Stacktrace:\n",
      "    #0 0x565c2a694f83 <unknown>\n",
      "    #1 0x565c2a34dcf7 <unknown>\n",
      "    #2 0x565c2a38560e <unknown>\n",
      "    #3 0x565c2a38226e <unknown>\n",
      "    #4 0x565c2a3d280c <unknown>\n",
      "    #5 0x565c2a3c6e53 <unknown>\n",
      "    #6 0x565c2a38edd4 <unknown>\n",
      "    #7 0x565c2a3901de <unknown>\n",
      "    #8 0x565c2a659531 <unknown>\n",
      "    #9 0x565c2a65d455 <unknown>\n",
      "    #10 0x565c2a645f55 <unknown>\n",
      "    #11 0x565c2a65e0ef <unknown>\n",
      "    #12 0x565c2a62999f <unknown>\n",
      "    #13 0x565c2a682008 <unknown>\n",
      "    #14 0x565c2a6821d7 <unknown>\n",
      "    #15 0x565c2a694124 <unknown>\n",
      "    #16 0x7bf3032caac3 <unknown>\n",
      "\n",
      "any idea - what goes on here - i guess that i am facing selenium issues - while the headless-browser does not work propperly\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19ctamo/selenium_issues_on_a_scraper_that_runs_on/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: trying to get more insights into the use of selenium on google -colab - a handson approach\n",
      "Text: &#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "i am trying to get more insights into the use of selenium on google -colab. \n",
      "\n",
      "for that i have read lots of documents and tried outsome steps on  colab: now  i w ant to discuss this with you ... \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "this is the beginning of a tiny - manual of what can go wrong ever  - if you want to run little scraper that does all that - just read on. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "btw: this is a very rough manual - lots of  you can add thoughts ideas and more... so come on - add your ideas ...\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "To use Selenium in Google Colab, you first need to install the necessary packages and set up the WebDriver. Here's a simple example of a Selenium web scraper in Google Colab:\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "    # `Install necessary packages\n",
      "    !pip install selenium\n",
      "    !apt-get update\n",
      "    !apt install -y chromium-chromedriver\n",
      "    \n",
      "    # Set up WebDriver\n",
      "    from selenium import webdriver\n",
      "    from selenium.webdriver.common.keys import Keys\n",
      "    from selenium.webdriver.chrome.options import Options\n",
      "    \n",
      "    chrome_options = Options()\n",
      "    chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)\n",
      "    chrome_options.add_argument('--no-sandbox')\n",
      "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
      "    \n",
      "    # Specify the path to the chromedriver executable\n",
      "    chrome_driver_path = \"/usr/lib/chromium-browser/chromedriver\"\n",
      "    \n",
      "    # Create a WebDriver instance\n",
      "    driver = webdriver.Chrome(chrome_driver_path, options=chrome_options)\n",
      "    \n",
      "    # Example: Open Google and search for a term\n",
      "    search_term = \"Hello, Google Colab!\"\n",
      "    driver.get(\"https://www.google.com/\")\n",
      "    search_box = driver.find_element(\"name\", \"q\")\n",
      "    search_box.send_keys(search_term)\n",
      "    search_box.send_keys(Keys.RETURN)\n",
      "    \n",
      "    # Wait for a few seconds to let the results load\n",
      "    driver.implicitly_wait(5)\n",
      "    \n",
      "    # Print the title of the page\n",
      "    print(\"Page title:\", driver.title)\n",
      "    \n",
      "    # Close the browser window\n",
      "    driver.quit()\n",
      "    \n",
      "\n",
      "\\`\\`\\`\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\*\\*important\\*\\*:  Make sure to adjust the chrome\\_driver\\_path based on the version of the Chrome browser installed in the Colab environment.\n",
      "\n",
      "\\*\\*my guess\\*\\*: well  i guess that i have to take care for the adjustment of the \"chrome \\_driver\\_path\"  based on the version of the chrome-brwoser that is installed in the colab environment \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "see what i ve got back - how  to proceed?\n",
      "\n",
      "    \n",
      "    `Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "    Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "    Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "    Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "    Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "    Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "    Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "    Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "    Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "    Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
      "    Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "    Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "    Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "    Reading package lists... Done\n",
      "    Reading package lists... Done\n",
      "    Building dependency tree... Done\n",
      "    Reading state information... Done\n",
      "    chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
      "    0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
      "    ---------------------------------------------------------------------------\n",
      "    TypeError                                 Traceback (most recent call last)\n",
      "    <ipython-input-2-87c7a8d826d3> in <cell line: 19>()\n",
      "         17 \n",
      "         18 # Create a WebDriver instance\n",
      "    ---> 19 driver = webdriver.Chrome(chrome_driver_path, options=chrome_options)\n",
      "         20 \n",
      "         21 # Example: Open Google and search for a term\n",
      "    \n",
      "    TypeError: WebDriver.__init__() got multiple values for argument 'options'\n",
      "    `\n",
      "\n",
      "**- how  to proceed?**\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "It seems like there's a version mismatch or an issue with the [webdriver.Chrome](https://webdriver.Chrome) initialization in your code. The error suggests that there's a problem with the arguments being passed to the WebDriver constructor.\n",
      "\n",
      "Try modifying the [webdriver.Chrome](https://webdriver.Chrome) initialization line like this:\n",
      "\n",
      "    `\n",
      "    # Create a WebDriver instance`\n",
      "    \n",
      "    driver = webdriver.Chrome(executable_path=chrome_driver_path, options=chrome_options)\n",
      "    \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "This ensures that the chrome\\_driver\\_path is explicitly provided as the executable\\_path argument. Please update your code accordingly and see if it resolves the issue.\n",
      "\n",
      "If the issue persists, there might be a compatibility problem between the ChromeDriver version and the Chrome browser version. In such cases, you may need to download the appropriate version of ChromeDriver that matches the installed version of Chrome in your Colab environment.\n",
      "\n",
      "well -  we can download ChromeDriver from the official website: \n",
      "\n",
      "[https://sites.google.com/chromium.org/driver/](https://sites.google.com/chromium.org/driver/)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "After downloading, upload the ChromeDriver executable to the Colab environment and adjust the chrome\\_driver\\_path accordingly.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "well how to proceed - how to  test the usage of selenium on google-colab!\"?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19cz28l/trying_to_get_more_insights_into_the_use_of/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape articles from Apple News widget on iOS\n",
      "Text: Hello everyone, \n",
      "\n",
      "even though Apple News has officially launched in very few countries, Apple still offers an Apple News widget (see screenshot below) that displays curated articles (as far as I know they are updated once a day). Does anyone have any idea how I can find out the data source for the widget in order to scrape it over a longer period of time and analyze it (topics, media, etc.)? \n",
      "\n",
      "Thanks in advance for any kind of hints! \n",
      "\n",
      "https://preview.redd.it/vt3903q41zdc1.jpg?width=1284&format=pjpg&auto=webp&s=2802e5489a7aeb8db4143dae664df634b271a481\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19cswv0/scrape_articles_from_apple_news_widget_on_ios/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: fun stuff, fu** captcha\n",
      "Text: working on https://inconsistentcontacts.com\n",
      "\n",
      "this is getting NYSE company data… to be pushed to the project tonight.\n",
      "\n",
      "did most of Fortune 500 yesterday - need to complete industry tags and a bunch of other shi…\n",
      "\n",
      "Kinda sucks rn but #remindme\n",
      "URL: https://v.redd.it/4rolyiyg5udc1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web Scraping Using the Network Tab\n",
      "Text: Hey, I am trying to scrape data from a website, and I decided to use the network tab to search or the json response that is being rendered on the front end.\n",
      "\n",
      "I found the request that yielded the response but when I copied and pasted this URL on my browser I got a response similar to this\n",
      "\n",
      "     {\"code\":40101,\"msg\":\"no permission\",\"request_id\":\"2024012207483102E5C956EFC333680D96\"} \n",
      "\n",
      "I am looking for a remedy around this. I also found an access token response on the network tab that looks something like this\n",
      "\n",
      "    {\n",
      "        \"code\": 0,\n",
      "        \"data\": {\n",
      "            \"tokenType\": \"Bearer\",\n",
      "            \"token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwbGF0Zm9ybSI6eyJwbGF0SUQiOiI3MjMyMTEyNzEwNzg2MzUxMTA2IiwiZG9tYWluTGlzdCI6WyJhZHMudGlrdG9rLmNvbS9idXNpbmVzcy9jcmVhdGl2ZWNlbnRlciJdLCJ0cmlnZ2VyS2V5TGlzdCI6WyJjY19haWdjX3N1cnZleV9pbl9zY3JpcHRfcGFnZSIsImNjX2VkdWNhdGlvbl9iaVdlZWtseV90b3BBZHMiLCJjY19lZHVjYXRpb25fc2hvd2NhcmRzIiwiY2Nfc2hvd19zdXJ2ZXlfaW5fZGV0YWlsX3BhZ2UiLCJjY19zaG93X3N1cnZleV9pbl90b29sYm94Il19LCJpYXQiOjE3MDU5MDkyNzgsImV4cCI6MTcwNTkxNjQ3OH0.8mI5R8slfUP7Ie55HO2b26hip0daezfaUt3t09fnMzQ\",\n",
      "            \"expiresIn\": 7200\n",
      "        },\n",
      "        \"message\": \"success\"\n",
      "    }\n",
      "\n",
      "I tried passing this into the url but I still get the unauthorised message.\n",
      "\n",
      "Any help will be gratly appreciated\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19cqj71/web_scraping_using_the_network_tab/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: What a more professional scraping project looks like?\n",
      "Text: Hello reddit,\n",
      "\n",
      "Currently I have a project where i have between 50-70 spiders in scrapy that i need to run throughout the year. I think my current set up is not bad, but I would like to see more professional pipelines looks like or maybe you have some suggestion for me. Let's use this thread to discuss, it may help more people.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "My current approach is more or less as follows\n",
      "\n",
      "\\- First i collect all urls with needed filters from the websites, and store them neatly in a google sheets (i know, maybe db is better but google sheets allows me to quickly make any changes if need be, and this document is very alive)\n",
      "\n",
      "\\- I have individual spiders for each website\n",
      "\n",
      "\\- Go through all the items in the website, and then go through the pipelines to clean the data...etc and store the final data into a postgresql database\n",
      "\n",
      "\\- Then each page is saved in .html format in my local file system, just in case the data inserted in the database is wrong and i need to restart again, i just scrape the offline files instead of doing it online agai\n",
      "\n",
      "\\- Then i have several one-off scripts, but the main one is extract to excel, where i take the data i want, do some analysis of some missing attributes...etc and then produce a final excel file, nicely formated with the results, ready to be sent to a client.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "This is more or less a high level view, but do you have any other approach to this? do you maybe store the whole HTML in the database? or store the 'unclean' data in database and then have some ETL process?, in my current process i do all the cleaning before i insert into database, so that in database i always have the clean data\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Thanks! \n",
      "URL: https://www.reddit.com/r/webscraping/comments/19c325e/what_a_more_professional_scraping_project_looks/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: New to programming and web scraping in general, in python how can I iterate over a complete site that has URLs end in /xyz/\"some random text\"\n",
      "Text: the /xyz/ is always constant however not sure how I can iterate over a site where all the URLs end in different and random text\n",
      "\n",
      "This is the site in question:\n",
      "\n",
      "[https://wodwell.com/](https://wodwell.com/wod/hotshots-19/)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I want to get all the URLs for all the workouts for example\n",
      "\n",
      "[https://wodwell.com/wod/hotshots-19/](https://wodwell.com/wod/hotshots-19/)\n",
      "\n",
      "I want to scrape the following data\n",
      "\n",
      "    hotshots-19\n",
      "    CROSSFIT HERO WOD\n",
      "\n",
      "and the entire description text so all the info below\n",
      "\n",
      "    6 Rounds For Time\n",
      "    30 Air Squats\n",
      "    19 Power Cleans (135/95 lb)\n",
      "    7 Strict Pull-Ups\n",
      "    400 meter Run\n",
      "    With a running clock, as fast as possible perform 6 rounds of the work in the order written: 30 Air Squats, 19 Power Cleans, 7 Strict Pull-Ups, and then a 400 meter Run.\n",
      "    Score is the time it takes to complete the 6 rounds.\n",
      "    Movement Standards\n",
      "    Strict Pull-Ups: Begin with your hands on the Pull-Up Bar just outside your shoulders, arms fully extended. Without any help/momentum from the lower body (no kip), pull so your chin gets higher than the bar. Complete at full arm extension.\n",
      "    Good Score for “Hotshots 19” (estimated)\n",
      "    – Beginner: 35+ minutes – Intermediate: 30-34 minutes – Advanced: 25-29 minutes – Elite: <24 minutes Tips and Strategy Choose a steady, moderate pace (around 75-80% of your max pace). A fast pace out of the gate will likely lead to burn out and a poor overall score. When you complete round 1, look at the clock. Aim to complete each of the next 5 rounds at the same pace, +/- 30 seconds. The crux of this WOD is going from 19 Power Cleans directly into 7 Strict Pull-Ups: They are both pulling exercises, so you’ll experience major fatigue in your grip, biceps, shoulders, and lats. Break up the Power Cleans early (5+5+5+4 or 8+6+5 for example) to ensure that you don’t get trapped at the Pull-Up Bar for too long. Intended Stimulus “Hotshots 19” should feel long and exhausting. The Power Clean load should feel relatively light. If this workout were 4 rounds, it would probably be “enjoyable;” the 6 rounds make this WOD brutally hard. Scaling Options This Hero WOD is long and high volume. Reduce the volume, the load, and or skill level (see: Pull-Up Scaling) to keep this workout under 40 minutes. Intermediate Option 6 Rounds for Time of: 30 Air Squats 19 Power Cleans (115/75 lb) 3 Strict Pull-Ups 400 meter Run Beginner Option 4 Rounds for Time of: 15 Air Squats 10 Power Cleans (75/55 lb)  5 Ring Rows 400 meter Run\n",
      "\n",
      "Another example would be this url\n",
      "\n",
      "[https://wodwell.com/wod/robert-nagel/?ref=listmodal](https://wodwell.com/wod/robert-nagel/?ref=listmodal) \n",
      "\n",
      "workout name is robert-nagel\n",
      "\n",
      "555 FITNESS HERO WOD\n",
      "\n",
      "and has the following info that I would like to persist \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "    3 Rounds for Time\n",
      "    100 foot Overhead Walking Lunges (45/25 lb)\n",
      "    20 Wall Ball Shots (20/14 lb)\n",
      "    20 Box Jumps (24/20 in)\n",
      "    10 Push Presses (135/95 lb)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I am doing this for an online course as my final project, want to scrape all workouts and persist them in a DB. Site doesnt offer APIs,\n",
      "\n",
      "each workout has a name, so the url is always /wod/workout name/\n",
      "\n",
      "just not sure how I can get all the names so I can use requests and beautiful soup to scrape them after using a base url of [https://wodwell.com/wod](https://wodwell.com/wod) and append the workout name \n",
      "\n",
      "I am very new to this so would appreciate some input and thanks for your help\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19bx1rh/new_to_programming_and_web_scraping_in_general_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Advice on best method to scrape codebase documentation (ultimately into a format suitable for RAG for LLMs)\n",
      "Text: As title says, I'm looking into what methods I should research or test, to pull some codebase documentation. For example, the documentation on, say, how to use typescript for JS (just an example, as I know there are already LLMs that may know that).\n",
      "\n",
      "I guess this would involve getting a scraping tool to crawl everything on a subdomain or subfolder, for example codinglanguageexample.com/docs/ or docs.codingexample.net  as fake EGs.\n",
      "\n",
      "I would like to do this free or lowest possible cost. But the more I research scraping, the less possible anything free seems to be.\n",
      "\n",
      "Anyway, formatting:\n",
      "\n",
      "Been learning steadily - but most examples are \"how to scrape Amazon! How to scrape hotels!\" and that's all data to CSV.  \n",
      "Whereas coding docs will be paragraphs interspersed with code examples, not lists of tabular data.\n",
      "\n",
      "Best method to clean and format may be dependent on the parent method of obtaining the data in the first place? EG if I could somehow pull it all via some basic python and puppeteer we'd clean with BS4, right? But if we obtained it with another method it may be cleaner/messier? (apologies for vagueness).\n",
      "\n",
      "Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19brb49/advice_on_best_method_to_scrape_codebase/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Is there a current solution to scrap tweets\n",
      "Text: So I decided for my master project that I would like to use twitter data on python to get Sentiments on a certain subject and I am now starting to realise that it is not as easy as it used to be due to Musk. I found this YouTube video here https://youtu.be/MNEw3Mplm7E?si=Q7eGcZawsNvSu9zB. Has anyone tried this?\n",
      "\n",
      "What I would realistically like to do is scrape twitter data by certain keywords during a certain time period. Is this realistically achievable?\n",
      "\n",
      "Lastly, if not is there any other python libraries that allow to scrape other social media data that can be used for Sentiment analysis.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19bekjv/is_there_a_current_solution_to_scrap_tweets/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Production level scraping\n",
      "Text: For production-level scraping, does it happen in real-time or is the data stored in warehouses or maybe in vector DBs?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19b7k9k/production_level_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Cloud Flare bypass\n",
      "Text: Hello!I am doing a project that's main focus is to do some web scraping directly trough API calls (so no browser involved / no selenium or puppeteer).\n",
      "\n",
      "My main problem is that when I make the API call to login to the website I want to scrape, I am only able to get the desired response (Bearer Auth token) when I send the request with my own IP (no proxy) and with the cookie header copied from the request sent trough a chrome browser.\n",
      "\n",
      "Otherwise, if I try to send that request without the cookies and trough the a proxy, I get Cloud Flare protection screen as a HTML response.Any idea / solution to bypass this would be a lot of help, thanks!\n",
      "\n",
      "this is the HTML I get as response from cf\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "https://preview.redd.it/t19exrqugldc1.png?width=1812&format=png&auto=webp&s=e5e44163e18301fddfcd486079dc795a12f358b5\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19bbd4t/cloud_flare_bypass/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Business/work e-mail address?\n",
      "Text: I've been trying to sign up for various e-mail scraping services but they all seem to ask for a \"business/work e-mail address\", i.e. Gmail and so on don't work. Is there a way around this? If I create a generic website with a domain name, will using the e-mail from that domain work?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19b9nrm/businesswork_email_address/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping the optionschain from Euronext\n",
      "Text: Greetings all,\n",
      "\n",
      "I'm trying to scrape the optionschain from Euronext. For example, for the company Heineken, the weburl is [https://live.euronext.com/en/product/index-options/HEI-DAMS](https://live.euronext.com/en/product/index-options/HEI-DAMS)\n",
      "\n",
      "What I used is the following:\n",
      "\n",
      "    import requests\n",
      "    url = 'https://live.euronext.com/en/ajax/getPricesOptionsAjax/stock-options/HEI/DAMS/' \n",
      "    req = requests.get(url).json()\n",
      "\n",
      "However, this only gets the options for the upcoming expiry date. How do I generalize this such that I get the options for all expiry dates?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19ail6n/webscraping_the_optionschain_from_euronext/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Crawling and scrape a huge amount of domain\n",
      "Text: Hi there!\n",
      "\n",
      "I would like to scrape the web and collect domains to a list with domains that has a specific ending like .io og .net\n",
      "\n",
      "I’m new to scraping but I have created a spider with scrapy. At first I just need to create the list with a meta title and description like google. However I could imagine that my ip would be black listed if I just go through the entire web with a spider. I am building it on a linode server. \n",
      "\n",
      "Do you have any recommendations like\n",
      "Timeout time before moving to the next domain or things I should notice, and maybe legal advice before I start to crawl. I will of cause obey the robot.txt\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19a9hjj/crawling_and_scrape_a_huge_amount_of_domain/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping oldest/first google maps review date for a given list of place IDs?\n",
      "Text: Hello :) having issues trying to figure out how I can scrape the date of the oldest/first review for a list of 500+ place IDs I have stored in a CSV. The Places API can only pull the 5 most recent or most relevant reviews, and I really just can't manage to get selenium to work for me in R. any help greatly appreciated!!! thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19a1qtf/scraping_oldestfirst_google_maps_review_date_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: [Question] Can't get any useful info off of this website, any idea why?\n",
      "Text: My code is below. I originally had an issue where the website had cloudfare protection so I used Zenrows to exract but when I run the code it's not giving the names of players on the page. Reason behind this is I want to simulate the Gacha mechanic of the NBA2k game locally for myself to still enjoy all of the player variations without spending the time or money that is common with microtransactions. 2kdb has all of the cards for the last four years so I am trying to find a way to easily scrape for each card the player's name, over, def, height, weight, and photo ideally but the photo is optional. Any suggestions of tutorials or other resources would be great as I have limited experience in Python and none with web scraping (yet).\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "import requests\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "url = \"[https://2kdb.net/players/24?freeAgents=false&page=1&pageSize=50](https://2kdb.net/players/24?freeAgents=false&page=1&pageSize=50)\"\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\# Send a GET request to the URL\n",
      "\n",
      "response = response\\_Zenrow\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\# Parse the HTML content of the response using BeautifulSoup\n",
      "\n",
      "soup = BeautifulSoup(response.content, 'html.parser')\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\# Find all span tags with the specified class\n",
      "\n",
      "name\\_tags = soup.find\\_all('span', {'class': 'font-bold'})\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\# Extract and print the names\n",
      "\n",
      "for name\\_tag in name\\_tags:\n",
      "\n",
      "player\\_name = name\\_tag.text.strip()\n",
      "\n",
      "print(player\\_name)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Here is what actually outputs\n",
      "\n",
      "General\n",
      "\n",
      "Outside scoring\n",
      "\n",
      "Inside scoring\n",
      "\n",
      "Playmaking\n",
      "\n",
      "Athleticism\n",
      "\n",
      "Defense\n",
      "\n",
      "Rebounding\n",
      "\n",
      "Tendencies: Defense\n",
      "\n",
      "Tendencies: Inside\n",
      "\n",
      "Tendencies: Shooting\n",
      "\n",
      "Tendencies: Freelance\n",
      "\n",
      "Tendencies: Passing\n",
      "\n",
      "Finishing\n",
      "\n",
      "Shooting\n",
      "\n",
      "Playmaking\n",
      "\n",
      "Defensive/Rebounding\n",
      "\n",
      "Extended Filters (Collections, Plays, Stats Sliders)\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "URL: https://www.reddit.com/r/webscraping/comments/199r635/question_cant_get_any_useful_info_off_of_this/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping data through a map\n",
      "Text: Hey, Guys I was wondering if anyone could help me regarding this. I want to scrape the data about these charging locations. How do I approach this?\n",
      "\n",
      "https://preview.redd.it/3020tflf86dc1.png?width=1303&format=png&auto=webp&s=4ceb28b4e90f43792e7962652750437519499381\n",
      "URL: https://www.reddit.com/r/webscraping/comments/199mnfm/scraping_data_through_a_map/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: [question] scraping notion\n",
      "Text: I am trying to scrape the webside notion and for this I am using selenium but apparently notion detects strange entries and now to enter the notion it asks me for temporary codes.\n",
      "\n",
      "Has anyone worked with notion to extract information?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/199sl6c/question_scraping_notion/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: how to scrape a pdf with multiple columns (like paper newspaper or magazine)?\n",
      "Text: I have PDFs that I have to scrap that has 2 columns of text.  I'm able to scrape it via python but it will not distinguish the 2 column but rather copy the full line of text from both columns as a single line of data.  \n",
      "\n",
      "This is should be a common use case just based on the sheer amount of articles, hard copy news papers, magazines that display data in multiple columns.  I'm sure someone has parsed it and able to maintain proper column / data structure.  \n",
      "\n",
      "Any ideas?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/199r9ia/how_to_scrape_a_pdf_with_multiple_columns_like/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Amazon Reviews API\n",
      "Text: Hi everyone\n",
      "\n",
      "I am in the process of developing an amazon parser on Scrapy.\n",
      "\n",
      "When creating an algorithm to parse API reviews, I tried to find information on the 'filterByAge' query attribute, but found nothing.\n",
      "\n",
      "It's definitely filtering reviews by age (either from the time of publication or some other age...)\n",
      "\n",
      "Does anyone know what this attribute really means?\n",
      "\n",
      "What data does it accept, and in what form?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/199ldf1/amazon_reviews_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrapping Amazon - Forced timeout?\n",
      "Text: Hello guys,  \n",
      "I am developing a amazon scrapping script using playwright. I am able to scrapy some information, but every page I go \"page.goto\", the response get slower and slower until I get timeout on playwright.  \n",
      "\n",
      "\n",
      "Does anyone knows if this behavior is some kind of Amazon bot protection?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/199fb24/scrapping_amazon_forced_timeout/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Things are getting pretty hopeless for any attempt to use Python requests on Cloudflare protected websites\n",
      "Text: \n",
      "URL: https://i.redd.it/g2n48wx6lxcc1.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: RSS Feed and Scrapes The Content\n",
      "Text:  Before I dive into buidling an article scaper through [Make.com](https://Make.com) RapidAPI and  I want to see if there is something like inoreader that follows news rss but will also scrape me the whole article so that I can have access to it without having to scrape the article myself? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1995wr5/rss_feed_and_scrapes_the_content/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: [Typescript] Question mark characters for content of site using Axios\n",
      "Text: Hello there. I'm trying to scrape a certain page which has latin characters as content for some text. Usually I'd not care much about them but in this case I have to collect them.\n",
      "\n",
      "The trouble begins when I'm getting the page data, in which all latin characters are turned into those question mark diamonds and I keep having them into the rest of my logic.\n",
      "\n",
      "The code is quite simple, I use a GET request and then get it's content, then further I'm using it with JSDOM to search through it. Until then, the content is having the broken characters. Here's the simple code as an example:\n",
      "\n",
      "    return new Promise(async (resolve, reject) => {\n",
      "            try {\n",
      "                const url = 'https://cdep.ro/pls/parlam/structura2015.mp?idm=284&cam=2&leg=2020';\n",
      "                axios.get(url, {headers: {'Content-Encoding': 'utf-8', 'accept-encoding': null}}).then(response => resolve(response.data));\n",
      "            } catch (error) {\n",
      "                throw new Error(`Failed request with url: ${url} having error ${error}`);\n",
      "            }\n",
      "        });\n",
      "\n",
      "I tried the same thing with Puppeteer, and fortunately I don't have the same issue, but I get double the time to complete my scraping process. Any help is much appreciated.  \n",
      "\n",
      "\n",
      "Update: Managed to solve this by getting the response as arraybuffer, then decoding it locally using [iconv-lite](https://www.npmjs.com/package/iconv-lite) from ISO-8859-2, the encoding the content was initially from.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19954aq/typescript_question_mark_characters_for_content/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Python Selenium scraping help\n",
      "Text: Hello guys I have a question. I want to scrape data from this website: [https://www.aldcarmarket.com/en-it/sales/14014/](https://www.aldcarmarket.com/en-it/sales/14014/). To be precise I want to scrape data from car cards. I managed to extract headers timers and specifications from each card but I have problem. Every card has a details button. When that button is clicked additional information appears. I want to scrape that information as well but I don't know how. I don't know how to target that element any ideas? I am very lost.\n",
      "\n",
      "    Edit:Added code\n",
      "    from selenium import webdriver\n",
      "    from selenium.webdriver.common.keys import Keys\n",
      "    from selenium.webdriver.common.by import By\n",
      "    from selenium.webdriver.support.ui import Select\n",
      "    from selenium.webdriver.support.ui import WebDriverWait\n",
      "    from selenium.webdriver.support import expected_conditions as EC\n",
      "    import time\n",
      "    import pandas as pd \n",
      "    from bs4 import BeautifulSoup as bs\n",
      "\n",
      "    #Setup\n",
      "    driver = webdriver.Chrome()\n",
      "    userName = \"username\"\n",
      "    password = \"pasword\"\n",
      "    site_url = \"https://www.aldcarmarket.com/en-it/\"\n",
      "    driver.get(site_url)\n",
      "    driver.maximize_window()\n",
      "\n",
      "    #Agree button plus waiting for login\n",
      "    buttonWait = WebDriverWait(driver, \n",
      "    30).until(EC.element_to_be_clickable((By.ID, \"didomi-notice-agree-button\")))\n",
      "    buttonAccept = driver.find_element(By.ID,\"didomi-notice-agree-button\").click()\n",
      "    loginWait = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, \n",
      "    \"dropDownLogin\")))\n",
      "    buttonLogin = driver.find_element(By.ID, \"dropDownLogin\").click()\n",
      "\n",
      "\n",
      "    #Login\n",
      "    driver.find_element(By.ID,\"userName\").send_keys(userName)\n",
      "    driver.find_element(By.ID,\"password\").send_keys(password)\n",
      "    driver.find_element(By.ID,\"login-submit-button\").click()\n",
      "\n",
      "\n",
      "    #Rating bypass\n",
      "    crossWait = WebDriverWait(driver, \n",
      "    30).until(EC.element_to_be_clickable((By.CLASS_NAME, \"cross\")))\n",
      "    driver.find_element(By.CLASS_NAME, \"cross\").click()\n",
      "\n",
      "\n",
      "    #Navigate to sales page\n",
      "    salesWait = WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.ID, \n",
      "    \"btnShowSale_13956\")))\n",
      "    driver.find_element(By.ID, \"btnShowSale_13956\").click()\n",
      "\n",
      "\n",
      "\n",
      "    #Scroll down 140355\n",
      "    WebDriverWait(driver,30)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    x = 0\n",
      "    while True:\n",
      "       x += 1\n",
      "       driver.execute_script(\"scrollBy(0,1000)\")\n",
      "       time.sleep(0.5)\n",
      "       if x>7:\n",
      "           break\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    cardsData = []\n",
      "    #cards = driver.find_elements(By.CLASS_NAME, \"card-body\")\n",
      "    titles = driver.find_elements(By.CLASS_NAME, \"vehicle-title\") \n",
      "    timer = driver.find_elements(By.CLASS_NAME, \"sale-timer\")\n",
      "    specifications = driver.find_elements(By.CLASS_NAME, \"vehicle-specifications\")      \n",
      "  #\"vehicle-specifications\"\n",
      "details = driver.find_elements(By.CLASS_NAME, \"details-title\")\n",
      "\n",
      "\n",
      "specificationsResult=specifications[::2]\n",
      "\n",
      "\n",
      "for i in range(25):\n",
      "    cardData = {\n",
      "        \"Title\": titles[i].text,\n",
      "        \"Time\": timer[i].text,\n",
      "        \"Specifications\": specificationsResult[i].text,\n",
      "    }\n",
      "    cardsData.append(cardData)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   #print(cardsData)\n",
      "\n",
      "\n",
      "   df = pd.DataFrame(cardsData)\n",
      "   html_table = df.to_html(index=False)\n",
      "   print(df)\n",
      "\n",
      "\n",
      "\n",
      "   with open('table.html', 'w') as file:\n",
      "         file.write(html_table)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    #while True:\n",
      "     #x += 1\n",
      "    #driver.execute_script(\"scrollBy(0,1000)\")\n",
      "    #time.sleep(0.5)\n",
      "    #if x>70:\n",
      "        #break\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1994i29/python_selenium_scraping_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How do I export a list of websites ending in a certain TLD?\n",
      "Text: Hi, I'm hoping to export a list of websites, preferably by country to an excel sheet.\n",
      "\n",
      "I've tested builtwith and semrush though they don't seem to be able to find it or be able to use wild card selectors to only list all websites ending in business.site etc.\n",
      "\n",
      "Any ideas would be appreciated. \n",
      "\n",
      "Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/198rt13/how_do_i_export_a_list_of_websites_ending_in_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Octoparse scraping - duplicate help\n",
      "Text: Hey guys, I'm working on a project to pull job postings using Octoparse. For sites like Y-combinator, there's only so many entries per day, but I'm seeing duplicate entries every time a new round runs and the data gets uploaded to a google sheet. Each run tells me, say, 50 runs with 45 duplicates, but it still appears to be uploading the duplicates. What setting do I have to tweak to make sure the duplicates aren't getting uploaded? This UI is less intuitive than I would like. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/198ogv3/octoparse_scraping_duplicate_help/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help scraping a PDF embedded through CloudPDF\n",
      "Text: You can visit their website to view an example of the exact look an embedded PDF has (i.e. the one I'm trying to rip).\n",
      "\n",
      "[https://cloudpdf.io/](https://cloudpdf.io/)\n",
      "\n",
      "The thing is I have near to zero knowledge about web developing so I'm really struggling to get any results. I hope it isn't really hard for you guys. Any help is welcome.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/198nmlj/need_help_scraping_a_pdf_embedded_through_cloudpdf/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Do you suggest any Proxy List Premium to bypass cloudflare?\n",
      "Text: I need about 60-80K requests per day altough, and the cloudflare blocked the mullvad VPN network the last days\n",
      "URL: https://www.reddit.com/r/webscraping/comments/198hw8r/do_you_suggest_any_proxy_list_premium_to_bypass/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Instagram Reels Scrapping\n",
      "Text: Hii. I am currently implementing a python program for Scrapping instagram reels data.\n",
      "\n",
      "I make the respective API calls for that and I am able to get the data perfectly. \n",
      "\n",
      "Now the issue is Instagram blocks me after sometime or I reach rate limit.\n",
      "\n",
      "Is there any workaround for it? I heard proxy is one way of doing it, then how can I implement it? Is there any reliable way for that? \n",
      "\n",
      "If there is any other way it would be very helpful\n",
      "\n",
      "\n",
      "Thanks ✨\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1983i44/instagram_reels_scrapping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any way to bypass cloudflare?\n",
      "Text: Trying to scrap images from platesmania.com ,\n",
      "Tried with undetected_chromedriver didn't work.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/197w38j/any_way_to_bypass_cloudflare/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping data from video game stat page.\n",
      "Text: I really like looking into Warzone Weapon Pick rates from websites like\n",
      "\n",
      "[https://wzstats.gg/](https://wzstats.gg/)\n",
      "\n",
      "and\n",
      "\n",
      "[https://www.wzranked.com/mw3/meta](https://www.wzranked.com/mw3/meta)\n",
      "\n",
      "How can I get the weapons pick rate during a certain period and study that information, track the pick rate of weapons by making my own graphs, could also try to compare the data from both sites also and see how they different or similar it is.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/197ctjk/scraping_data_from_video_game_stat_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selenium '.text' different than all other methods to get source on dynamic page\n",
      "Text: Chrome 120; Selinum, Chromedriver 120; Python\n",
      "\n",
      "Just want to automate my flight price searches on kayak. It was working for abut 3 months.  I changed up the search for a new flight and now having issues.   Changed to headful for now (was headless).  I manually wait 25 seconds for dynamic content to load (webdrivewait never seems to work right for me, but that tries to wait an additional 30).  I just want the source so I can parse with beautifulsoup, but I can't seem the CURRENT (dynamically loaded) page source.  The following returns price info:\n",
      "\n",
      "* driver.find\\_element(By.XPATH '//body').text\n",
      "\n",
      "But I need structured data, not just the text. The following DO NOT work (all return 0 prices; sometimes I see some reformatted, obscure price info nested in script (non-html) code--page\\_source does this I believe)\n",
      "\n",
      "* driver.page\\_source (was using this method before)\n",
      "* driver.page\\_source after waiting for a deeply nested element, i.e. one of the result containers ('//\\[div\\[@data-resultid\\]')\n",
      "* driver.find\\_element(By.XPATH, '//body').get\\_attribute('outerHTML')\n",
      "* driver.find\\_element(By.XPATH, '//body').get\\_attribute('innerHTML')\n",
      "* driver.execute\\_script(\"return document.body.innerHTML\")\n",
      "* there are no relevant iframes that I can see. I've switched to both iframe 1 and 2 and I see no relevant content, though I could be something wrong here.\n",
      "\n",
      "The selenium API is pretty bad.  Is there really no equivalent to \\`.text\\` that will just give me all the HTML? That's all I want is just the source of the current page and beautifulsoup can take care of the rest.  I also really feel like the webdriver/webelement object contains this info.  At this very moment, I'm staring at 52 prices in the \\`.text\\` results, yet even when I search for the html attribute from the driver I don't see them. And as I mentioned, this is running headful, so when I inspect the source in the browser of course I do see what I'm looking for (though attributes and names change in some ways, anti-scraping measures?)  I can provide code if needed, though my bet it's my ignorance about something that is obvious to those in the know.\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1971elp/selenium_text_different_than_all_other_methods_to/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best tool to scrape the Facebook Marketplace\n",
      "Text: Hi all,  \n",
      "i'm looking to build a tool to pull the latest listings off Facebook marketplace.  I found this video\n",
      "\n",
      "[https://www.youtube.com/watch?v=uHqlfSy6e3U&t=457s&pp=ygUQZ3JhcGhxbCBmYWNlYm9vaw%3D%3D](https://www.youtube.com/watch?v=uHqlfSy6e3U&t=457s&pp=ygUQZ3JhcGhxbCBmYWNlYm9vaw%3D%3D)\n",
      "\n",
      "However,  after checking myself , I wasn't able to find the responses mentioned  in the video. Is it still possible to pull listings using GraphQL or  shall i use something like Puppeteer\n",
      "URL: https://www.reddit.com/r/webscraping/comments/196uwdg/best_tool_to_scrape_the_facebook_marketplace/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Are There Exchanges/Markets To Purchase Scraped Data?\n",
      "Text: So I have a client who is looking for numerous databases, some of them were easy to just scrape the usual suspects, but he has a few others that are more difficult to find through typical B2B sources, especially since he's getting so granular. \n",
      "\n",
      "I'd seen in the past, through the dashboard of one of the more well-known scraping platforms a mini marketplace that had some scraped databases you could purchase, but for the life of me, I cannot recall which, and so far coming up empty handed. \n",
      "\n",
      "So this got me thinking, if I've seen that market, are there other markets I might be able to browse and see if someone else has already scraped the data he needs? \n",
      "\n",
      "Thank you in advance. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/196iw6v/are_there_exchangesmarkets_to_purchase_scraped/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Discord Channels\n",
      "Text: Im looking to get messages from a couple discord channels. Maybe 10-12 times daily on each channel.  Since I can’t add a bot to the server and have to use my personal account i wanted to ask if anyone had issues with discord doing this?\n",
      "\n",
      "Since it’s my personal, dont want to get banned or anything like that. Since im only going to do this max 24 times a day, would I even be bothered by discord? \n",
      "\n",
      "Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/196omh8/discord_channels/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: how to make money out of web scraping\n",
      "Text: Hello I'm pretty good at web scraping and i want to make money out of that so please suggest to me some ways to make  good money out of web scraping besides working as freelancer\n",
      "URL: https://www.reddit.com/r/webscraping/comments/195uc8k/how_to_make_money_out_of_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping gets blocked through fetch but not in browser\n",
      "Text: Hi I am trying to scrape a grocerystore for their product data. When I visit the site through the browser it works and I can see the data, however when I try to use fetch or puppeteer it gets blocked. I copied the same headers and everything from the api request in the browser, but it still gets blocked.   \n",
      "\n",
      "\n",
      "Any clue how they detect that it is sent through a fetch request or puppeteer?  How could I get around this?   \n",
      "This is the endpoint that I am trying to scrape: [https://www.ah.nl/zoeken/api/products/product?webshopId=564313](https://www.ah.nl/zoeken/api/products/product?webshopId=564313) \n",
      "URL: https://www.reddit.com/r/webscraping/comments/196bbrf/scraping_gets_blocked_through_fetch_but_not_in/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can i scrape a website without being Blocked (ip address )\n",
      "Text: I want a free method 😅, if you have some resources just tell me\n",
      "URL: https://www.reddit.com/r/webscraping/comments/196i4qj/how_can_i_scrape_a_website_without_being_blocked/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: High quality Yelp web crawler made in go, open source\n",
      "Text: Happy new year to everyone. I'm full stack developer especialiazed on building web crawlers, bots, rpa tools.I've been on the industry for about 6 years and now I would like to contribute a high quality project to the community.\n",
      "\n",
      "Yelp web crawler made in go:[https://github.com/johnbalvin/goelp](https://github.com/johnbalvin/goelp)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\- Open source- Includes full seach support\n",
      "\n",
      "\\- The bot will get  information including reviews, rating,  images, description,  title ..etc\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Previous bots:\n",
      "\n",
      "\\- Amazon web crawler [https://github.com/johnbalvin/gozon](https://github.com/johnbalvin/gozon)  , I'lll add search support on future realeases\n",
      "\n",
      "\\- AirBnB web crawler  [https://github.com/johnbalvin/gobnb](https://github.com/johnbalvin/gobnb) , I'lll add search support on future realeases\n",
      "\n",
      "Warning:- Try using proxies, if you don't,  your IP will be banned, my IP got banned and still cant access the page, so use proxies\n",
      "\n",
      "It uses Golang as the main language and only HTTP requests like an animal, I hate using selenium, puppeteer, playwright .. etclet me know what you thinkthanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/196086b/high_quality_yelp_web_crawler_made_in_go_open/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Can I upload IMDB data on Kaggle\n",
      "Text: Greetings everyone,\n",
      "\n",
      "I am new to web scraping and don't know much about data laws. I have just completed my first project, extracting the IMDb top 100 anime list. Can I upload that data on Kaggle and make it public? Also, can I publish that project on my GitHub?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Thank you for your response In Advance.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/196bg2e/can_i_upload_imdb_data_on_kaggle/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to get out of shadowban.\n",
      "Text: Hi everyone!\n",
      "\n",
      " I'm trying to scrape a site. It's completely dynamic - the page source contains only a bunch of scripts and the text \"Turn on scripts, dummy\", so I'm using Selenium. Everything seemed to be okay until I got shadowbanned. There are no error messages, 404s or 403s, nothing like that. The page itself loads just fine, the site just doesn't provide any data for my requests.\n",
      "\n",
      "It's not a surprise since the first versions of my script weren't utilizing any masquerading techniques. But now I've implemented everything I could think of - a random user agent, request headers matching that user agent, rotating proxies, and a bunch of fingerprint spoofing browser extensions. Nothing works. Just nothing. The site somehow knows that it's me and redirects me to the data input page. \n",
      "\n",
      "The same URL that fails on the server where I scrape the site works just fine on my personal machine. I have no idea what to do except drop my server and pick a new one.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/195p6i2/how_to_get_out_of_shadowban/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Amazon Time Interval\n",
      "Text: I’m scraping 100 listings on Amazon with a 30 second delay between pages . My bot can handle the captcha but I was wondering if I could lower the time interval between pages?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1961blo/amazon_time_interval/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: LinkedIn Job Description Scraping\n",
      "Text: Hi,\n",
      "\n",
      "I want to use Python to automate feeding LinkedIn job description URLs and tell me whether a job is no longer accepting applications? This status only shows up when I log into LinkedIn so generic scraping doesn't work. \n",
      "\n",
      "Any idea how to do it? Thank you!\n",
      "\n",
      "https://preview.redd.it/fmz6akyf85cc1.png?width=797&format=png&auto=webp&s=60d01881b3d2f4d21e04c07e856143fc4723eb89\n",
      "URL: https://www.reddit.com/r/webscraping/comments/195gmd0/linkedin_job_description_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: DOTABUFF Web Scrape\n",
      "Text: I have been trying to transition to a career in Data Science for a while now. For this purpose, i have started creating my portfolio. As my first project, I decided to create a web scraping script that can scrape match data from DOTABUFF and I was able to successfully do so. \n",
      "\n",
      "Now my concern is, I went through the robots.txt at DOTABUFF, and they have mentioned this: 'Disallow: /esports/matches', in the document. So does this mean i am not allowed to scrape the above mentioned data, even though it is publicly available? I was planning to create a GitHub repository for this project as part of my portfolio. What do I do now?  \n",
      "\n",
      "\n",
      "Any insights would be appreciated. Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/195j25k/dotabuff_web_scrape/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How could have the pointerpointer.com images been sourced?\n",
      "Text: Looking at the types of images on [pointerpointer.com](https://pointerpointer.com) ([https://pointerpointer.com/images/347.jpg](https://pointerpointer.com/images/347.jpg), [https://pointerpointer.com/images/698.jpg](https://pointerpointer.com/images/698.jpg)) they all seem to be mundane early 2000s disposable camera type photos containing a person pointing at something. Any idea where / how to scrape these types of photos from a myspace/facebook archive or something?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/195hjep/how_could_have_the_pointerpointercom_images_been/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping 404 page.\n",
      "Text: As I encounter a problem, which I am not sure that this page exist or not.\n",
      "\n",
      "**Page:**\n",
      "\n",
      "[https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/](https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/)\n",
      "\n",
      "**General Tricks:**\n",
      "\n",
      "\"Some servers are smart enough and tries to prevent the page from being scraped.\"\n",
      "\n",
      "1. User headers: User Agent, Cookies.\n",
      "2. URL redirections.\n",
      "\n",
      "Can we scrape this page?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/195g7s1/scraping_404_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Visit LinkedIn profile and use with OpenAI?\n",
      "Text: I want to code a python script for my own use, that will go through my lead list, go to each LinkedIn profile, get the data there and use OpenAI to generate a compliment for the first line of a cold email.\n",
      "\n",
      "Any guidance on how to do this?\n",
      "\n",
      "Maybe I need to use a web scraping API to make it easier (rather than using something like Selenium and proxies)…\n",
      "\n",
      "Or is there an easier way getting the AI to visit the profiles itself?\n",
      "\n",
      "Thank you for your help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19558zr/visit_linkedin_profile_and_use_with_openai/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape data that is inside a div that changes its content?\n",
      "Text: Hi,\n",
      "\n",
      "site: [https://komora-ucetnich.cz/cze/seznamy/clenove-komory/sort:surname/direction:asc/page:1](https://komora-ucetnich.cz/cze/seznamy/clenove-komory/sort:surname/direction:asc/page:1)\n",
      "\n",
      "I'm trying to scrape this website for names and addresses to create an .xlsx or .csv file to use as address book as it's necessary for my job. More specifically, I'm trying to create a script that makes an excel file with those 2 parameters/colunms. \n",
      "\n",
      "I could access names by themselves, but am struggling with addresses.\n",
      "\n",
      "I'm also new to this and I cannot quite figure out how to scrape addresses. Addresses are visible when you click on a name under \"Prijmeni\" column in the \"adresa\" row.\n",
      "\n",
      "I would appreciate any tips on how to approach this.\n",
      "\n",
      "Kind regards\n",
      "URL: https://www.reddit.com/r/webscraping/comments/194vfck/how_to_scrape_data_that_is_inside_a_div_that/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Lowes Item Number-Based Hyperlinks\n",
      "Text: I'm not sure which channel this best fits on, but I'm trying to generate hyperlinks to Lowes products based on a unique identifier of the product, such as item number. Their website instead gives the client a long hyperlink that has a lot of noise in it, but it probably supports unique-identifier based links if given the right format.\n",
      "\n",
      "I've called Lowes, but non-employees aren't allowed to contact their IT department and they have yet to respond after two months.\n",
      "\n",
      "I've tried the following format, taken from [https://images.lowes.com/animate/lowesvendorguide.pdf](https://images.lowes.com/animate/lowesvendorguide.pdf), but it redirects to their homepage.  \n",
      "http://www.lowes.com/lkn?action=productDetail&productId=\\[item\\_number\\]\n",
      "\n",
      "You all can probably figure this out, easy. Please advise, thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1950swc/lowes_item_numberbased_hyperlinks/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Request URL not functional at all\n",
      "Text: Hi I am scraping a website that I've scraped for a couple of years now. I recently was scraping and I received no data when exporting to a CSV. I looked to see if the HTML had changed at all and noticed that the request URL was different. I updated that and am receiving a bad request status=400 error. Doesn't matter if I use post vs get. The HTML indicates post only works. My first thought is the site has upped its security for scraping. What could the issue be?\n",
      "\n",
      "This is what pops up when I look up the request URL.\n",
      "\n",
      "https://preview.redd.it/dnofqc4yo1cc1.png?width=661&format=png&auto=webp&s=daaa67c65e7f7294768484d1d6de8d30b2f96030\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1950qxt/request_url_not_functional_at_all/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help - Scraping X/Twitter\n",
      "Text: Fellow redditors,\n",
      "\n",
      "I humbly ask for your support in the following issue.\n",
      "\n",
      "I am currently working on a linguistics project examining political speech on Twitter/X. One major issue I have encountered is creating a corpus of tweets/posts. \n",
      "\n",
      "This is why I am looking for a scraper that would allow me to extract content (text with/without media such as pictures, with/without retweets) of a single user for a definable period of time (e.g. March 2022-August 2023). Obviously if there is a tool out there that would allow for further breakdown of the data (extract hashtags, time of posting, device, etc.) that would be even better, but the bare minimum of text and date would be sufficient.  \n",
      "I have next to no experience coding or programming, in other words the tool should be easy to use or some sort of \"manual\" should be avaiable to get myself acquainted with it. \n",
      "\n",
      "Feel free to suggest other workarounds if you happen to know of any other (other than paying X/Twitter) \n",
      "\n",
      "Thank you for you help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/194sodo/help_scraping_xtwitter/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: scrapy gets stuck 0 pages/ min\n",
      "Text: Hey  \n",
      "I'm scraping big domain with lots of pagination and the crawler keeps getting stuck for hours with this log info   \n",
      "2024-01-12 02:11:16 \\[scrapy.extensions.logstats\\] INFO: Crawled 2016 pages (at 0 pages/min), scraped 4352 items (at 0 items/min)\n",
      "\n",
      "  \n",
      "Anyone faced that before?   \n",
      "\n",
      "\n",
      "https://preview.redd.it/t1pybbfgbybc1.png?width=1043&format=png&auto=webp&s=8426e827f0f25c246fb838bcaef920c142b6c604\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/194o53x/scrapy_gets_stuck_0_pages_min/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Extract database information from wordpress site\n",
      "Text: Hi!   \n",
      "\n",
      "\n",
      "There is a huge page with lenses and camera information [https://lens-db.com/](https://lens-db.com/) I would like to extract from. I know I could browse it and scrap manually the HTML but something tells me there must be a smarter way to just do queries to its database and extract it in a cleaner format.  \n",
      "\n",
      "\n",
      "I'm not so familiar with Wordpress or the php/jquery calls they do so looking for some advice on how I should do.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19453m4/extract_database_information_from_wordpress_site/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: [Help Request] Missing elements on a page\n",
      "Text: Hi all,\n",
      "\n",
      "I'm pretty new to webscraping generally. Also, I don't really have any background in webdev or anything like that (I'm an economist), so if I misname anything or don't refer to things correctly, I apologise. I've been trying to do some data collection for my PhD work and am running into an issue that I can't seem to get my head around. I've consulted Stackoverflow and chatGPT but can't seem to make any headway.\n",
      "\n",
      "The crux of the issue is: I'm trying to scrape a website of services that people offer, that contains their user profiles and a host of information related to them (location, price per hour of service, user ratings etc.). For reference, I'm doing it in R Selenium as the page is dynamically generated and scraping involves cycling through different pages on the website, and selecting/clicking on bits of the page. My background is in R, because I mainly do statistics/econometrics work - though I can limp along in Python as well. \n",
      "\n",
      "The issue I'm running into is: \n",
      "\n",
      "- The page can load a max. of 1,000 query results loading 20 at a time. Great. I write a script that takes the n number of returned queries and triggers the dynamic scroll the appropriate number of times to load all queries.\n",
      "- I write up a couple of lines that find the elements for userID. I extract them. I get a vector of N userIDs. \n",
      "- I then write up a couple of lines that find the elements for the price per hour. I extract them. I get a vector of prices. \n",
      "\n",
      "However... not all users provide a price per hour. So now, I end up with a vector of 1,000 user names, but only a vector 995 prices, and I have no idea where they become misaligned. When I manually check to see the profiles that don't contain prices, they simply do not contain the HTML class that relates to prices (as if it were empty, I'd just write something to impute an NA).\n",
      "\n",
      "My next approach was to try and forloop over the profiles themselves. So I run a findElement function for the profiles, assign to an object, and then for every *profile* in *profiles* I do a findElement for the prices and check length of HTML class 'price' and if >0 getElementText, if <= 0, then NA. The problem with this, is that for some reason I get *ALL* the text from the profile, not just the price. This sucks for 2 reasons, a) it is inefficient and runs very slowly (and I want to scrape the entire site, so this wouldn't be feasible) and b) it then requires a lot of data processing at the end to extract specific price information from the wall of text it has extracted. \n",
      "\n",
      "Does anyone know of any workarounds for this kind of thing? \n",
      "\n",
      "Much obliged!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1942z4d/help_request_missing_elements_on_a_page/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrape Submissions and Comments from specific subreddits.\n",
      "Text:  I am currently working on a project that involves extracting a large volume of submissions and their associated comments from a specific subreddit. I've attempted to achieve this using PRAW (Python Reddit API Wrapper), but I'm facing challenges in efficiently handling the rate limits and obtaining a vast amount of data.\n",
      "\n",
      "My goal is to retrieve thousands of submissions and their respective comments for in-depth analysis. I would greatly appreciate any guidance, tips, or examples.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/193xvk9/scrape_submissions_and_comments_from_specific/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Fully configurable scraper in go\n",
      "Text: Hello everyone!\n",
      "\n",
      "Link on project: [https://github.com/PxyUp/fitter](https://github.com/PxyUp/fitter)\n",
      "\n",
      "Last half year I was working on scraper solution for personal usage (cheap flight ticket). For that I was built scraper which also can aggregate information from different sources and stick them and slowly improve it.\n",
      "\n",
      "Current functionality:\n",
      "1. HTML/json parsing\n",
      "2. File downloading\n",
      "3. Create aggregation of different sources in json format\n",
      "4. Notification in telegram or console\n",
      "5. Browser simulator\n",
      "6. CLI for home using for bots/cron jobs and etc\n",
      " \n",
      "\n",
      "I feel this project can also be helpful to other people so please take a shot!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/193r4yd/fully_configurable_scraper_in_go/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Very small daily Twitter scrape - best way? OK with python, small amount of JS, or no-code.\n",
      "Text: Hi all\n",
      "\n",
      "I'm looking to do a daily scrape of say, 10-20 accounts, and collect tweets from the past 24hrs from each one. I think this should not add up to many requests (?).\n",
      "\n",
      "I feel I could almost do this manually, as the intention is to feed this into an LLM for some analysis each day (and eventually I will have a nice piece of research). But the LLM doesn't necessarily require the scrape to be amazingly formatted as JSON...  I think could just about load the page, do a text copy, paste into .txt, then send to the LLM.\n",
      "\n",
      "However that's a little boorish and I'd rather do something more elegant!\n",
      "\n",
      "Obviously API access is an issue these days.And not sure also if I need to be logged in to do this (probably on a new account so I don't brick mine?).\n",
      "\n",
      "Many one-click or no-code solutions are simply WAY too expensive for my small test case here...\n",
      "\n",
      "So I am looking for recommendations for a tool or tool stack/combo that could do this, or even run automated each day without me needing to specifically kick it off.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I've looked into a few (can't name them here?).\n",
      "\n",
      "Many other tools and guides are simply way of out date, from before Musk killed the API.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Thanks for your advice and recommendations.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\## Update 1 ##\n",
      "\n",
      "I think I was specifically looking for a twitter solution, but I might be better served diving deeper into what no-code/low-code scraping solutions exist.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\## Update 2 ##\n",
      "\n",
      "I'm surprised there are no AI solutions either! Apparently now with LLM models with vision (am I allowed to say the obvious one?) you can easily scrape sites behind logins because it's analysing a screencap and extracting the structure and text. \n",
      "\n",
      "Also, how do the free chrome extensions go? Seems like this could also be small-scale enough for me to use. Not like I'm trying to sneaker-bot and need incredibly large number of calls and robust filtering.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1937al7/very_small_daily_twitter_scrape_best_way_ok_with/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selenium - Can't extract text from item using \"driver.find_element(By.CLASS_NAME \"class name\")\n",
      "Text: I am trying to extract the text content of a using the CLASS_NAME method, and even though the is the only element with that class, when I assign the text of the span to a variable and print said variable, what gets printed is a return line.\n",
      "\n",
      "The structure of the page is the following:\n",
      "\n",
      "• There is the most outer that contain basically any information I need, which I was able top identify with the following snippet:\n",
      "```\n",
      "#Only One with this classed\n",
      "main_div = driver.find_element(By.CLASS_NAME,\"//div[@class='am-appointments am-section']\")\n",
      "```\n",
      "\n",
      "Inside of previous there a variable number of other that compartmentalize the informations I am seeking on a date basis, so each day has it own day. , which I was able top identify with the following snippet:\n",
      "```\n",
      "child_divs = main_div.find_elements(By.XPATH,\"./div[not(contains(@class, 'am-appointments-list-head') or contains(@class, 'am-pagination am-section'))]\")\n",
      "```\n",
      "\n",
      "with the former command I extracted a list of these child , each one of them has again a variable number of and each on of these contains the informations I need. So I ran a for loop like this:\n",
      "```\n",
      "for child_div in child_divs:\n",
      "\n",
      "appointments=child_div.find_elements(By.XPATH,\"//div[@class='am-appointments-list']//div[@class='el-collapse']//div[contains(@class, 'el-collapse-item am-appointment am-back-appointment') or contains(@class, 'el-collapse-item am-appointment am-front-appointment')]\")\n",
      "\n",
      "for appointment in appointments:\n",
      "    phone=appointment.find_element(By.CLASS_NAME, \"am-appointment-data-phone\")\n",
      "    phone_text=phone.text\n",
      "```\n",
      "\n",
      "I created again a list of all the inside each child_div, and ran again the loop, to find and extract the phone number from a < span> that has the following features:\n",
      "```\n",
      "<span class=\"am-appointment-data-phone\">+393314569013</span>\n",
      "```\n",
      "\n",
      "Even though the I am targeting in this final step is the only item with this class, the script doesn't print anything. So I guess somehow doesn't get assigned to the variable 'phone_text'. The < span> I am targeting is part of an animation, you click the button, the module expands and it's visible, or maybe there's some interference with Javascript ? I really don't understand what could be the issue. Thank you for anyone who can help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/193j6rp/selenium_cant_extract_text_from_item_using/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any idea on this?\n",
      "Text:     const puppeteer = require('puppeteer');\n",
      "    const {Cluster} = require('puppeteer-cluster');\n",
      "    \n",
      "    (async () => {\n",
      "      const cluster = await Cluster.launch({\n",
      "        concurrency: Cluster.CONCURRENCY_CONTEXT,\n",
      "        maxConcurrency: 10,\n",
      "        timeout: 120000,\n",
      "        skipDuplicateUrls: true,\n",
      "        puppeteerOptions: {\n",
      "          headless: true,\n",
      "          args: [\n",
      "            '--no-sandbox',\n",
      "            '--disable-setuid-sandbox',\n",
      "            '--disable-dev-shm-usage'\n",
      "          ],\n",
      "        },\n",
      "      });\n",
      "    \n",
      "            cluster.on(\"taskerror\", (err, data) => {\n",
      "            console.log(`Error crawling ${data}: ${err.message}`);\n",
      "      });\n",
      "    \n",
      "        await cluster.task(async({page, data: url }) => {\n",
      "            page.setDefaultNavigationTimeout(120000);\n",
      "        await page.goto(url, { waitUntil: 'networkidle2'});\n",
      "    \n",
      "    \n",
      "         const acceptBtn = await page.$('#onetrust-accept-btn-handler');\n",
      "             if (acceptBtn) {\n",
      "             await acceptBtn.click();\n",
      "             console.log('Clicked on the accept button');\n",
      "             } else {\n",
      "             console.log('Accept button not found');\n",
      "             }\n",
      "    \n",
      "        console.log(await page.url());\n",
      "    \n",
      "        await page.waitForSelector('.col-partnumber .highlight-link');\n",
      "            page.setDefaultNavigationTimeout(120000);\n",
      "        const urls = await page.$$eval('.col-partnumber .highlight-link', links => links.map(link => link.href));\n",
      "    \n",
      "        for (let i = 0; i < urls.length; i++) {\n",
      "          const fileName = urls[i].split('partno=')[1].replace(/%23/g, '') + '.pdf';\n",
      "          await page.goto(urls[i], { waitUntil: 'networkidle2'});\n",
      "          await page.waitForSelector('#PIMBrowserLayout > div.torso > div > div.row > div > div.col-xs-8.page-left > div:nth-child(2)');\n",
      "    const pdf = await page.pdf({ path: fileName, format: 'Letter', background: true });\n",
      "          console.log(`PDF generated for ${urls[i]}`);\n",
      "        }\n",
      "    \n",
      "    });\n",
      "    \n",
      "             const baseUrl = 'https://www.murata.com/ja-jp/search/productsearch?cate=cgsubCeramicCapacitors&stype=2&realtime=1';\n",
      "             const numPages =100;\n",
      "    \n",
      "                     for (let i = 1; i <= numPages; i++) {\n",
      "                             const url = (`${baseUrl}&pageno=${i}`);\n",
      "                     cluster.queue(url);\n",
      "                             }\n",
      "    \n",
      "    \n",
      "        await cluster.idle();\n",
      "        await cluster.close();\n",
      "    \n",
      "    })();\n",
      "\n",
      "anyone knows a work around timeouts, here is my current code, we are tasked to download all the pdf files of our product as practice but i am having issue handling timeout errors, i have tried to run it parallel at first but it took time so i switched to asynchronous, but now timeout errors are occurring more often...should i give up speed of run time for more accurate result?\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/193b02h/any_idea_on_this/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: In a search of an undetectable headless web browser\n",
      "Text: Well, the goal is: scrape the CloudFlare-protected website with a set of proxies. Assuming I have 4gb RAM VM with 1 CPU and the target website has like 2k pages. I will need to run it every week.\n",
      "\n",
      "The problem is, I still struggle to find the most developed and maintained undetectable headless browser.\n",
      "\n",
      "Which option do would you recommend? Share your experience!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/193hmsc/in_a_search_of_an_undetectable_headless_web/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraped data precision - any tools to improve the relevancy/accuracy of product price scraping?\n",
      "Text: I am in the process of making a scraper using python/selenium. I scraped product data/prices from price comparison websites, but we found many incongruencies in some of the products.\n",
      "\n",
      "The problem is - with some product codes (we have to use codes as the query for this website), there are many irrelevant products found, which ruins some data.  \n",
      "Example: [https://www.kurpirkt.lv/cena.php?q=68116](https://www.kurpirkt.lv/cena.php?q=68116)  \n",
      "We are looking for Beurer Humidifier, but there are many other irrelevant products found.\n",
      "\n",
      "Are there any common practices for cleaning such data?  \n",
      "We already had an idea to calculate the average price and if the variance is huge, add the brand to the query.  \n",
      "Any other ideas?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1936nor/scraped_data_precision_any_tools_to_improve_the/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help with understanding why my web scraping attempt is not returning the right page\n",
      "Text: So I am trying to create a script (Python) to scrape this page:  [Results in Windows - Microsoft Community](https://answers.microsoft.com/en-us/windows/forum?sort=LastReplyDate&dir=Desc&tab=Threads&status=answered&mod=&modAge=&advFil=MeTooVotes&postedAfter=2023-01-09&postedBefore=&threadType=questions&isFilterExpanded=true&page=1) \n",
      "\n",
      "Eventually I want to grab the question title, URL and views\n",
      "\n",
      "However I am falling at the first hurdle as what is returned is not even that page\n",
      "\n",
      "My Code:\n",
      "\n",
      " \n",
      "\n",
      "`from bs4 import BeautifulSoup`  \n",
      "`import requests`  \n",
      "`import csv`  \n",
      "`page_to_scrape = requests.get(\"https://answers.microsoft.com/en-us/windows/forum?sort=LastReplyDate&dir=Desc&tab=Threads&status=answered&mod=&modAge=&advFil=MeTooVotes&postedAfter=2023-01-09&postedBefore=&threadType=questions&isFilterExpanded=true&page=1\", allow_redirects=False)`  \n",
      "`soup = BeautifulSoup(page_to_scrape.text, \"html.parser\")`  \n",
      "`titles = soup.findAll(\"a\", attrs={\"class\":\"thread-title single-line-text\"})`  \n",
      "`print(page_to_scrape.content)`\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "What is returned:\n",
      "\n",
      "`b'<html><head><title>Object moved</title></head><body>\\r\\n<h2>Object moved to <a href=\"`[`https://answers.microsoft.com/en-us/site/silentsignin?returnUrl=https%3A%2F%2Fanswers.microsoft.com%2Fen-us%2Fwindows%2Fforum%3Fsort%3DLastReplyDate%26dir%3DDesc%26tab%3DThreads%26status%3Danswered%26mod%3D%26modAge%3D%26advFil%3DMeTooVotes%26postedAfter%3D2023-01-09%26postedBefore%3D%26threadType%3Dquestions%26isFilterExpanded%3Dtrue%26page%3D1`](https://answers.microsoft.com/en-us/site/silentsignin?returnUrl=https%3A%2F%2Fanswers.microsoft.com%2Fen-us%2Fwindows%2Fforum%3Fsort%3DLastReplyDate%26dir%3DDesc%26tab%3DThreads%26status%3Danswered%26mod%3D%26modAge%3D%26advFil%3DMeTooVotes%26postedAfter%3D2023-01-09%26postedBefore%3D%26threadType%3Dquestions%26isFilterExpanded%3Dtrue%26page%3D1)`\">here</a>.</h2>\\r\\n</body></html>\\r\\n'`\n",
      "\n",
      "As though the page is being redirected\n",
      "URL: https://www.reddit.com/r/webscraping/comments/192fav8/help_with_understanding_why_my_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Find where headers are being added to Ajax Request\n",
      "Text: I am trying to recreate an HTTP request through Python Requests but it looks like there are headers being added to the Ajax Request on the page that is validated server-side. Resending the same request (that worked previously) causes an error to be returned so the headers are important.\n",
      "\n",
      "[Here](https://reg.usps.com/portal/login) is the website I am trying to login to.\n",
      "\n",
      "I was able to find the code that sends the POST request:\n",
      "\n",
      "    var b = \"/entreg/json/AuthenticateAction\";\n",
      "    var a;\n",
      "    var c = {\n",
      "        username: d.username,\n",
      "        password: d.password,\n",
      "        newPassword: d.newPassword,\n",
      "        retypeNewPassword: d.retypeNewPassword\n",
      "    };\n",
      "    $.ajax({\n",
      "        url: b,\n",
      "        cache: false,\n",
      "        type: \"post\",\n",
      "        data: $(\"#loginForm\").serialize()\n",
      "    })\n",
      "\n",
      "Even when I run this code by itself, it sends the request and adds the headers (all prefixed with `X-jFuguZWB-`). It looks like there is some obfuscated code that runs something like `$.ajaxSetup({headers: {\"X-jFuguZWB-Z\": \"test-value\"}})` to attach that header to all requests.\n",
      "\n",
      "My question is, which part of [the page](https://reg.usps.com/portal/login) is adding these headers and is there any way to recreate the headers when I make my own POST request via Python?\n",
      "\n",
      "I did find some obfuscated javascript in the `<!-- Google Tag Manager -->` section of the page but I am unable to tell what it is doing.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/192s0pi/find_where_headers_are_being_added_to_ajax_request/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: 200 in Postman but 403 in Python\n",
      "Text: Hey there,I have an issue where I can't get my API request from Python Requests to go through. The same request works fine from Postman and also from the browser. I have already tried copying the exact headers that my browser used. I have also tried switching my VPN on and off.\n",
      "\n",
      "This is a service that requires authentication, which I have, and which gets accepted when I request from Postman/Firefox.\n",
      "\n",
      "Any ideas what causes the API to block my request when it's coming from Python (even when the 'user-agent' header says it's coming from Firefox), and how to avoid it?  \n",
      "\n",
      "\n",
      "*EDIT: I was able to get a response with Python by using the 'curl-cffi' library as suggested by The\\_\\_Strategist below. Thank you for the inputs.*\n",
      "URL: https://www.reddit.com/r/webscraping/comments/192citd/200_in_postman_but_403_in_python/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Trying to download images from a clothing site\n",
      "Text: Hi, automod said my previous post needed more details, so trying again. I'm trying to download clothing images from this site - https://www.myntra.com/fusion-wear\n",
      "\n",
      "The attributes of the images i'm trying to download are as follows:\n",
      "\n",
      "* HTML:\n",
      "<picture draggable=\"false\" class=\"img-responsive\" style=\"width: 100%; height: 100%; display: block;\"><source srcset=\"https://assets.myntassets.com/f_webp,dpr_1.5,q_60,w_210,c_limit,fl_progressive/assets/images/20695836/2022/11/10/ba1724c2-c606-481c-a0ca-63424b61a8661668078028270WomensRayonPrintedEmbroideredKurtaWithPantAndDupatta1.jpg\" type=\"image/webp\"><img draggable=\"false\" src=\"https://assets.myntassets.com/dpr_2,q_60,w_210,c_limit,fl_progressive/assets/images/20695836/2022/11/10/ba1724c2-c606-481c-a0ca-63424b61a8661668078028270WomensRayonPrintedEmbroideredKurtaWithPantAndDupatta1.jpg\" class=\"img-responsive\" alt=\"SINGNI Women Purple Ethnic Motifs Embroidered Mirror Work Kurta with Trousers &amp; Dupatta\" title=\"SINGNI Women Purple Ethnic Motifs Embroidered Mirror Work Kurta with Trousers &amp; Dupatta\" style=\"width: 100%; display: block;\"></picture>\n",
      "\n",
      "\n",
      "* XPATH: //*[@id=\"desktopSearchResults\"]/div[2]/section/ul/li[1]/a/div[1]/div/div/div/picture\n",
      "\n",
      "\n",
      "* Attribute: #desktopSearchResults > div.search-searchProductsContainer.row-base > section > ul > li:nth-child(1) > a > div.product-imageSliderContainer > div > div > div > picture\n",
      "\n",
      "\n",
      "I've tried using Selenium to scrape these but I'm only getting 11 images and then it stops. I have implemented a 60 sec pause to avoid getting blocked, but that doesn't seem to be helping as well.\n",
      "\n",
      "Advice much appreciated! This is not a commercial project, just want to be able to scrape 100-odd images as part of a project.\n",
      "\n",
      "Update: Also pasting the code here, in case someone can review as well!\n",
      "https://codeshare.io/7849m1\n",
      "URL: https://www.reddit.com/r/webscraping/comments/192apt1/trying_to_download_images_from_a_clothing_site/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Legal advice: scraping IMDB data\n",
      "Text:  Hello,\n",
      "\n",
      " I'm working on a movie-related project and need movie data like posters and ratings. I've been scraping IMDB for this purpose, but I'm considering going public and potentially making money from it. My question is, could I get into trouble for using data from IMDB and making money? I've come across mixed opinions on this issue. The most crucial information I found is from IMDB's website, stating, \"Robots and Screen Scraping: You may not use data mining, robots, screen scraping, or similar data gathering and extraction tools on this site, except with our express written consent as noted below.\" It's pretty clear that scraping is not allowed. On the flip side, there are plenty of websites out there earning money that are displaying IMDB ratings (like justwatch.com). \n",
      "URL: https://www.reddit.com/r/webscraping/comments/192clf5/legal_advice_scraping_imdb_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping naukri.com\n",
      "Text: I'm trying scrap [naukri.com](https://naukri.com) from f this page [https://www.naukri.com/data-science-jobs?k=data+science](https://www.naukri.com/data-science-jobs-2?k=data+science) , using bs4 and requests the parsed data shows only <path> tag with some numbers. Chat gpt said \"**it might be because the site uses java script to generate or update the content, and your scraping tool might not be able to capture the dynamic changes made by JavaScript.** \" I m putting the ss of the response here. Please help if you can....\n",
      "\n",
      "https://preview.redd.it/w57gau5zycbc1.png?width=1920&format=png&auto=webp&s=7d80abc553c7705fe8b980e8f5d4d9d1b2828de9\n",
      "\n",
      "https://preview.redd.it/8eeblu5zycbc1.png?width=1920&format=png&auto=webp&s=1d284c3b5a7ee08359a528e7f5c4a066fcd373e0\n",
      "\n",
      "https://preview.redd.it/1p7g6u5zycbc1.png?width=1920&format=png&auto=webp&s=da507819273c43cc4413d8d11bc30d02b33e37ff\n",
      "\n",
      "https://preview.redd.it/1v8edp90zcbc1.png?width=1920&format=png&auto=webp&s=8e252b3328793bf9ab0111ccd8d8a06c13be5ca0\n",
      "URL: https://www.reddit.com/r/webscraping/comments/192863q/scraping_naukricom/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Stuck with Socks5 on aws lambda script\n",
      "Text: Hi guys, I am stuck with a socks proxy on aws lambda, did anybody manage to run a socks4 or 5 proxy on aws lambda, I am using python and tried installing pysocks but it does not seem to work. any help would be appreciated \n",
      "URL: https://www.reddit.com/r/webscraping/comments/1927toq/stuck_with_socks5_on_aws_lambda_script/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Are there free hosting options to deploy a web scraper online?\n",
      "Text:  Are there reliable platforms that support web scraping tasks??  \n",
      "My options so far: PythonAnywhere, Heroku (I can't signup to Heroku for some reason so its out),  AWS .  \n",
      "The problem is that I don't know how to deploy them to these sites and what are the limitations.\n",
      "\n",
      "Does anyone here have experience with this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/191yyb2/are_there_free_hosting_options_to_deploy_a_web/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape the details from the MS Community Forums?\n",
      "Text: Hi all, I am wanting grab the post title and view count from the following Microsoft Community forum url:  [Results in Windows - Microsoft Community](https://answers.microsoft.com/en-us/windows/forum?sort=LastReplyDate&dir=Desc&tab=Threads&status=all&mod=&modAge=&advFil=MeTooVotes&postedAfter=&postedBefore=&threadType=Questions&isFilterExpanded=true&page=1) \n",
      "\n",
      "Ideally as a csv for a project I am working on.\n",
      "\n",
      "I would like do this once a month.\n",
      "\n",
      "Whats the best way to start. I am not a programmer, but can usually pull someone else script and fudge it to my own means.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/191mqa5/how_to_scrape_the_details_from_the_ms_community/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping from BI iframe\n",
      "Text: Hey all,\n",
      "\n",
      "I am trying to scrape some data from microsoft BI, from an iframe component  ([Link](https://app.powerbi.com/view?r=eyJrIjoiYzU0NDk3ZWQtMjQxZS00M2YwLTkyYzMtODA3ZWE5ZGQ2NzYwIiwidCI6IjdjZjgxMzU0LTE4OGUtNGM2MC1hMWFmLWQ2NDAyOGYzZGJmOSIsImMiOjl9)). The data is publicly available but a source comfortable to manipulate and analyze is not possible to find.  \n",
      "Any help? I should probably note that I am not experienced at this at all. Tried to use octoparse for this but can't seem to get it to scroll correctly.  \n",
      "\n",
      "URL: https://www.reddit.com/r/webscraping/comments/191jsm8/scraping_from_bi_iframe/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: List of newly created businesses?\n",
      "Text: Hey. Is there a website or a way I can scrape newly listed businesses? Does anyone know of anything like that or how to do it?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/191bzz9/list_of_newly_created_businesses/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to bypass nuData / nuDetect Anti-bot\n",
      "Text: NuData, a subsidiary of Mastercard, integrates with 3DS verification and features an anti-bot system called nuDetect, similar to Akamai's reliance on human behavioral analysis.\n",
      "\n",
      "# Understanding nuData's Functionality\n",
      "\n",
      "nuData operates through a website's domain or server, using a fingerprinting script found in URLs containing `/init/js/` or `/sync/js/`. For instance, [Kohls.com](https://Kohls.com) uses the following URL for nuData integration: `https://fc.kohls.com/2.2/w/w-552128/sync/js/`, utilizing a customer ID for request identification.\n",
      "\n",
      "The configuration of nuData is stored in a `pageModeConfig` property, and its fingerprinting script initializes a window property at `window.ndsapi`.\n",
      "\n",
      "# nuData's Initial Analysis Process\n",
      "\n",
      "nuData examines various properties, such as:\n",
      "\n",
      "* `pageX`, `pageY` coordinates of page elements, including the main document.\n",
      "* Scroll positions: `document.body.scrollLeft` & `document.body.scrollRight`.\n",
      "* Current time: [`Date.now`](https://Date.now)`()`. Likely for timezone calculation.\n",
      "* Numerous Math operations, likely assessing the JavaScript VM.\n",
      "* Common properties like `window._phantom`, `window.callPhantom`, `window.__phantomas`, etc.\n",
      "* Screen details: `window.screen.width`, `window.screen.height`, `window.screen.colorDepth`.\n",
      "* Flash plugin presence.\n",
      "* Navigator attributes, including language and device timezone.\n",
      "* WebGL parameters:\n",
      "   * `VERSION RENDERER SHADING_LANGUAGE_VERSION DEPTH_BITS MAX_VERTEX_ATTRIBS MAX_VERTEX_TEXTURE_IMAGE_UNITS MAX_VARYING_VECTORS MAX_VERTEX_UNIFORM_VECTORS MAX_COMBINED_TEXTURE_IMAGE_UNITS MAX_TEXTURE_SIZE MAX_CUBE_MAP_TEXTURE_SIZE NUM_COMPRESSED_TEXTURE_FORMATS MAX_RENDERBUFFER_SIZE MAX_VIEWPORT_DIMS ALIASED_LINE_WIDTH_RANGE ALIASED_POINT_SIZE_RANGE`\n",
      "* Canvas fingerprinting and font metrics, using a specific script to draw and analyze text on a canvas using the following script:\n",
      "\n",
      "&#8203;\n",
      "\n",
      "            var b = document.createElement(\"canvas\");\n",
      "            b.width = 200;\n",
      "            b.height = 40;\n",
      "            b.style.display = \"inline\";\n",
      "            var c = b.getContext(\"2d\");\n",
      "            c.fillText(\"aBc#$efG~ \\ude73\\ud63d\", 4, 10);\n",
      "            c.fillStyle = \"rgba(67, 92, 0, 0.5)\";\n",
      "            c.font = \"18pt Arial\";\n",
      "            c.fillText(\"aBc#$~efG \\ude73\\ud63d\", 8, 12);\n",
      "            a = b.toDataURL()\n",
      "\n",
      "# Behavioral Analysis by nuData\n",
      "\n",
      "nuData tracks all page events, including keyboard (`keyCode`) and mouse movements (`pageX`, `pageY`), logging the sequence and timing of these events. This data suggests analysis of user interaction speed. The collected data is encoded in a proprietary format, for example: `\"vce\":\"apvc,0,656p336o,2,1;fg,0,;zz,153,24r,2sn,;zzf,5r8,0...\".`\n",
      "\n",
      "# Bypassing nuData\n",
      "\n",
      "Bypassing nuData security requires a nuanced approach very much like bypass Akamai ([https://reddit.com/r/webscraping/comments/186jdhk/how\\_to\\_bypass\\_akamai/](https://reddit.com/r/webscraping/comments/186jdhk/how_to_bypass_akamai/)), especially considering its reliance on GPU rendering information to determine the operating system and device type. \n",
      "\n",
      "Here are some strategies:\n",
      "\n",
      "1. **GPU Rendering:** Emulate consumer-grade GPUs rather than professional hardware, as nuData's algorithms are tuned to recognize and differentiate between them.\n",
      "\n",
      "2. **Behavioral Analysis:** Utilize tools like ghost-cursor (found at [https://npmjs.com/package/ghost-cursor](https://npmjs.com/package/ghost-cursor)) to simulate human-like cursor movements and keystrokes. Timing is crucial here; movements or keystrokes that are too rapid can be flagged as suspicious.  \n",
      "\n",
      "\n",
      "# Learn more:\n",
      "\n",
      "Anti-bots are a growing issue among web scrapers, especially given how complex bypass can be for beginners. If you are struggling with nuData, come say hi on the **Web Scraping & Data Extraction** discord server!\n",
      "\n",
      "[https://discord.com/invite/fHbbHTq4CQ](https://discord.com/invite/fHbbHTq4CQ)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/190vc9w/how_to_bypass_nudata_nudetect_antibot/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help with the TG Desktop Restrictions.\n",
      "Text:  Hello, I need a way to surpass content d0wnl0ad restriction and forwarding on Telegram Desktop. I know on android files can be easily downloaded, and on TG Web there's some scripts for extensions for browser d0wnl0ading.\n",
      "\n",
      "However, I prefer to d0wnl0ad with the Desktop version because it's the faster one, and more easier to use to d0wnl0ad. And nope, I don't want to d0wnl0ad nothing ¡ll3gal or f0rbbid3n. Just a bit of soft-NSFW content,\n",
      "\n",
      "Hope you have a solution for me, any app of script to work with the Desktop version.\n",
      "\n",
      "Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1915o43/need_help_with_the_tg_desktop_restrictions/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Find URL not in google search results\n",
      "Text: Im trying to locate several web pages that do not appear in Google search results. I know the main domain and I know keywords that appear on the page. Can this be done with scraping? What would be the best/easiest tool for this?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/19193lw/find_url_not_in_google_search_results/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can my client run the script?\n",
      "Text: Hi guys, Iam having a selenium script that autofills a form in a particular website. Iam making this for a client. How can I set it up in a way that he can run the script whenever he needs? \n",
      "\n",
      "Should the script be stored in cloud? Or can it be made into an executable file, so that it runs with just a click in his local machine?\n",
      "\n",
      "Iam a noobie in web scraping. Hope you guys would help :)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/190mly1/how_can_my_client_run_the_script/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Airbnb web crawler made in go\n",
      "Text: Happy new year to everyone. I'm full stack developer especialiazed on building web crawlers.  \n",
      "I've been on the industry for about 6 years and now I would like to contribute a small project to the community.\n",
      "\n",
      "The project will get Airbnb's information including images, description, price, title ..etc\n",
      "\n",
      "[https://github.com/johnbalvin/gobnb](https://github.com/johnbalvin/gobnb)\n",
      "\n",
      "It uses only HTTP requests like an animal, I hate using selenium, puppeteer, playwright .. etc\n",
      "\n",
      "let me know what you think\n",
      "\n",
      "thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/190bz82/airbnb_web_crawler_made_in_go/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to find a job with web scraping skills?\n",
      "Text: The title says all.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18zynqw/how_to_find_a_job_with_web_scraping_skills/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Pulling Redfin URLS not for sale\n",
      "Text: Hi all,\n",
      "\n",
      "I'm somewhat new to scraping but I've found a lot of great resources here and elsewhere for pulling home information off of Redfin in Python. So far I'm able to:\n",
      "\n",
      "1. Given a list of urls, pull all of the information I'm looking for\n",
      "2. Use a Redfin search url to generate a list of for sale or recently sold houses to use in step #1\n",
      "\n",
      "However, I want to get a list of URLs for every single home in a certain zip code. For example, if I look up the [house from Scream](https://www.redfin.com/CA/Santa-Rosa/1820-Calistoga-Rd-95404/home/2278313) I can easily find it on Redfin. I could take that URL and add it to my list for step #1. However, I can't find an efficient way to get all of these urls for a zip code or neighborhood. Everything I've tried runs through Redfin's own search methods, which only ever produce urls for recently sold and currently for sale homes.\n",
      "\n",
      "Any assistance would very much appreciated, this is the last step in my project. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/1906yd7/pulling_redfin_urls_not_for_sale/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping google maps\n",
      "Text: Has anyone had success in scraping google maps? I am stuck on attempting to scroll down the list of results. Only the first few results are loaded. (Imitating a higher screen resolution results in more results being loaded at start). Upon scrolling, more results are fetched and loaded. I have been stuck for a long time attempting to recreate scrolling down with the mouse wheel on the results panel. I have tried everything across multiple platforms, puppeter, selenium, html-requests, everything.\n",
      "\n",
      "Link to the example of what I am talking about:\n",
      "\n",
      "https://www.google.com/maps/search/scrap+yard+Kansas+City&hl=en\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18zjqc5/scraping_google_maps/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I scrap Emails from Youtubers when I've reached the daily limit?\n",
      "Text: I need to get the emails of a list of youtubers for my work but I reached the daily limit, how can I circumvent this? Thankis in advance. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18zaorx/how_can_i_scrap_emails_from_youtubers_when_ive/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Proxies + VPN?\n",
      "Text: Currently making a scraper and using proxies. However, I don't want to accidentally reveal my IP. Would simply turning a VPN on reduce the risk of accidental IP reveal? Or are proxies sufficient? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18z9kt8/proxies_vpn/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help! Scraping products from anaconda\n",
      "Text: I am trying to automatically get product numbers on certain items at each store individually.  \n",
      "\n",
      "\n",
      "Ordering 1 item from a click and collect store and then changing that number in cart view to \"9999999\" it gives me the error \"Only # left\"  \n",
      "\n",
      "\n",
      "I want to use automatically pull this number into a spreadsheet. I just want to know the simplest way I can pull this number because my previous web scraping projects have been overcomplicated and I am sure there is an easier way to do this.  \n",
      "\n",
      "\n",
      "[Error](https://preview.redd.it/txoacus69lac1.png?width=1920&format=png&auto=webp&s=c604002dfbfae9175c7414daca75f95dcf70cb6d)\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18z2r1x/help_scraping_products_from_anaconda/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Hungama.com\n",
      "Text: Hi, I am trying to scrape hungama.com for practising my skills. I tried using selenium, html-requests but nothing seems to work.\n",
      "1. I send request to hungama.com\n",
      "2. I send a request to hungama.com/music \n",
      "3. Pick up all a tags from div id languageBar from /music page html.\n",
      "4. I tried using .click on the a tags using selenium it would not fetch the complete html that I can see on the browser. I tried using sleep, wait.until, implicitly_wait. \n",
      "Please guide me as to what I might be missing and point me in the right direction.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18z0xdl/scraping_hungamacom/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Static tables: web scraping\n",
      "Text: Hello everyone,\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I have this paginated table here([link](https://www.immihelp.com/all-h1b-visa-sponsors/2021/5289/)), can anyone tell me tricks to scrape this? I want the data from years 2010-present and each year have 5000+page. Can anyone tell me a strategy to scrape this data?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ynaad/static_tables_web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help with Python Scraper not working\n",
      "Text: I’m trying to create my own scraper, and following a tutorial online, but I can only get data from one product to appear rather than all the data on the page. In the tutorial, all the data for every product on the page appears for the guy. As you can see, on my terminal, only one shoe “Women’s Altra” appears. The last photo shows what his results in. Please help!\n",
      "URL: https://www.reddit.com/gallery/18yv9xg\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help Scraping information from an iframe\n",
      "Text: Hello, \n",
      "\n",
      "I am currently working to scrape information from a website where I need to login and then select a link where an iframe pops up. \n",
      "\n",
      "I’m currently using selenium to login which is working successfully, but I cannot find any elements inside of the iframe to begin scraping the information. \n",
      "\n",
      "Any suggestions on how to traverse an iframe? Any insight would be much appreciated. \n",
      "\n",
      "Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18yfuyb/help_scraping_information_from_an_iframe/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrapping ecom site built on SAP Commerce\n",
      "Text: Hello everyone, most ecom site are built using some framwork (shopyfiy, magento etc) and that make it quite easy to scrape maybe through API of Json or something..\n",
      "\n",
      "I've been trying to scrape [extra.com/en-sa](https://extra.com/en-sa), its a nightmare to scrape  efficiently.   \n",
      "Can you guyz please provide any idea :) \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Thank you.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18yc86j/scrapping_ecom_site_built_on_sap_commerce/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape leads based on registration date?\n",
      "Text: Hello guys, I have a SMMA agency, and usually we find our leads from google maps, yelp or yellowpages or whatever, but recently i had this idea of why not targeting newly registered businesses, i think it would be a great idea specially for niches that requires a solid budget to start the business like car rental firms, or hotels...  what do you think of this idea? and whats the best method to scrape leads of those businesses?\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18yc3nl/how_to_scrape_leads_based_on_registration_date/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How To Replace Expired Cookies\n",
      "Text: Scraping noob here.\n",
      "\n",
      "I am using python to scrape websites with known bot detection by finding the exact endpoint I want data from and creating a request in python for that endpoint. Simple enough and it is working for a majority of my use cases. \n",
      "\n",
      "For a few of the websites certain cookies are required. Some of these cookies seem to expire over time for different reasons. I've recently been experimenting with using residential proxies and that does in fact cause the cookies to last longer.  \n",
      "\n",
      "How can I either   \n",
      "A) make one set of cookies last forever (I feel like this is impossible)  \n",
      "B) replace the expired cookies with new cookies\n",
      "\n",
      "My question then becomes how can I replace the cookies?   \n",
      "\\- Should I find the exact requests that create these cookies and reproduce those?  \n",
      "or   \n",
      "\\- Should I try using selenium to retrieve all the expiring cookies and then use the requests I currently have in place? \n",
      "\n",
      "I can put in the work to find the exact cookies that actually need to be replaced.   \n",
      "\n",
      "\n",
      "PS. I know it's expiring cookies because if I go back to the site and get new cookies for that request , my code works again. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18y750w/how_to_replace_expired_cookies/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I am building real-time online css selector tool.\n",
      "Text: Inspired by https://try.jsoup.org/ and https://anglesharp.github.io/\n",
      "I started to build real-time css selector so I can instantly see the result of css select.\n",
      "\n",
      "This is the first version: https://css-selector-xi.vercel.app/\n",
      "\n",
      "Please provide some feedback so I know what to build and if there's any errors.\n",
      "\n",
      "If you know any other tool like this let me know.\n",
      "\n",
      "Thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18y702s/i_am_building_realtime_online_css_selector_tool/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Detect CMS for scraping\n",
      "Text: First time scraping this kind of shops.\n",
      "\n",
      "Anyone have similar experience, any other way instead of locating elements by class names ?\n",
      "\n",
      "[https://www.etniabarcelona.com/ba/en/optical/psator-o-gdpt](https://www.etniabarcelona.com/ba/en/optical/psator-o-gdpt)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "h1: product-sidebar\\_\\_title   = Title ... and so on\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18y6zz4/detect_cms_for_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Instagram stories?\n",
      "Text: The title basically, a couple of years ago I was scraping Instagram stories - it was pretty straight forward back then. \n",
      "\n",
      "Now, I just get the feeling the the security system is much more robust, I’m using pva account (get blocked all the time) and so on. \n",
      "\n",
      "Any high level tips and tricks based on latest updates?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18xueh7/scraping_instagram_stories/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to run browser code on NodeJS?\n",
      "Text: There is an API I am trying to reverse engineer, but it has tons of obfuscated code. There are IIFEs, closures, and more importantly, it uses document and window object judiciously. How do I take the script and run it in a NodeJS environment?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18xtbqe/how_to_run_browser_code_on_nodejs/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping data sources for airfares\n",
      "Text: Hi All,\n",
      "\n",
      "I have no previous experience with webscraping but I'm trying to learn and have a specific use case in mind I'd like yourinput on. I want to find flight deals from a specific airport to any destination in the world (and ideally multi-city options for closeby cities as well) with departure date and duration date flexible (e.g. departure anytime in next 3 months and trip duration anything between 1 and 3 weeks). I'm having trouble pinning down though which data source I can use. Sites such as Skyscanner offer search options for destination \"anywhere\" and some predetermined durations like 1 or 2 weeks but not for any duration within e.g. 1 and 3 weeks. I'd appreciate your suggestions for airfare data sources where I could be able to find this data (perhaps it is available in Skyscanner/ Google Flights/ Kayak etc) and how to go about extracting these. Would be much appreciated as it would really help me in my first step towards learning webscraping.\n",
      "\n",
      "Basically what I'd like to make for myself  is something like this: \n",
      "\n",
      "[https://escape.flights/flights-from-new-york/](https://escape.flights/flights-from-new-york/)  \\--> shows all flight deals departing from LAX\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18xkl9j/webscraping_data_sources_for_airfares/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web Scraping\n",
      "Text: How to check is it allowed to scrape a website? even i checked by adding the word robots.txt but shows nothing.   \n",
      "and secondly, is it okay to scrap a immigration website for the immigration agency to provide all the visa details?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18xb6z0/web_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Amazon web crawler made in go\n",
      "Text: Happy new year to everyone.I've been working as web crawler developer for about 6 years and now I would like to contribute a small project to the community.\n",
      "\n",
      "The project will get Amazon's product information  including images, description, price, title ..etc\n",
      "\n",
      "[https://github.com/johnbalvin/gozon](https://github.com/johnbalvin/gozon)\n",
      "\n",
      "It uses only HTTP requests like an animal, I don't like using selenium, puppeteer .. etc\n",
      "\n",
      "let me know what you think\n",
      "\n",
      "thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18wa3n0/amazon_web_crawler_made_in_go/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How can I extract only the Player stats table from FBRef?\n",
      "Text: Hello! I'm kinda new to web scraping and the like, so if I sound like a beginner, it is cuz I am :)  \n",
      "Anyway, I want to extract the Player stats table alone from the below Fbref link with python (I'm using Jupyter notebook btw)  \n",
      "[https://fbref.com/en/comps/9/misc/Premier-League-Stats#all\\_stats\\_misc](https://fbref.com/en/comps/9/misc/Premier-League-Stats#all_stats_misc)\n",
      "\n",
      "If you open the link there are two tables, one with Squad misc stats and another with Player misc stats below it. I want the latter. I tried using BeautifulSoup and only the Squad misc table was extracted which I don't need.  \n",
      "So if anyone can help me out with a code snippet in python with BeautifullSoup or a different method altogether, it'd be really helpful. Thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ws6ky/how_can_i_extract_only_the_player_stats_table/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: ERR_BAD_REQUEST with axios.get()\n",
      "Text: Hello everyone,I need to do some data collection on the HLTV team rankings for the Counter-strike game. As of December 25, 2023, the team rankings are :  [Counter-Strike Ranking | World Ranking | HLTV.org](https://www.hltv.org/ranking/teams/2023/december/25) . I need to have access to the complete HTML of the page so I can lookup the rank of the team \"FaZe\", for example. I am totally new to webscraping, so I tried using a GET request via axios :\n",
      "\n",
      "`const TEAM_RANKING_URL = \"`[`https://www.hltv.org/ranking/teams/2023/december/25`](https://www.hltv.org/ranking/teams/2023/december/25)`\"`\n",
      "\n",
      "`const axios = require('axios')`\n",
      "\n",
      "`axios.get(TEAM_RANKING_URL)` \n",
      "\n",
      "`.then(res => console.log(res))` \n",
      "\n",
      "`.catch(err => console.warn(err))`\n",
      "\n",
      "Unfortunately, I get the error message  `AxiosError {message: 'Request failed with status code 403', name: 'AxiosError', code: 'ERR_BAD_REQUEST', config: {…}, request: ClientRequest, …}` . I am running this code in node.js in VScode.\n",
      "\n",
      "I think the issue is related to how the server accepts GET requests, i.e. they do not accept GET requests not coming from browsers. Can somebody help me connect to this website via node.js and have access to the html? Thank you\n",
      "\n",
      "P.S. Ultimately, my goal is to be able to do stuff like `document.getElementsByClassname()` so I can parse through the rankings.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18wrpv0/err_bad_request_with_axiosget/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping for reddit posts and comments in a specific subreddit / subreddits with a specific keyword search ?\n",
      "Text: Hi is there a way to to scrape for posts and comments , including meta data by keyword in specific subreddit s ? \n",
      "\n",
      "For example I want to scrap all the posts and comments for the keyword \"acoustic\" from audioengineering and python sunreddits ? I fhink praw library does not provide a way to do that right ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18w6dcu/scraping_for_reddit_posts_and_comments_in_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: how to extract data with css in class names\n",
      "Text: I am trying to extract data from this website:\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "[https://catalog.registrar.ucla.edu/major/2022/mathematicsofcomputationbs?siteYear=2022](https://catalog.registrar.ucla.edu/major/2022/mathematicsofcomputationbs?siteYear=2022)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "`response.xpath('//div[@id=\"Overview\"]/div[@class=\"css-3chzt3-Box--Box-Box-Card--CardHeader e12hqxty2\"]/h3/text()').extract()`\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I'm starting off by trying to get the \"Overview\" text using both CSS and xpath but nothing seems to be working and the class names and id seem to be long and random, which I assume is because its JavaScript generated. I was wondering how I would approach extracting data with those type of class names using scrapy-splash and scrapy. It would be very helpful if you can link some video or articles too.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I checked the generated html on my dockers and realized that it is giving me the data in a json script. Is that supposed to happen?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18w8ont/how_to_extract_data_with_css_in_class_names/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Looking for a scraper - maps data\n",
      "Text: I'll start by saying I'm *somewhat* technologically advanced, but stuff like this is way over my head. I'd like to find a tool that's executable, or cloud-based because that's about what I'm able to understand and use. \n",
      "\n",
      "I know there's numerous options out there, but would love some direction on what the best fit for me is. \n",
      "I need to pull in Google Maps data from around 500~ restaurants. I have a list compiled of the ones I need data from. \n",
      "The data I need is their website address - and their Summery / Details in whatever format they have listed. (for instance, if it's an Italian restaurant and they have a brief 2-3 sentence description for their establishment, I need this) \n",
      "\n",
      "\n",
      "Also, I notice many restaurants don't have a summary or details listed on their Google business profile. Would there then be a way to pull this info from their site in some format?\n",
      "\n",
      "\n",
      "I appreciate the help!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18w2i7l/looking_for_a_scraper_maps_data/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Chegg scraper ideas?\n",
      "Text: [ Removed by Reddit in response to a copyright notice. ]\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18w8n95/chegg_scraper_ideas/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Making sure the scraped data looks the same when uploading..whats the best and easiest way to do this?\n",
      "Text: https://us.store.bambulab.com/products/x1-carbon-combo i want to scrape this webpage, save it as excel file and then upload to wordpress, but i want the content to look the same i.e headings, formatting, pictures etc.. whats the best and easiest way to achieve this using python?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18vvat9/making_sure_the_scraped_data_looks_the_same_when/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Monthly Self-Promotion Thread - January 2024\n",
      "Text: Hello and howdy, digital miners of /r/webscraping!\n",
      "\n",
      "The moment you've all been waiting for has arrived - it's our once-a-month, no-holds-barred, show-and-tell thread!\n",
      "\n",
      "* Are you bursting with pride over that supercharged, brand-new scraper SaaS or shiny proxy service you've just unleashed on the world?\n",
      "* Maybe you've got a ground-breaking product in need of some intrepid testers?\n",
      "* Got a secret discount code burning a hole in your pocket that you're just itching to share with our talented tribe of data extractors?\n",
      "* Looking to make sure your post doesn't fall foul of the community rules and get ousted by the spam filter?\n",
      "\n",
      "Well, this is your time to shine and shout from the digital rooftops - Welcome to your haven!\n",
      "\n",
      "Just a friendly reminder, we do like to keep all our self-promotion in one handy place, so any separate posts will be kindly redirected here. Now, let's get this party started! Enjoy the thread, everyone.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18vukvj/monthly_selfpromotion_thread_january_2024/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping a price using AI, Vision, without tracking classnames and id's?\n",
      "Text: A year ago I built a book price webapp that checks a few websites, I was using puppeteer to find the correct div and extract the data.\n",
      "\n",
      "Now with AI tools around, is there an easier way to do this? Is it possible, for example to get a screenshot of a webpage and use AI to get the correct price? Any other methods now that are more fail-safe then using classNames? Since there can change.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18vt6ma/scraping_a_price_using_ai_vision_without_tracking/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: https://pixelscan.net/ tips?\n",
      "Text: Hello guys, recently I've become aware of https://pixelscan.net/, an interesting site to analyse my scraper if it's fingerprint is identifiable as bot.\n",
      "\n",
      "As expected, its informing that my fingerprint is inconsistent, but does not offer any tip of \"how\" it detected. \n",
      "\n",
      "Any tips ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18vme1n/httpspixelscannet_tips/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Bulk scrape big list of URL's, the visible inner text (article content) to doc or txt files\n",
      "Text: I need something where can input CSV file of URL's which will then be scraped in bulk and then an output file to save the content to word document or text file if cant do word, ideally appending the data to the document as i want to scrape multiple URL's into the same file in many cases (like similar articles), purpose is bulk data collection for AI analysis for research.\n",
      "\n",
      "Any ideas on apps to do this or get close, it feels simple enough to exist already?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18vgxkp/bulk_scrape_big_list_of_urls_the_visible_inner/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Would this project be feasible/useful?\n",
      "Text: I want make a YT scraper that searches for a word, and then finds videos from the last week, comparing their views to channel avg views in last 10, and then attribute the positive or negitive difference to each word in the title, and finally end with a list of words sorted by how much they increase views compared to average.\n",
      "\n",
      "Just wondering how feasible and or useful this could be?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18v436h/would_this_project_be_feasibleuseful/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrapping Player stats from sofascore\n",
      "Text: I' trying to extract players stats from sofascore however everytime i switch to inspect page the complete page change from this (pic 1.) to this (pic 2.) and the table dissapear.  \n",
      "\n",
      "\n",
      "[https://www.sofascore.com/tournament/football/colombia/primera-a-clausura/11536#id:52847,tab:top\\_players](https://www.sofascore.com/tournament/football/colombia/primera-a-clausura/11536#id:52847,tab:top_players)\n",
      "\n",
      "https://preview.redd.it/9puoxne1fj9c1.png?width=1522&format=png&auto=webp&s=b83a1faeb689dfe4623664377213deaa00803a49\n",
      "\n",
      "https://preview.redd.it/hc12knlefj9c1.png?width=1871&format=png&auto=webp&s=7a8ab2bbc6210e92a76ede9380effcb6890ce74f\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18uwf27/scrapping_player_stats_from_sofascore/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help.\n",
      "Text: In output the same name is repeating, I want name and expertise of all the doctors on the website. Also, I need data of 500 doctors but it is giving only 10. Any solutions?\n",
      "URL: https://www.reddit.com/gallery/18uixsn\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape YouTube data like SocialBlade?\n",
      "Text: I want to build something similar to SocialBlade. They're tracking millions of YouTube channels. Does anybody know how I can scrape YouTube frequently at scale like that?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18umw6p/how_to_scrape_youtube_data_like_socialblade/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Webscraping Shopee Marketplace\n",
      "Text: I’m trying to scrap product data from shopee.com.br, but the pages don’t load correctly, loads empty, with just the cookies CTA and background, without any data. It looks like it has advanced protection against scraping. What’s the best approach in this scenario?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18uke2d/webscraping_shopee_marketplace/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help.\n",
      "Text: from selenium import webdriver\n",
      "from selenium.webdriver.common.by import By\n",
      "from selenium.webdriver.support.ui import WebDriverWait\n",
      "from selenium.webdriver.support import expected_conditions as EC\n",
      "import pandas as pd\n",
      "\n",
      "url = 'https://intake.steerhealth.io/doctor-search/aa1f8845b2eb62a957004eb491bb8ba70a'\n",
      "driver = webdriver.Chrome()\n",
      "driver.get(url)\n",
      "\n",
      "# Use WebDriverWait to wait for elements to be present\n",
      "wait = WebDriverWait(driver, 20)\n",
      "\n",
      "# Wait for at least one element with the specified class name to be present\n",
      "doctors = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'NewProviderCard__Wrapper-sc-12vowct-0.iysCTA')))\n",
      "\n",
      "doctor_list = []\n",
      "\n",
      "for doctor in doctors:\n",
      "    # Use find_element_by_xpath on the doctor element\n",
      "    name_element = wait.until(EC.presence_of_element_located((By.XPATH, './/*[@id=\"__next\"]/div/div/div/div[2]/div[3]/div[1]/div[1]/div[1]/div/div/b')))\n",
      "    expertise_element = wait.until(EC.presence_of_element_located((By.XPATH, './/*[@id=\"__next\"]/div/div/div/div[2]/div[3]/div[1]/div[1]/div[2]/div[1]/div[1]/b')))\n",
      "    \n",
      "    # Get the text content of the elements\n",
      "    name = name_element.text\n",
      "    expertise = expertise_element.text\n",
      "    \n",
      "    doc_item = {\n",
      "        'name': name,\n",
      "        'expertise': expertise\n",
      "    }\n",
      "    \n",
      "    doctor_list.append(doc_item)\n",
      "\n",
      "df = pd.DataFrame(doctor_list)\n",
      "print(df)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help: I am a beginner in web scrapping. I need names of all doctors but it's repeating the first name only. Also, I need data of all 500 doctors on the website, but getting only 10. Any idea how to do this?\n",
      "URL: https://i.redd.it/sgbwo8rkeg9c1.jpeg\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: My scraper stops working from time to time. Would an E-Commerce scraper API be a solution?\n",
      "Text: Hello, a guy from Fiverr build a webscraper for me. I am scraping Idealo and I think they are changing their website and bot protection often and that causes the problems. I am thinking to switch to an E-Commerce scraper API. It would be a bit more expensive, but I hope that would fix the issues and then the bot would run more stable. Can somebody here recommend a good E-Commerce scraper API service? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ualmc/my_scraper_stops_working_from_time_to_time_would/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best way to booking webscrape without coding\n",
      "Text: Hi, what is the best way to webscraping booking.com hotels using (booking filter) without coding?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18uc0u3/best_way_to_booking_webscrape_without_coding/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: new here and trying to learn web scraping on a government website\n",
      "Text: hi there,\n",
      "\n",
      "I new here and trying to learn web scraping for the first time ,so i tried doing a project about finance and managing budget by using a open source government website that show all the cost for old projects.\n",
      "\n",
      "link = [https://tenders.etimad.sa/Tender/AllTendersForVisitor](https://tenders.etimad.sa/Tender/AllTendersForVisitor)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "What do you guys suggest the best approach for getting all name of project and the cost ?\n",
      "\n",
      "i tried using BeautifulSoup and i got a lot of error for some reason (mainly ssl.SSLCertVerificationError)\n",
      "\n",
      "which libraries is best to learn for this kinda situation?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "thanks\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18uf7md/new_here_and_trying_to_learn_web_scraping_on_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help Needed: Managing Multiple Users in a Selenium Flask App Without Data Conflict\n",
      "Text: &#x200B;\n",
      "\n",
      "Hi everyone,\n",
      "\n",
      "I'm currently working on a Flask app that uses Selenium for web scraping. I've encountered a challenge where multiple users might be using the scraper at the same time. I'm seeking advice on how to ensure that there are no conflicts and users don't end up receiving data meant for others.\n",
      "\n",
      "Here are some specific points I'm unsure about:\n",
      "\n",
      "1. **Isolation of User Sessions:** How can I make sure that each user's scraping session is isolated and independent?\n",
      "2. **Data Integrity:** What are the best practices to maintain data integrity so that each user gets only their data?\n",
      "3. **Scalability:** As the number of users increases, what should I consider to keep the app scalable and efficient?\n",
      "4. **Error Handling:** How can I effectively handle errors that might occur when multiple users are scraping simultaneously?\n",
      "5. **Resource Management:** Are there any specific strategies for managing resources (like memory and processing power) when the app is under heavy load?\n",
      "\n",
      "Any insights, code snippets, or resource links would be greatly appreciated. I'm relatively new to this, so detailed explanations or references to similar projects would be super helpful.\n",
      "\n",
      "Here is my current flask app code\n",
      "\n",
      "    from flask import Flask, render_template, jsonify, request\n",
      "    import threading, string, random\n",
      "    from gmbscrape import Scrape\n",
      "    \n",
      "    app = Flask(__name__)\n",
      "    data = []\n",
      "    def generate_secret_key(length=24):\n",
      "        characters = string.ascii_letters + string.digits + string.punctuation\n",
      "        return ''.join(random.choice(characters) for i in range(length))\n",
      "    \n",
      "    app.secret_key = generate_secret_key()\n",
      "    \n",
      "    @app.route('/', methods=['GET', 'POST'])\n",
      "    def index():\n",
      "        if request.method == 'POST':\n",
      "            location = request.form.get('location').split(\",\")\n",
      "    \n",
      "            industry = request.form.get('industry')\n",
      "            if (industry and location):\n",
      "                scraper_thread = threading.Thread(target=run_scraper, args=(location, industry))\n",
      "                scraper_thread.start()\n",
      "                return render_template('index.html', submitted=True, curr_status=f\"Scraping {industry} in {', '.join(location)} | May Take A Couple Minutes For Data To Appear...\")\n",
      "    \n",
      "            else:\n",
      "                return render_template('index.html', submitted=False)\n",
      "    \n",
      "        return render_template('index.html', submitted=False)\n",
      "    \n",
      "    @app.route('/get-data')\n",
      "    def get_data():\n",
      "        return jsonify(data)\n",
      "    \n",
      "    def run_scraper(location, industry):\n",
      "        scraper = Scrape(location, industry, data)\n",
      "        scraper.start_scrape()\n",
      "        \n",
      "    \n",
      "    if __name__ == '__main__':\n",
      "        app.run(debug=False)\n",
      "    \n",
      "\n",
      "Thank you in advance for your help!\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18u91wo/help_needed_managing_multiple_users_in_a_selenium/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Reselling energy prices through my own API\n",
      "Text: I'm want to build a web scraper for energy prices in my country and resell these prices through my own API. The company responsible for calculating these prices has a monopoly (let's call them Ecorp) and are explicitly stating that \"automated extraction is not permitted\" on their webpage.\n",
      "\n",
      "Some other facts about the situation with Ecorp:\n",
      "\n",
      "1. They are publicly publishing the prices (without any login or authentication) once a day on their own website.\n",
      "2. The prices are also available on many other websites, but as far as I know all prices are sourced from the Ecorp website, one way or another.\n",
      "3. You can also access the energy prices through the Ecorp API that is quite costly to gain access to.\n",
      "\n",
      "What I'm reading from different sources is that publicly available data that is not protected (with auth mechanisms) is free for all, but I'm very curious in this case since Ecorp are explicitly saying scraping is not permitted and they are reselling the data through their own API today.\n",
      "\n",
      "Obviously they wouldn't be happy if I started selling this data too, but am I doing something illegal?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Edit: I reside in the EU.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18t66l8/reselling_energy_prices_through_my_own_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Issues with LinkedIn li_at Cookie Expiration During Automation\n",
      "Text: Hello fellow web scrapers,\n",
      "\n",
      "I'm encountering a problem with LinkedIn automation using Playwright and custom scripts. Previously, adding the li\\_at cookie to the context allowed smooth operations from cloud services like AWS Lambda. Typically, the cookie lasted a few weeks unless I logged in again. However, I'm now facing a challenge where a freshly obtained cookie expires almost immediately after one or two brief uses in the cloud.\n",
      "\n",
      "Attempts to use a residential proxy (specifically from Smartproxy) failed, as I couldn't access the login page. I suspect the issue might relate to logging in from Germany but running the scrape from a data center IP in a different location. Any advice or suggestions to overcome this cookie expiration problem would be greatly appreciated!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18t2xao/issues_with_linkedin_li_at_cookie_expiration/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping text and attachments on a Website\n",
      "Text: Hello, \n",
      "\n",
      "I am researching the management of the covid pandemic. \n",
      "\n",
      "I would like to scrape information from Hospital websites like this [https://documentale.asl2abruzzo.it/L190/?idSezione=369&id=&sort=&activePage=&search=](https://documentale.asl2abruzzo.it/L190/?idSezione=369&id=&sort=&activePage=&search=)\n",
      "\n",
      "However, other than what is written on the page, I would like to get also the attachments present on the site (PDF, EXCEL etc,). \n",
      "\n",
      "What approach do you suggest to get all the attachments on a website and \n",
      "\n",
      "also automatically read what is written in the documents? \n",
      "\n",
      "Do you suggest any approach/website to do that?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18sv41k/scraping_text_and_attachments_on_a_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Do price comparison sites infringe copyright law?\n",
      "Text: They usually scrape data and images from the site at least in the beginning. How do they still survive? These sites are obviously bad for the stores. Trolley.co.uk almost closed down because of this. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18sn5nx/do_price_comparison_sites_infringe_copyright_law/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Different content python (aiohttp) and safari\n",
      "Text: Hi there.\n",
      "\n",
      "I am currently scraping an online shop and they provide product data in json format, what is great.\n",
      "\n",
      "Unfortunately they only provide this data if requested with a browser (safari/firefox). If I imitate the browser's requests (same headers) with aiohttp / Python, they return the full page but the json is not included.\n",
      "\n",
      "Any hints on what I am missing out?\n",
      "\n",
      "Using playwright, I receive the json data.\n",
      "\n",
      "cheers!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18sbzmr/different_content_python_aiohttp_and_safari/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I need help to download a daily historical index price\n",
      "Text: This index doesn't propose any downlad button but I don't know anything about python or webscraping. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "[https://finance.yahoo.com/quote/%5ESP500-5020/history?period1=1672099200&period2=1691020800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true](https://finance.yahoo.com/quote/%5ESP500-5020/history?period1=1672099200&period2=1691020800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true)\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "If anyone can help me with that. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18s5hv5/i_need_help_to_download_a_daily_historical_index/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Any advice on how to scrape a 150M tweets?\n",
      "Text: Hello community,\n",
      "\n",
      "I'm new to twitter scraping and I want to scrape a 150M tweets from certain geographic areas for my research. Any advice on what is the best and most efficient way to do it? how much it may cost me? and What would be the size of the collected text from these tweets if stored on a text file?\n",
      "\n",
      "Thanks in advance \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18rzpgu/any_advice_on_how_to_scrape_a_150m_tweets/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to reverse-engineer the NEWS API?\n",
      "Text: I want to use the NEWS API: https://newsapi.org/. Obviously I do not want to pay for it. I also want to get a lot of results: potentially 1000+ results per query. Do you guys know how I could replicate the behavior of this API?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18s41z1/how_to_reverseengineer_the_news_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Weather Forecast Data from a Site With Dated Values\n",
      "Text: Looking for an ordinary weather forecast site that shows extensive info about meteorological parameters with history of values for our data mining project, any suggestions?\n",
      "\n",
      "A site like this maybe: [WunderGround](https://www.wunderground.com/weather/us/ca/san-francisco/37.78,-122.42) (A bot detection problem occurred.)\n",
      "\n",
      "Or if it is possible, how can I pull data from removed, past content of a site effectively?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18rz768/weather_forecast_data_from_a_site_with_dated/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Google Ads clicker detected as Invalid Clicks\n",
      "Text: Hey,  \n",
      "\n",
      "\n",
      "I'm using an ad\\_clicker which use undetected-chromedriver under the hood to click on some ads and some random residential proxy (Smart Proxy atm, no mobile proxy)  \n",
      "\n",
      "\n",
      "I'm looking for a way to increase my valid click counts on Google Ads.  \n",
      "\n",
      "\n",
      "Any tips appreciated ! :) \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18rxwwv/google_ads_clicker_detected_as_invalid_clicks/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Has anyone scrapped data from court websites ?\n",
      "Text: Real estate data etc… looking for some pointers\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18rtqe4/has_anyone_scrapped_data_from_court_websites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: a way effectively scrape off data without limitation on google maps?\n",
      "Text: if you use the google maps gui, or their apis, it seems that they do limit you results\n",
      "\n",
      "if you search like for local business restaurants, in a selected area, then you will have around 10 or 20 results, which are too few vs the actual restaurants in the area, and so by consequence I do suppose that for some reasons gmaps is limiting your research queries\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18r8ikj/a_way_effectively_scrape_off_data_without/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to pass antiBot test of aliexpress\n",
      "Text: .\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18quo0n/how_to_pass_antibot_test_of_aliexpress/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Do you have any resources of Heads and http protocol java selenium\n",
      "Text: .\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18qup0c/do_you_have_any_resources_of_heads_and_http/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Goodreads webscraping and TOS\n",
      "Text: So, I've been working on a side project just for fun where I wanted to measure my reading habits geographically and what not. This involved scraping Goodreads profiles, but the API has been deprecated for no good reason. Hence, I resorted to webscraping and am building a neat little scraper.   \n",
      "However, scraping probably breaks their robots.txt and what not, so I'm unsure how public I can make this scraper (I thought of it being on PyPi?). I'm very unsure hwo to proceed.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18qt4q5/goodreads_webscraping_and_tos/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Instagram scraping limits for media likers\n",
      "Text: Hi I noticed recently instagram has more restrictions to scraping post likers data:  'username' 'full name' 'followed by user' etc. than scraping same data from user following/followers. Anybody know what the limit usually is? I can freely scrape 10-15k per account without hittinh 429 limit (too many requests) from user followers/following, but for likers is very random I get the limit after 2,5k 5,5k and 6,5k in few other sessions. I use 5 sec wait time between requests and load 50 accounts in single request\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18qkz6l/instagram_scraping_limits_for_media_likers/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape crypto address from a website?\n",
      "Text: want to scrape the contact address for any cryptocurrency on the dexscreener.com website.\n",
      "Example the webpage https://dexscreener.com/solana/ 69gr|w4pcsypznn3xpsozcjft8vs8wa5817vuvnzngth\n",
      "Displays the information for ANALOS token, how do i get its contact address\n",
      "7iT1GRYYhEop2nV1dyCwK2MGyLmPHq47WhPGSwiqc\n",
      "Ug5) by scraping.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ql03r/how_to_scrape_crypto_address_from_a_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape this text from a website\n",
      "Text: I want to scrape the contact address for any cryptocurrency on the dexscreener.com website. Example the webpage https://dexscreener.com/solana/69grlw4pcsypznn3xpsozcjft8vs8wa5817vuvnzngth\n",
      "Displays the information for ANALOS token, how do i get its contact address (7iT1GRYYhEop2nV1dyCwK2MGyLmPHq47WhPGSwiqcUg5) by scraping.\n",
      "\n",
      "Kindly help.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18qkuxb/how_to_scrape_this_text_from_a_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Sites to host?\n",
      "Text: i wanna host my selenium-flask app i have found some sites but they dont have everything \n",
      "\n",
      "1. ability to host python code that works with selenium and flask\n",
      "2. persistent disks\n",
      "3. file system-like layout of being able to manually edit files\n",
      "\n",
      "i found that alot of sites that are like discord bot hosting have all of these except only 1 worked with flask/web apps in general and now they removed their free tier and non worked with selenium  \n",
      "i did find render/flo which have free tiers but flo doesnt have persistent disks and no file system like structure to even check on the ephemeral files and render has persistent disks although not in the free tier + also no file system  \n",
      "the file system thing is just smth i have seen alot on discord/minecraft hosting sites but it would be rlly useful to have it with selenium\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18qfsqi/sites_to_host/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Question regarding Nutch web crawl\n",
      "Text: Hi all! Recently, I have been trying to use Apache Nutch to crawl websites. It’s been working great. However, I have ran into 2 problems recently.\n",
      "1/ whenever I try to dump crawled data, it will result in a list of folders with names such as “9s”, “6b”, “3a”. I can’t even know which part of the site these data from. I want it to have folder structure like: “news”, “about us” instead of “3a”etc.. can some one help me with this\n",
      "\n",
      "2/ I dont know what I did, but recently when I try to dump a segment it keeps saying “corrupt segment”. I tried a lots of thing but it just doesn’t work.\n",
      "\n",
      "Please help me! Thank you so much!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18qeacc/question_regarding_nutch_web_crawl/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best web scraping tool or service for goodreads rss feeds? About 3 billion rss feeds to scrape.\n",
      "Text: This is what a typical rss feed looks like \n",
      "\n",
      "https://www.goodreads.com/review/list_rss/137464693?shelf=read&order=d&sort=rating&per_page=200&page=1\n",
      "\n",
      "All though there's 3 billion pages to check, but lot of those will be empty. I suspect that there's maybe 500 million with actual data for each. \n",
      "\n",
      "As you can see, each page is only a few kb of data.\n",
      "\n",
      "if it makes a difference, I had code to download from the feeds, but as of recently, been getting 503'd. I can't seem to download more than 10 pages in a row without getting 503'd, and I've been trying with different instances of the google colab servers. I figure it's better to use some sort of service.\n",
      "\n",
      "I don't need anything to parse the data, I can do that myself, just need the raw download data.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18qcgs8/best_web_scraping_tool_or_service_for_goodreads/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help! (New programmer)\n",
      "Text: I am trying to webscrape the Value Investors Club website for the ticker symbols within the ideas section ([https://valueinvestorsclub.com/ideas](https://valueinvestorsclub.com/ideas)). I am using BeauifulSoup and no matter what tags I search for I can't even get the body where all the post/ticker symbols are.  Could anyone point me in the right direction?   \n",
      "\n",
      "\n",
      "Keep in mind this would be part of my first \"nice\" project so im pretty inexperinced so if im looking over something stupid then thats why lol. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18q7rpn/need_help_new_programmer/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape using API\n",
      "Text: What's your general approach for scraping using a backend API? I understand the idea of inspecting XHR requests, but more often than not, they will have some token in the payload or the header, which renders replay attack useless. In those cases, how do you reverse engineer the obfuscated JS to get the token?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18q4hwm/how_to_scrape_using_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping an API\n",
      "Text: Hey Everyone! I am a noob when in comes to scraping so I am hpoing some of the pros out here can help out. I have a successful selenium code that is scraping some information from a website where I have to login and input a few things to get a graph which contains the info I need. Now, even though my code is working, it’s not scalable because it takes too long to get the content when I need to pull thousands a day ( I can only do 200 a day right now). I am trying to pull it by making direct api calls but I am struggling to get past the authentication part, do any of you have any pointers on videos or documents I get read regarding getting past the authentication part of the api? I have my username and everything I just don’t know what is not working. \n",
      "\n",
      "Another thing I have contemplated is running multiple python files at once, which since my computer is a tad bit old can only run 2 chrome browsers at once. But with a newer computer with better cpu and RAM, could I theoretically run say 100 selenium files, each opening a chrome browsers and scraping? \n",
      "\n",
      "Again I am relatively new so looking to understand all of these better\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18pe64g/scraping_an_api/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Teach me how to scrap this website!\n",
      "Text: Hey guys, i am looking for someone to teach me how to scrap this javascript driven website and get an  url for each housing item in this website. I don't know why i cannot find the url for every housing page, and the web is only redirected if i click the house item.  \n",
      "\n",
      "\n",
      "Here is the sample link: [https://mamikos.com/cari/yogyakarta-kota-yogyakarta-daerah-istimewa-yogyakarta-indonesia/all/bulanan/0-15000000?keyword=yogyakarta&suggestion\\_type=search&rent=2&sort=price,-&price=10000-1380000&singgahsini=0&gender=0,1&tag=10,11,14,15,17,648](https://mamikos.com/cari/yogyakarta-kota-yogyakarta-daerah-istimewa-yogyakarta-indonesia/all/bulanan/0-15000000?keyword=yogyakarta&suggestion_type=search&rent=2&sort=price,-&price=10000-1380000&singgahsini=0&gender=0,1&tag=10,11,14,15,17,648)  \n",
      "\n",
      "\n",
      "And i try to have this kind of url for evry item that listed:  \n",
      "[https://mamikos.com/room/kost-kabupaten-sleman-kost-putra-eksklusif-kost-kuning-exlusive-depok-sleman-2?redirection\\_source=list%20kos%20result](https://mamikos.com/room/kost-kabupaten-sleman-kost-putra-eksklusif-kost-kuning-exlusive-depok-sleman-2?redirection_source=list%20kos%20result)\n",
      "\n",
      "Can anybody want to teach me? I'm willing to pay for your hour to teach me how to do it my self.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18pmq3z/teach_me_how_to_scrap_this_website/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Google Maps Data Scraping\n",
      "Text: Hello,\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I am interested in extracting data for hotels, hostels, restaurants, and cafes from specific areas on Google My Maps. Please check out this link: ([https://www.google.com/maps/d/u/0/edit?mid=1XjPboKkbC0XkIpXLC-RX0\\_m1VNfJqTg&usp=sharing](https://www.google.com/maps/d/u/0/edit?mid=1XjPboKkbC0XkIpXLC-RX0_m1VNfJqTg&usp=sharing)) to view the designated areas for data extraction.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Do you know of any helpful tools or tutorials that could assist me in this task?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Thank you.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18p0bug/google_maps_data_scraping/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scrapping Zillow with Rotating Proxy, Denied Every Request\n",
      "Text: I have a list of home addresses, I make a request to every zillow url using a rotating data center proxy from a paid service. My program would request addresses until i reach a captcha where it says I am denied, it will close the driver, connect to a new proxy and start again. This worked great for a week or so. But now everytime I make a request, it gets denied instantly. I am have lots of trouble getting past this. Any help is appreciated.\n",
      "\n",
      "`def runDriver(driver, url_links, lats = None, longs = None):`\n",
      "\n",
      "`global total`\n",
      "\n",
      "`for idx, url in enumerate(url_links):`\n",
      "\n",
      "`print(url)`\n",
      "\n",
      "`initDictionary()`\n",
      "\n",
      "`try:driver.get(\"`[`https://www.zillow.com/homes/255%20Gilkeson%20Rd,%20Pittsburgh,%20PA%2015228/11398219_zpid/`](https://www.zillow.com/homes/255%20Gilkeson%20Rd,%20Pittsburgh,%20PA%2015228/11398219_zpid/)`\")`\n",
      "\n",
      "`time.sleep(2)`\n",
      "\n",
      "`updated_url = driver.current_url`\n",
      "\n",
      "`html_content1 = driver.page_source`\n",
      "\n",
      "`soup1 = BeautifulSoup(html_content1, \"lxml\")`\n",
      "\n",
      "`driver.quit()`\n",
      "\n",
      "For my driver set up it is:\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "`def serverDriver():`\n",
      "\n",
      "`host = HOST`\n",
      "\n",
      "`port = 22225`\n",
      "\n",
      "`username = USERNAME`\n",
      "\n",
      "`password = PASSWORD`\n",
      "\n",
      "`session_id = random.random()`\n",
      "\n",
      "`proxy_url = ('://{}-session-{}:{}@{}:{}'.format(username, session_id,password, host, port))`\n",
      "\n",
      "`sel_options = {'proxy': {'http': 'http'+proxy_url,'https': 'https'+proxy_url},}`\n",
      "\n",
      "`chrome_options = wb2.ChromeOptions()`\n",
      "\n",
      "`ua = UserAgent()`\n",
      "\n",
      "`user_agent = ua.randomprint(user_agent)`\n",
      "\n",
      "`chrome_options.add_argument(f'--user-agent={user_agent}')`\n",
      "\n",
      "`chrome_options.add_argument('--ignore-certificate-errors-spki-list')`\n",
      "\n",
      "`chrome_options.add_argument('--disable-extensions')chrome_options.add_argument('--window-size=1920,1080')`\n",
      "\n",
      "`chrome_options.add_argument('--disable-blink-features=AutomationControlled')`\n",
      "\n",
      "`chrome_options.add_argument('--disable-extensions')`\n",
      "\n",
      "`chrome_options.add_argument('--disable-popup-blocking')`\n",
      "\n",
      "`return wb2.Chrome(seleniumwire_options=sel_options, options = chrome_options)`\n",
      "\n",
      "I have tried using undetected chromium with no luck with my local ip\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18olkt8/scrapping_zillow_with_rotating_proxy_denied_every/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape entire website for products and information?\n",
      "Text: Hi. I am looking for help to scrape an entire online store. This would include hundreds of products and information. Is there a service available to save me the time? I don't need realtime updates for the data just a one off scrape to be stored into a csv file to be used in my online store. Any advice or suggestions would be great thank you. Adrian.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ory3p/how_to_scrape_entire_website_for_products_and/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping YouTube Shorts\n",
      "Text: Is there a way to use the YouTube search API to access YouTube shorts in particular? I can't find any documentation.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18oms8d/scraping_youtube_shorts/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape all of the reviews of a user\n",
      "Text: I want to ask for a user’s Google name and scrape all the reviews they’ve made on Maps. How can I do this? Also, I want to scrape the reviews of important places in a city. How can I do these?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18or31s/how_to_scrape_all_of_the_reviews_of_a_user/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: org.openqa.selenium.remote.http.websocket$listener onerror\n",
      "Text: How to handel this exeption connection rest\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18oq8zw/orgopenqaseleniumremotehttpwebsocketlistener/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Getting data then scrape. Help needed !\n",
      "Text: Hey ! \n",
      "\n",
      "I want to scrape Name of business owner from a business directory. It's the name of specific companies.\n",
      "\n",
      "I have the registration number of the companies on a google sheet and I want to search company from this number. \n",
      "\n",
      "So in resume I need to get the registration number from the sheet then either :\n",
      "\n",
      "* Make a search on google \"number\" + \"[businessdirectory.com](https://businessdirectory.com)\" then click on the first link then scrape the name on the page\n",
      "* Goes to the business directory website type in the search bar the number then click on the first result then scrape the name on the page\n",
      "\n",
      " I have 0 knowledge on how do to this type of things. Can you guys help me ? :)\n",
      "\n",
      "Thanks for your time. \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18oocua/getting_data_then_scrape_help_needed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping Instagram\n",
      "Text: I want to incorporate an instagram scraper into an existing web application running on heroku that will search instagram with a natural language search term, maybe \"indie music\". the scraper will then find a list of posts related to indie music and send them to a client application. \n",
      "\n",
      "I tried using instaloader, but I get a lengthy error message on my deployment: ConnectionException(\"Login: Checkpoint required. Point your browser to \" ... <long url>\n",
      "\n",
      "this only happens on the deployment, locally this runs fine. Do you all have any tips for how I can make this work?\n",
      "\n",
      "I guess a better way to say this is: How can I use instaloader-like python clients that require that I have a browser installed on my computer? Do I just need to make a container that will install chrome for me?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ohx9e/scraping_instagram/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Amazon Captcha\n",
      "Text: Any advice on getting around the Amazon Captcha when running a bot?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18omj8q/amazon_captcha/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to automate a website with hcaptcha enterprise?\n",
      "Text: I'm having trouble automating a website, which I'm not going to link to, so the post isn't removed.  Here is the link to the captcha provider they suggested for me to test,   \n",
      "[https://2captcha.com/api-docs/hcaptcha](https://2captcha.com/api-docs/hcaptcha)  \n",
      "\n",
      "\n",
      "but the documentation does not show how to extract the parameters for hcaptcha enterprise from the website, in the documentation of 2captcha have only this: An object containing additional parameters like: rqdata, sentry, apiEndpoint, endpoint, reportapi, assethost, imghost Usage example: \"enterprisePayload\":{\"rqdata\":\"test\\_string\"} So the question is how do I find and extract this information from website?  \n",
      "Thank you for any help?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ofdhs/how_to_automate_a_website_with_hcaptcha_enterprise/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Discord or Telegram community\n",
      "Text: Hi all of you !\n",
      "\n",
      "1st post in here for me :) I'm currently looking for webscraping community to improve myself in this. Do you have any link to some of those communities ? That could be Discord, Telegram, Forums, I don't care as long as the community is active.\n",
      "\n",
      "Thanks !\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ob2ol/discord_or_telegram_community/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape documentation?\n",
      "Text: I have to use an API published by an online company to get data. The Docs only have links to main pages and have NO word search capabilities.  They are also behind a username/password that you must enter to see them. (So Google cannot add these pages to its search engine.)\n",
      "\n",
      "I really need a way to search these documents by WORD. I tried to download them using HTTrack (open source site copying software) but it would not work. \n",
      "\n",
      "I also would like to add my own docs to it as they don't show all of the parameters and return values for the APIs that they publish.\n",
      "\n",
      "How would you get just the sub-set of pages dealing with their APIs ([https://somecompany.com/api](https://somecompany.com/api)) in a format that you could use and search locally?\n",
      "\n",
      "Would it be easier to just recreate their document pages (copy and paste) into some kind of documentation software?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18o16ku/how_to_scrape_documentation/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selenium grid\n",
      "Text: Hi guys, did anyone launched selenium grid server on the oracle cloud and was ablentonwork with it ?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18nrwxv/selenium_grid/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Approx how many puppeteer instances can I run with this server?\n",
      "Text: Any suggestions help I am trying to scale up my script to be able to run at least 100 threads.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18nmdxb/approx_how_many_puppeteer_instances_can_i_run/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Best method to \"reverse scrape\"* on LinkedIn? (I have job title and company, not individual person.)\n",
      "Text: Hi all, semi-competent and technical but I'm not sure how to ask the correct question to find my answer.  \n",
      "\n",
      "\n",
      "I have a list of attendees to a tradeshow. I know their self-identified state of residence, I know their company name, and I know their job title as they self-identify it on LinkedIn. Can I use this information to scrape LinkedIn to finding matching names?  \n",
      "\n",
      "\n",
      "From the reading I've done, the goal is often the opposite; find the company and scrape everyone from that company.   \n",
      "\n",
      "\n",
      "Thanks y'all.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ncd95/best_method_to_reverse_scrape_on_linkedin_i_have/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: API endpoint return error 400\n",
      "Text: Hi, so im currently scraping every bit of information on the European Commission Expert Groups i can through API endpoints (using Rstudio).   \n",
      "Each expert group have their own endpoints that im looping through. I get the endpoints via the network tab on the expert groups website. It generally works great, however, im having trouble with one endpoint link that give me error code 400.This is the endpoint:  [https://ec.europa.eu/transparency/expert-groups-register/core/api/front/meetings/search?page=0&size=10](https://ec.europa.eu/transparency/expert-groups-register/core/api/front/meetings/search?page=0&size=10)  \n",
      "I suspect its because there is no expert group ID in the endpoint like there is in the rest of the endpoints like this one, where the last 4 digits is the group ID:  [https://ec.europa.eu/transparency/expert-groups-register/core/api/front/expertGroups/3280](https://ec.europa.eu/transparency/expert-groups-register/core/api/front/expertGroups/3280)  \n",
      "\n",
      "\n",
      "Example expert group:  [Register of Commission expert groups and other similar entities (europa.eu)](https://ec.europa.eu/transparency/expert-groups-register/screen/expert-groups/consult?lang=en&groupID=3280) \n",
      "\n",
      "Under the response tab its says it should contain information, but i cant retrieve due to the error 400.  \n",
      "Does anyone know why i get the error 400, and how/if i can retrieve the information?   \n",
      "\n",
      "\n",
      "I just started webscraping last week - sorry if im not being clear enough. I still dont have the language for it :) \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18nc6mz/api_endpoint_return_error_400/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Scraping while changing urls?\n",
      "Text: So recently I've been trying to make a spreadsheet of my ao3 readings.\n",
      "\n",
      "I've searched online how to do it and I found this: https://www.reddit.com/r/AO3/comments/iv5yn2/comment/g5u130a/?utm_source=share&utm_medium=web2x&context=3 \n",
      "\n",
      "But the only way is through your history and I currently don't have an account.\n",
      "\n",
      "I might make an account in the future but right now I need to do my reading history with the urls i know instead.\n",
      "\n",
      "The thing I was trying to do was to make it so that I'd give the extension I'm using (webscraper.io) the Url of the fanfiction I read and the extension would scrape the data I want (title, fandom, word count, etc) from every URL I give it instead of from a reading history like how they do in the post I linked above.\n",
      "\n",
      "I don't know how to do this though, I tried to make a sitemap with a fanfiction url and made various selector with text and it worked for that fanfiction. But then there is no way to give it a different url and make it work for another fanfiction.\n",
      "\n",
      "I explained myself like shit i'm sorry I'm just really not familiar with these things.\n",
      "\n",
      "Basically I want something where I enter the URL and it gives me back the data I want (since all of the urls are organized the same way and from the same site), so like a sitemap but i can change the url each time.\n",
      "\n",
      "Again this is a shit explanation sorry but I can go into more detail if someone is willing to help please\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18nazo5/scraping_while_changing_urls/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Allocine scraping to get a csv\n",
      "Text: Hi!\n",
      "\n",
      "So recently I've been trying to get a csv file of my watchlist from allocine which is viewable from here :\n",
      "\n",
      "[https://www.allocine.fr/membre-Z20201030182341282651035/films/envie-de-voir/?page=2](https://www.allocine.fr/membre-Z20201030182341282651035/films/envie-de-voir/?page=2)\n",
      "\n",
      "I want to get the duration and genre of each of my movies.\n",
      "\n",
      "I tried using this\n",
      "\n",
      "[https://github.com/ibmw/Allocine-project](https://github.com/ibmw/Allocine-project)\n",
      "\n",
      "but it doesn't work for 2 reasons:\n",
      "\n",
      "\\- this code works for the general page of allociné which already give the duration of each film (you don't have to click on the film)\n",
      "\n",
      "\\- for a reason I don't know  I get a 'list' object has no attribute 'to\\_csv' on line 134\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Can you please help me I really don't know anything about scraping...\n",
      "\n",
      "Thanks !\n",
      "\n",
      "&#x200B;\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18n32b1/allocine_scraping_to_get_a_csv/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to scrape Amazon pages through a flutter mobile app\n",
      "Text: First, let me tell you I'm newbie to the scraping world and still trying to learn the basic best practices.\n",
      "\n",
      "I've setup a very basic scraping backend example with Playwright and have verified I'm able to extract some info from an Amazon product page. \n",
      "\n",
      "For some reasons I'd like to see if I can make the scraping algo works on the frontend, inside the mobile app which I am developing with flutter. I am not sure how to use headless browser or a library like Playwright in this case. I only see basic examples which use basic interfaces from http package which would not work since I need the whole page and not the initial html code. Also, seems like I am not detected like a human while doing that and can't get any meaningful html response.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "For example, I'd like to show the reviews and the rating of an Amazon product inside my app. One way to do that is to have a backend scraping server with proxies and a database, crawl the pages, fill-in the database and return the info to the user. The second way is to directly scrape inside the mobile app and show the info with no backend service, database and proxies needed. I'd like to understand if the second approach can be done at all. I'd like to evaluate both and decide which one to use based on cost and performance.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18mwvjm/how_to_scrape_amazon_pages_through_a_flutter/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need suggestions for high powered server for running puppeteer script that harvests Akamai cookies.\n",
      "Text: I’ve managed to make a script that can pretty much harvest valid cookies for any Akamai protected site my issues arise when trying to run at scale. Im still pretty new too webscraping with puppeteer so I was wondering if anyone had any suggestions on server specifications for trying to run 100 simultaneous browsers. It seems to me puppeteer uses more CPU than RAM but I’m looking for a good balance of both for steady running. Any tips are appreciated.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18msz6e/need_suggestions_for_high_powered_server_for/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How many requests needed to be banned from a website\n",
      "Text: I want to websrape a website\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ms4rr/how_many_requests_needed_to_be_banned_from_a/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help needed?\n",
      "Text: New to python in last few weekd, however starting to get somewhere. I have the following script that is doing some webscraping.  Everything is working fine, however the instock result is repeating twice for each result.  Ie the first item has 2 in stock, then the next has 3, however the scraped result repeats the 2 from the first item.  Then the 4th will be the 3rd, etc etc.\n",
      "\n",
      "The site code snippit :\n",
      "\n",
      "                <span class=\"Stock InStock HighStock\">\n",
      "                    <i class=\"fa fa-check Icon\"></i>\n",
      "                    \n",
      "                            2\n",
      "                        \n",
      "                    In Stock\n",
      "                </span>\n",
      "\n",
      "And my code:\n",
      "\n",
      "# Code starts here:\n",
      "  for url in urls:\n",
      "       driver.get(url)\n",
      "       soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
      "       groupings = soup.find('h1')\n",
      "\n",
      "       # Find items and prices (Adjust selectors as needed)\n",
      "       items = soup.find_all('div', class_='Name')\n",
      "       partnos = soup.find_all('span', class_='StockCode')\n",
      "    \n",
      "       for item in items:\n",
      "           title = item.find('a', class_='productTitle')\n",
      "           prices = soup.find_all('div', class_='Price')\n",
      "           instock = soup.find_all('span', class_='Stock InStock HighStock')\n",
      "           prices += soup.find_all(lambda tag: tag.name == 'span' and 'price' in tag.get('itemprop', ''))\n",
      "        \n",
      "\n",
      "       for partno, item, instock, price in zip(partnos, items, instock, prices):\n",
      "           table.add_row([partno.text.strip(), item.text.strip(), instock.text.strip(), price.text.strip()])\n",
      "\n",
      "   driver.quit()\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18ms40k/help_needed/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Advice with amazons waf (invisible) and intelligent threat javascript.\n",
      "Text: I used to scrape a website via http requests to an endpoint - I then parse the JSON file that is returned.\n",
      "\n",
      "Recently the website has upped their security and I'm now getting 403s. (Yes, I use proxys).\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "What Changed?\n",
      "\n",
      "1. The webpage now requires javascript to load.   \n",
      "HTTP requests now fail and returns a message \"...enable javascript...\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2. Endpoint now has a header for a V3 token - 2captcha can assist here.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3. I also see a cookie header with an aws-waf-token parameter.  \n",
      "I also see the \"...\\\\challenge.js\" link in the DOM - but no way to to get past this.\n",
      "\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I've been reading up on amazon intelligent threat url and this [link](https://docs.aws.amazon.com/waf/latest/developerguide/waf-js-challenge-api.html) pretty much sums up my roadblock.\n",
      "\n",
      "I need a way to get a legit aws-waf-token to embed in my cookie header.\n",
      "\n",
      "  \n",
      "I can use selenium to do the html parsing however NOT ALL the data from the JSON file is displayed on the webpage - which brings me back to http requests.\n",
      "\n",
      "Any advice is appreciated. Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18mk1bj/advice_with_amazons_waf_invisible_and_intelligent/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Selenium and cloudflare seem to have an issue\n",
      "Text: If these are my configurations for firefox selenium:\n",
      "\n",
      "`options = webdriver.firefox.options.Options()`\n",
      "\n",
      "`options.set_preference(\"general.useragent.override\",  \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:120.0) Gecko/20100101  Firefox/120.0\")`\n",
      "\n",
      "`options.set_preference(\"dom.webdriver.enabled\", False)`\n",
      "\n",
      "`options.set_preference(\"webdriver.firefox.marionette\", False)`\n",
      "\n",
      "`service = webdriver.firefox.service.Service(executable_path=\"geckodriver\")`\n",
      "\n",
      "`browser = webdriver.Firefox(options=options, service=service)`\n",
      "\n",
      "Why  is it that I am getting flagged by cloudflare turnstile and looping  through the same webpage again and again. The captcha is manual filled  out by me. I would appreciate any suggestions or feedback.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18mfbwy/selenium_and_cloudflare_seem_to_have_an_issue/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Need help scraping in Hacker Rank\n",
      "Text:   I need help scraping the leaderboard of a contest after the contest in HackerRank k(a programmingwebsitee)\n",
      "\n",
      "I have tried but it is not working\n",
      "\n",
      "well I think that what I ask is straightforward as it is not like I am half into a project and need help completing it\n",
      "\n",
      "it is like hacker rank is not letting anyone scrape and wants any suggestions\n",
      "\n",
      "sorry if I sound rood\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18m80tw/need_help_scraping_in_hacker_rank/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Mismatch between length of variable while scraping Airbnb\n",
      "Text: I am from a data analyst background and I have been recently trying to learn to webscraping. I recently tried to scrape Airbnb but I have a mismatch in the length of expense and rate/cabins. I have tried adding an error check for empty prices list but that does not work.\n",
      "```\n",
      "url=\"https://www.airbnb.com/s/Norway/homes?tab_id=home_tab&refinement_paths%5B%5D=%2Fhomes&flexible_trip_lengths%5B%5D=one_week&monthly_start_date=2024-01-01&monthly_length=3&price_filter_input_type=0&channel=EXPLORE&search_type=filter_change&price_filter_num_nights=5&date_picker_type=calendar&source=structured_search_input_header\"\n",
      "base_url=\"https://www.airbnb.com\"\n",
      "cabins=[]\n",
      "accomodation=[]\n",
      "price=[]\n",
      "expense=[]\n",
      "rate=[]\n",
      "count=1\n",
      "while True:\n",
      "    try:\n",
      "        resp=requests.get(url)\n",
      "        soup=bs(resp.text, \"html\")\n",
      "        # print(f\"Page {count}: {url}\")\n",
      "        next_page_url=soup.find(\"a\",{\"aria-label\":\"Next\"})[\"href\"]\n",
      "        url=base_url+next_page_url\n",
      "        count+=1\n",
      "        \n",
      "        places=soup.find_all(\"div\",class_=\"t1jojoys atm_g3_1kw7nm4 atm_ks_15vqwwr atm_sq_1l2sidv atm_9s_cj1kg8 atm_6w_1e54zos atm_fy_1vgr820 atm_7l_18pqv07 atm_cs_qo5vgd atm_w4_1eetg7c atm_ks_zryt35__1rgatj2 dir dir-ltr\")\n",
      "        for place in places:\n",
      "            cabins.append(place.text.strip())\n",
      "            \n",
      "        rooms = soup.find_all(\"span\",class_=\"dir dir-ltr\")\n",
      "        for room in rooms:\n",
      "            accomodation.append(room.text.strip())\n",
      "        \n",
      "        prices=soup.find_all(\"span\",class_=\"_tyxjp1\")\n",
      "        if not prices:\n",
      "            expense.append(0)    \n",
      "        else:\n",
      "            for price in prices:\n",
      "                expense.append(price.text.strip())\n",
      "                \n",
      "        ratings=soup.find_all(\"span\",class_=\"t1a9j9y7 atm_da_1ko3t4y atm_dm_kb7nvz atm_fg_h9n0ih r4a59j5 atm_h_1h6ojuz atm_9s_1txwivl atm_gy_z1y8gd_n6nuqf dir dir-ltr\")\n",
      "        for rating in ratings:\n",
      "            rate.append(rating.text.strip())\n",
      "            \n",
      "    except:\n",
      "        print(resp.raise_for_status())\n",
      "        break\n",
      "```\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18m1wxv/mismatch_between_length_of_variable_while/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: I keep getting redirected to this page after getting the content i want which makes me lose It\n",
      "Text: \n",
      "URL: https://www.reddit.com/gallery/18m5kvn\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Automated tool for find data collection entry points\n",
      "Text: Hello, can anyone recommend a scrapping tool that identifies domains or websites (specific urls) that host data collection forms(like registrations, subscriptions etc.), where users can input their information?\n",
      "If there's no automated tool, I would like some advice on resources or techniques that can help me identify such websites.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18lvlpj/automated_tool_for_find_data_collection_entry/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Concerns building on top,b2b\n",
      "Text: hey I was just curious if there's people who have built a scraper and let b2b clients to build on top of their solution via an api.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "What are the risk/concerns you had to assure the corporation ? What do they ask?How did it go?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18lt09r/concerns_building_on_topb2b/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Help Scraping data from Airtable\n",
      "Text: Hello Everyone!\n",
      "\n",
      "I'm sorry if this seems like a dumb question but I'm very new to web scraping and I wanted to make a project by scraping data from my university research opportunities which are posted using Airtable and log all the different types of \"jobs\" by sorting them with their department, But I have been struggling with scraping this site \"[https://airtable.com/embed/appd7poWhHJ1DmWVL/shrCEHNFUcVmekT7U/tbl7NZyoiJWR4g065](https://airtable.com/embed/appd7poWhHJ1DmWVL/shrCEHNFUcVmekT7U/tbl7NZyoiJWR4g065)\"\n",
      "\n",
      "I just want to pull all the data with a specific title but it seems to not work. Any hints or steps I should take would be very appreciated, I really hope to learn in the process!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18lsq2q/help_scraping_data_from_airtable/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Empty Results from Table Scrape\n",
      "Text: Hey all, thanks for any help in advance, feel like I am losing my mind. I am using Puppeteer to practice scraping this website: [Foreclosure Site](https://franklin.sheriffsaleauction.ohio.gov/index.cfm?zaction=AUCTION&zmethod=PREVIEW). Specifically, I am trying to map each of the listings here: \n",
      "\n",
      "      let currentWeek = await page.evaluate(() => {\n",
      "    \n",
      "            const listings = document.querySelectorAll(\".AUCTION_ITEM\");\n",
      "    \n",
      "            return Array.from(listings).map((listing)=>{\n",
      "                const startTime =         listing.querySelector(\"div[class=AUCTION_STATS]\").innerText;\n",
      "                const details =        listing.querySelector(\"div[class=AUCTION_DETAILS]\").innerText;\n",
      "        \n",
      "                return { startTime, details };\n",
      "            });\n",
      "        });\n",
      "\n",
      "However, this returns an empty array when run. I validated the selectors via the console and validated that the page had loaded before the scrape. During the troubleshooting, the listings array was returning results which leads me to think the issue is within the mapping. Any thoughts or help is really appreciated - been stuck on this for a couple of days.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18lpm51/empty_results_from_table_scrape/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Youtube Short Mass Upload Bot\n",
      "Text: Looking to hire a developer to create a Youtube short mass upload bot. Must be able to automate account creation, posting shorts, commenting and pinning a comment. If no direct experience doing this, must be well-versed in proxies and automation on other jobs. IM to discuss.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18lh6uo/youtube_short_mass_upload_bot/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: AI finder\n",
      "Text: I'm looking for an AI where you upload a file and be able to ask it questions, where it gives you answers from the file\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18lhifs/ai_finder/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Facebook groups (post, comments, reactions)\n",
      "Text: Hi, I need to scrape all posts, comments and reactions to the post in a certain group. I found nothing on github. Maybe I am too \"new\" to this. Do you have an idea how to achieve this? I am (relatively) comfortable with requests-, beautifulsoup and selectolax library. But maybe this is not sufficient to achieve this. Many thanks!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18lghid/facebook_groups_post_comments_reactions/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Suggestions on scraping SEC fillings websites?\n",
      "Text: What is the best way to scrape a text dataset out of SEC fillings? I'm trying to extract the text content of all shareholder proposals provided in a company's annual DEF 14A filling, the Board's recommendations for each shareholder proposal, and the text of its justification for those recommendations.\n",
      "\n",
      "I have links to the all the filings I need as [.htm](https://www.sec.gov/Archives/edgar/data/108516/000156459021044538/wor-def14a_20210816.htm) and [.txt](https://www.sec.gov/Archives/edgar/data/108516/000156459021044538/0001564590-21-044538.txt) links. I've tried using BeautifulSoup to parse the links, but given that the filing structure and terminology varies across companies (\"shareholder\" vs \"shareowner\" and other unknown terminologies) and across years even within the same company, I've found this extremely challenging. I even set up a HIT on MTurk paying people to copy and paste but I keep getting 99% shitty bot responses, even with 1000 HITs/99% approval rates as qualifications. I also created a GPT using the to extract the proposal data - while I couldn't write the prompt I needed to get it to return the proposal text, it could return the proposal title at least in a few cases.\n",
      "\n",
      "So, I thought I'd try turning to this community for other suggestions to collect this data. Any suggestions? This is for academic research btw. Thanks.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18laltc/suggestions_on_scraping_sec_fillings_websites/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How to get/request API key from Street Easy and Zillow ?\n",
      "Text: Hi everyone, I will start a project on real estate. And I need to collect data from Street Easy and Zillow. But first step would be to get the API keys from these websites. Can anyone tell me how to get the keys? If the above doesn’t work, what are the other ways to collect data from these websites? Thank you!\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18kviwx/how_to_getrequest_api_key_from_street_easy_and/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Tips for scraping a site that only loads 1 screen full of data with js at a time?\n",
      "Text: Hi.   I'm trying to scrape Dutchie for a personal project.  It's a Javascript heavy site.   It's pretty obfuscated, but I believe it only loads a screen of data at a time,  even though to the user it feels like a regular page with a scroll bar.   If I scroll down to the bottom and let all the images load,  it still only saves 3/4 of the data.\n",
      "\n",
      "I feel like using a real browser in python is more likely to give good results,  so I'm trying selenium.   Am I on the right path?  Any tips?  I've looked at other code that scrapes dutchie in ruby and they have gone nuclear on the anti scraping technology since then.\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18kvdb0/tips_for_scraping_a_site_that_only_loads_1_screen/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Web Scraping Betano\n",
      "Text:  Is it possible to do web craping on Betano and predict the results of virtual football? \n",
      "URL: https://www.reddit.com/r/webscraping/comments/18kssa2/web_scraping_betano/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: How would you scrape thousands of sites for key terms?\n",
      "Text: I have a list of some three thousand sites and want to search them for terms like financing and payment plans - thoughts on the best way to do this at that scale?\n",
      "URL: https://www.reddit.com/r/webscraping/comments/18k6w5b/how_would_you_scrape_thousands_of_sites_for_key/\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncpraw\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    reddit = asyncpraw.Reddit(\n",
    "        client_id='ba4JlGm6xCB-rmoKu5zHzA',\n",
    "        client_secret='2GdQbtBtMA4ZauDjU_wxMR688x4lcw',\n",
    "        user_agent='script:WebScraping:v1.0 (by u/Dry-Support-9317)'\n",
    "    )\n",
    "\n",
    "    # Modification ici pour cibler le subreddit 'webscraping'\n",
    "    subreddit = await reddit.subreddit('webscraping')\n",
    "    async for submission in subreddit.hot(limit=1000000):  # Vous pouvez augmenter la limite si nécessaire\n",
    "        print(\"Title:\", submission.title)\n",
    "        print(\"Text:\", submission.selftext)\n",
    "        print(\"URL:\", submission.url)\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        # Schedule the coroutine to be run on the existing event loop\n",
    "        loop.create_task(main())\n",
    "    else:\n",
    "        # Run the event loop and wait until the coroutine finishes\n",
    "        loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
